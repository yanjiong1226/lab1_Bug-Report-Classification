id,Number,sentiment,text
688,17355,0,"The Python __doc__ for each global op should be carried over from its source. ## 🐛 Bug

 is missing for many global operators (, , ...).

## To Reproduce





## Expected behavior

Global ops should contain valid s.

## Environment

PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.9.4

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX TITAN Black
GPU 1: GeForce GTX TITAN Black
GPU 2: GeForce GTX TITAN Black
GPU 3: GeForce GTX TITAN Black

Nvidia driver version: 390.30
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.1
"
616,13787,0,"Dataloader Segmentation Fault when using MPI backend & single process per gpu. ## 🐛 Bug

When using DistributedDataParallel with mpi backend and assigning each gpu a single process on the host, program crashes at the end of an epoch. Failure is not always consistent.




## To Reproduce
[https://github.com/pytorch/examples/blob/master/imagenet/main.py](https://github.com/pytorch/examples/blob/master/imagenet/main.py)

run


Modifications:



## Environment

## Additional context

Crash is less likely to occur if the  is decreased,  or the  is increased.

core dump generated


[pytorchsegfault](https://github.com/pytorch/pytorch/files/2572658/pytorchsegfault.txt)

"
322,18998,0,"torch.from_PIL() Request ?. ## 🚀 Feature
A simple method for transforming a PIL images directly into torch tensor.

## Motivation
It's frustrating to use transforms for a simple conversion between a PIL image and torch tensors and in the same time it's very easy to get tensor from numpy through  

## Pitch

Giving it a PIL images of any type and it returns a torch tensor

## Alternatives
I have considered two:
1. Transforms
2. convert it first to numpy then to torch
"
241,4302,0,"'torch.HalfTensor' object has no attribute 'mean'.  results into
 > 'torch.HalfTensor' object has no attribute 'mean'

 is 0.4.0a0+5f7c550"
647,367,0,Support dilation in Conv1d and Conv3d
701,3093,0,"better error messages on compiling with cudnn v5 (which is not supported anymore). Hi All --

I'm getting the following error when I try to build:



Seems like the  flags were added within the past couple of weeks, and I can't find anything Googling the errors.

I have CUDNNV5 and CUDA 8.0 on an Ubuntu16.04 box.  Can provide more details as needed.

Thanks
"
57,23871,1,"Quantized conv is 6x slower than float on CPU. Repro:
 Run resnext101-32x8d with quantized conv instead of float conv and measure elapsed time.

Possible causes:
 Layout transform before and after quantized conv2d calls."
350,15284,0,"index_add_ with scalar values instead of tensors. Hi, I was wondering whether we could update functions like  so that the third argument () can also be a scalar value?"
311,9983,0,"[feature request] Add matrix functions. Add matrix power as implemented by [numpy.linalg.matrix_power](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_power.html), matrix exponential as implemented by [scipy.linalg.expm](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.linalg.expm.html), and matrix logarithm as implemented by [scipy.linalg.logm](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.linalg.logm.html).


- [x] matrix_power
- [x] matrix_exp
- [ ] matrix_log
- [ ] matrix_sqrt

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @Lezcano @rgommers"
277,9484,0,"WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode. WARNING:root:Debug message: libcurand.so.9.0: cannot open shared object file: No such file or directory Segmentation fault (core dumped). HI,
I have GPU and CUDA, CuDNN, and NCCL. My OS is Ubuntu 16.04, I followed this tutorial to install Coffe2 with GPU support (
conda install -c caffe2 caffe2-cuda9.0-cudnn7
) and the installation finished successfully but this command: python2 -c 'from caffe2.python import workspace; print(workspace.NumCudaDevices())'

returns: WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.
WARNING:root:Debug message: libcurand.so.9.0: cannot open shared object file: No such file or directory
Segmentation fault (core dumped)
Any Idea how can I solve it??
TNX

$nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Tue_Jun_12_23:07:04_CDT_2018
Cuda compilation tools, release 9.2, V9.2.148

$ conda -V
conda 4.5.8

 nvidia-smi
Tue Jul 17 01:58:01 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro M520         Off  | 00000000:02:00.0 Off |                  N/A |
| N/A   41C    P0    N/A /  N/A |    298MiB /  2002MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     21578      G   /usr/lib/xorg/Xorg                           234MiB |
|    0     22157      G   compiz                                        62MiB |
+-----------------------------------------------------------------------------+

"
430,3514,0,"Unable to use cat on torch.LongTensor. Hello again,

I'm trying to concatenate two tensor together using  applied on a Tensor that I have created from a Numpy array. Here is the example:



Instead if you try with this it works:


What's wrong?

Cheers,
Alessandro"
329,26714,0,"spectral_norm used in RNN causes ""parameter types mismatch"" in GPU. ## Issue description
spectral_norm used in  is okay. But when it's used in ,there will be a 
model = network().cuda()
## Environment
 - PyTorch Version (e.g., 1.0): 1.1.0
 - Python version: 3.6.8
 - CUDA/cuDNN version:  cuda9.0


"
693,219,0,"Conv2dTranspose needs cudnn integration. Right now, only Conv2d is wrapped to be so."
413,5712,0,"Multiple versions of Dockerfiles fail to build. PyTorch GitHub Issues Guidelines
--------------------------------

- OS: Mac
- PyTorch version: master, v0.3.1, v0.3.0, v0.2.0, v0.1.12
- How you installed PyTorch (conda, pip, source):
- Python version: N/A
- CUDA/cuDNN version: N/A
- GPU models and configuration: N/A
- GCC version (if compiling from source): N/A

Version 0.3.1 below:

This produces this:

UPDATE:
Same goes for v0.3.0

UPDATE:
Running  fails with below

UPDATE:
Running 0.1.12 gives this, similar to latest versions but different package:

UPDATE:
Running  gives this:
"
274,2129,0,"[Feature request] truncated normal initializer(sampler). There has been a recent discussion on the discuss about truncated normal initializer [link](https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/15).
I have implemented one on my own, but the code is basically paraphrasing the tensorflow code, which I feel like may cause problem. If you feel good to add this, I can submit a PR once you tell me if it's ok to paraphrase or what's the alternative."
500,22269,0,"Add user-facing Function, so we don't have to expose torch.jit._C.Function. Add user-facing Function, so we don't have to expose torch.jit._C.Function

More details:
https://github.com/pytorch/pytorch/pull/22090#discussion_r296443544"
569,16574,0,"Make it possible to use mypy to typecheck code that uses PyTorch. We generated a type stub for  module in , which is good enough to get autocomplete working in PyCharm. However, the type stub is not that well tested for actually, you know, accurately reflecting the types of our functions (we have light testing, but it's not enough) and it's unlikely that it's actually good enough to actually use mypy to typecheck PyTorch-using code. It seems like this is something that people might be interested in, so I'm creating this issue to track. Please emoji if this is something that you would use.

cc @ezyang @gchanan @zou3519"
214,6258,0,"[feature request] use mkl_vml.h for exp, log vectorization on CPU. @cpuhrsch #6192 disables imprecise vectorized function from avx_mathfunc.h
[vml](https://software.intel.com/en-us/mkl-developer-reference-c-vm-mathematical-functions) is a vector math library from MKL, both performance and precision is promised. As long as  and  is installed in conda, vml can be used in ATen.
@MlWoo was preparing the code previously, the job got interrupted since he is in hospital lately :(
Below is the performance comparison of avx_mathfunc and vml in exp. Though the performance improvement is not so much, the precision is guaranteed.
if this looks ok to you, we will continue the job, after @MlWoo finish his therapy anyway.
"
653,16990,0,"Unsafe .data in fbgemm integration. https://github.com/pytorch/pytorch/blob/4292d13240e23a4a343b4ccb153214ab11c8d255/aten/src/ATen/native/QuantizedLinear.cpp#L39-L40

Here, e.g., if  isn't contiguous, the temporary tensor  can be freed and making the pointers invalid.

cc @jamesr66a "
504,17335,0,.circleci/regenerate.sh doesn't work when run from root directory
687,18862,0,"matmul uses too much memory in some batched cases. ## 🐛 Bug

PyTorch . The GPU times reported are on a P100.

In this case matmul uses about 12 GB of memory when it shouldn't use more than ~3 MB. (i.e. it's using 4096x more memory than necessary)

#### A


Note that this is equivalent to the following memory efficient operation:


#### B


It's also equivalent to the following which is memory efficient and faster, but may require a copy of y and the output may be batched-column-major without some extra work:

#### C


On GPU, **A** takes ~125 ms and uses 12 GB of memory, **B** takes ~22 ms, and **C** takes ~1 ms.

Originally https://discuss.pytorch.org/t/unexpected-huge-memory-cost-of-matmul/41642/4

See also https://github.com/pytorch/pytorch/issues/13222 which may be related

----

I believe the problem is the unnecessary contiguous call here:

https://github.com/pytorch/pytorch/blob/15b318de840de61e2e789c013e34d23819715090/aten/src/ATen/native/LinearAlgebra.cpp#L460-L461

Instead of using  and  it may be possible to use . That might achieve performance of **B**.

"
142,8716,0,[JIT] Traced and Script modules are not properly inlined into script functions
411,3882,0," Leaking dataloader. I’ve met a strange memory leak when I tried to implement “Improved Training of Wasserstein GANs”. I’m getting OOM in the middle of second epoch both on CPU and GPU. Memory usage seems to increase after each batch, the profiling of CPU version points on the for loop over dataloader. Here is the kinda minimal example:



From the discussion on https://discuss.pytorch.org/t/leaking-dataloader/10418 I've found out that the code runs fine on  torch 0.2.0_3, while I'm using https://github.com/pytorch/pytorch/tree/8ebf18b5b1d57ef16c24366649e720867f394a98 built from source, with CUDA 8 and cudnn7, so I supposed it should be some recently introduced bug"
68,3958,1,"10% performance regression on CuDNN convolution dispatch path. This PR https://github.com/pytorch/pytorch/pull/3666 introduces a 10% performance regression on the CuDNN convolution dispatch path. There is no single smoking gun, but it needs to be as fast as the old CuDNN code was. I spent this morning trying to fix it to no avail, so this is to remind me to fix it before next release.

cc @csarofeen @ptrblck"
736,30375,0,"Distributed Using Gloo on Multiple Nodes Does not Work. ## 🐛 Bug

I am trying to run a distributed deep learning workload on multiple nodes using Gloo. But unfortunately I receive the error below:



This is a two node environment with two VMs. Each of these VMs have 16 GB of RAM and 8 cores.
<!-- A clear and concise description of what the bug is. -->

## To Reproduce

The code is very similar to this link: https://pytorch.org/tutorials/intermediate/dist_tuto.html

The only difference is in the initiation phase:



<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

The expected behavior is not seeing the error code and the distributed training process kicking off.

## Environment

PyTorch version: 1.4.0.dev20191122+cpu
Is debug build: No
CUDA used to build PyTorch: None

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.17.4
[pip3] torch==1.4.0.dev20191122+cpu
[pip3] torchvision==0.5.0.dev20191122+cpu
[conda] Could not collect


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"
698,21761,0,"[JIT] Bad error message when instantiating a script class with no __init__. 

"
389,347,0,"cudnn 3d and 1d modules. we only added bindings for 2d modules. finish up.
Assigning to Sam."
451,30627,0,"Everything under /usr/local/include is copied.. ## Bug

When I  to a custom location for packaging (previously set by ), I got everything under my  copied to this custom place.

This only happens on Ubuntu 18.04, and CentOS 7 seems fine with the same build script.
And I've only started to see this recently.
Builds from 2 weeks ago was still fine.

![image](https://user-images.githubusercontent.com/5203025/69997152-5ba9d580-1508-11ea-98c2-fa155ffc8d4e.png)

## Environment

 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Ubuntu 18.04 docker
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): cmake+ninja
 - Python version: 3.6.9 (system default)
 - CUDA/cuDNN version: 10.2


cc @ezyang @gchanan @zou3519"
347,18645,0,"c++ Windows  compile error . visual studio2017  ；Reference directory  O:\pytorch\libtorch\include\torch\csrc\api\include  
  O:\pytorch\libtorch\include   library directory O:\pytorch\libtorch\lib .... 
At the 101 of document rnn.h. There's a mistake.have error :cast to function type is illegal
 pytorch is https://download.pytorch.org/libtorch/cu100/libtorch-win-shared-with-deps-latest.zip
cuda is 10.0
windwos 10
visual studio 2017
release x64
cl.exe 14.16.27023


I think this is a compiler error. To this, there are Windows installation tutorials, you can compare the difference. Select the compilation environment and version.
"
669,23649,0,Installation with non-root access. When I run  it need root access to install the final package. How can I install that in user home (I mean user packages for pip)?
273,9853,0,"TestSequenceOps.test_gather_padding failing. reference(*inputs)threshold

Sample: https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda9.0-cudnn7-aten-ubuntu16.04-test/4954/consoleText"
630,18083,0,"A common class in Linear, Conv, LSTM, ... . Hi,

Is there any class that all  inherit from that? 
I want to create a general , which has , so that later I can fill  with any type of layer that I want, e.g.,  Is there such a capability in the current API? This can be done easily in python, though here we need declaration and this hinders the python's easiness. 

Thanks,
Afshin
"
301,7944,0,"Better error message in DataChannelTCP::_receive. ## Issue description

The following code will produce an error from the TCP distributed backend:



Error:



This is a bit misleading, since the sizes of the tensors are the same but the types are different. I think it would be better to give a message along the lines of:



I'd be happy to submit a PR for this if others agree that the original message should be changed."
594,3946,0,"Any Pytorch function can work as Keras's ""TimeDistributed""?. In keras,there is a timedistributed function (https://github.com/fchollet/keras/blob/master/keras/layers/wrappers.py)  which can apply a layer to each temporal slice, I hope the author can develop a function like that."
608,1752,0,"dockerfile: directory '.' is not installable. file 'setup.py' not found. I am using the dockerfile, but the there is an error: directory '.' is not installable. file 'setup.py' not found.

I believe it's located at the line ""pip install -v ."" Anyone know how to fix it? Thanks!"
254,31317,0,"[feature request] [onnx] Export torch.stft as Conv1d till ONNX supports stft op. As far as I understand, for the time being STFT [isn't natively supported by ONNX](https://github.com/onnx/onnx/blob/master/docs/Operators.md)
 
STFT can be exported as a Conv1d op with precomputed (windowed) Fourier basis: https://github.com/NVIDIA/mellotron/blob/master/stft.py

This is useful for tracking/exporting speech recongition models.

I've done a pure PyTorch version in https://github.com/vadimkantorov/convasr/blob/master/models.py#L315-L334 and it seems to work

cc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"
187,24062,0,"[quant] use qconfig_dict in graph mode. ## 🚀 Feature

Extend the initial implementation to use qconfig_dict,
Need scoping support, cc @ZolotukhinM"
164,3657,0,"No link to the previous version doc in master doc. http://pytorch.org/docs/master/
The link to switch doc is dead.

http://pytorch.org/docs/0.2.0/
Old versions are still available and the link is ok there."
287,6257,0,"[feature request] adding a nonzero element ""in-place"" in sparse tensor. First mentioned here, but turns out to be non-existent. 
https://github.com/pytorch/pytorch/pull/6225#pullrequestreview-108887248

This operation would simply add a new index and a new value to a sparse tensor. 

cc @vincentqb @aocsa @nikitaved @pearu @mruberry"
377,28329,0,"No in-place version of where(). ## 🚀 Feature
There is no way to perform the functionality of  in-place. This feature would either add a  method to  or an  parameter to the existing , or both.

## Motivation

All of the usual reasons for doing operations in-place.

## Pitch

As stated above, add a  method to  or an  parameter to the existing .

## Alternatives

One could use the out-of-place  for a less efficient alternative.

Note that the  method does not have the same functionality as an in-place .
"
705,20875,0,"BatchNorm2d implementation returns different results than expected. ## 🐛 Bug

As per https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d I expect the result of BatchNorm2d to be (x - mean) * scale / sqrt(var + eps) + bias.  However this is not the case.

Given an already loaded BatchNorm2d module as bn0, I have the following:


## Environment
condapip`, source): pip
 - Python version: 3.5
 - CUDA/cuDNN version: 9.2 / 7.1.3
 - GPU models and configuration: 
 - Any other relevant information:
"
245,20402,0,"Tensor and nn.Module Pruning. ## 🚀 Tensor and nn.Module Pruning
Tensor method and/or  util to sparsify tensors and/or model, according to various pruning techniques in the literature. 

## Motivation
State-of-the-art deep learning techniques rely on over-parametrized models that are hard to deploy. On the contrary, biological neural networks are known to use efficient sparse connectivity. It's important to identify best techniques to compress models by reducing the number of parameters in them, in order to reduce memory, battery, and hardware consumption without sacrificing accuracy, deploy lightweight models on device, and guarantee privacy with private on-device computation. On the research front, pruning is used to investigate the differences in learning dynamics of over-parametrized and under-parametrized networks, to study the role of lucky sparse subnetworks and initializations (""lottery tickets"" [[1]](https://arxiv.org/abs/1803.03635)), as a destructive neural architecture search technique, and others.
Goal of this feature: harmonizing pruning practices by providing a standard interface in PyTorch.
Target audience: researchers, engineering and product teams.

## Pitch
Minimalist API, with deeper flexibility for power-users. 

At the tensor level, this could look as follows:

A not-in-place  method will return a  of the same type and size as the one it acts on.
In-place pruning supported via .

At the model level, this will require a bit of thinking but should follow similar API patterns. This is important because not all pruning methods make sense on all parameters in a model (pruning conv kernels != pruning biases != pruning RNNs != pruning in the presence of batch norm, etc.). 
First, we should have a sensible, well-documented default behavior for the average-user's API, where a call to  defaults to pruning PyTorch ""prepackaged"" modules (such as linear, conv, and recurrent layers) in some sensible, expected way. 
Most power users though would probably want to prune custom layers, or prune different layer types or layers at different depths using different pruning methods or pruning method parameters. This could be specified via a dictionary, which maps parameter names (contained in ) to a pruning method and its parameters: 


Similar to the tensor operations, model-level pruning could return a copy of the model, or act on the model in place.

Pruning methods can be used during or post- training; this implementation will be training-loop-agnostic: the user will have to take care of writing their own training loop to decide when to prune and what to do with the pruned object (re-initialize and retrain, finetune, etc.).

## Alternatives
Depending on where this will live within the codebase,  could also look like:  or . I personally would prefer the first option because the kwargs are parameters of the pruning method itself, while  is the tensor it acts on (some pruning methods will also have to take in some data  or  when they're applied), but the last option is more in line with how, say,  is implemented. Perhaps, for Module-level application, following the [example of the  implementation](https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html) using hooks will make this more PyTorch-y, but I don't know if we want to sacrifice the ability to act directly on a tensor that is not part of a Module. Would that go into ? Open to suggestions here.


cc @albanD @mruberry @jbschlosser"
90,8154,1,"Performance drop on small models trained on CPU between 0.3.1 and 0.4. ## Issue description

Updating from Pytorch 0.3.1 to 0.4 yields a significant performance drop on small feedforward networks, trained **on CPU**. This effect happens on Linux (in a virtual machine) and on Windows.

The attached code was executed on a Windows 10 machine with an Intel i7-7700HQ.
The Linux test was done in a virtual machine. The computer is equipped with an Intel i7-4702MQ.

The average runtime over 10 repetitions on Linux is 26.141655 s for Pytorch 0.3.1 and 32.092158 s on 0.4  (~23 % increase)
The average runtime over 10 repetitions on Windows is 12.577887 s for Pytorch 0.3.1 and 17.759720 s on 0.4  (~41 % increase)

Observations: 
- The time difference between 0.3.1 and 0.4 seems to be more or less constant on both Windows and Linux. 
- Also profiling suggests the DataLoader __next__ takes 8 s instead of 4 s previously, a major part of the increased time
- Weirdly, in different calls of the script, the early stopping if-clause activates sometimes despire setting the pytorch seed as to avoid effects due to randomness

## Code example

See my results and the data/script used here:
[PytorchSpeedTest.zip](https://github.com/pytorch/pytorch/files/2071136/PytorchSpeedTest.zip)

## System Info

(the collect_env is not yet adapted to Windows, therefore information is missing. I am using pip 10.x and the latest conda)

PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 9.1

OS: Microsoft Windows 10 Home
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): all conda (Windows 0.3.1. via conda install -c peterjc123 pytorch-cpu)
- OS: Windows 10 / Ubuntu 16.04 
- PyTorch version: 0.3.1 to 0.4
- Python version: 3.6

## Bottleneck Profiling
0.4


### cProfile
0.3.1

 "
591,11757,0,"Unable to build PyTorch from source without NumPy support. ## Issue description

So far, all the builds in the CI have had NumPy support, meaning that this issue has not been exposed in its entirety yet.

I tried to build PyTorch without NumPy support, and it wasn't possible at first. Editing  made sense to accomplish this, but then it seems that Caffe2 requires NumPy support.

This is the error I got after (almost) building PyTorch.


## Code example

This is what I did to the  file:

## System Info
- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): 
- OS: Ubuntu 18.04.1
- Python version: 3.7
- GCC version (if compiling from source): 7.3
- CMake version: 3.11.3
- Versions of any other relevant libraries: None
"
426,16806,0,"storage_initialized() ASSERT FAILED when I execute tensor.resize_(-1). ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

When I execute , I get a run-time error message that tells me to report the bug: . Because  is not a valid argument to , it is reasonable that we get an error. However, the above error message seems to suggest that this error is not handled properly in the source code.  

## To Reproduce

Steps to reproduce the behavior:

1. 
2. 
3. 

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

A proper error message should have been issued. 

## Environment

 - PyTorch Version (e.g., 1.0): 1.0.0
 - OS (e.g., Linux): Windows 10
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source): N/A
 - Python version: 3.6.2
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information: N/A
"
191,25117,0,"Nvidia driver & cudatoolkit installed properly but check_driver fails. I have successfully installed NVIDIA driver & cudatoolkit via conda. However, I am not able to use cuda in pytorch (even though it installed successfully). 

Previously, I was using Pytorch with CUDA 8.0, and wanted to upgrade. I removed / purge all CUDA through:


 
Then I updated my Nvidia drivers to 4.10 via PPA (Ubuntu 16.04):



Everything worked smoothly. The output of :


The output of 


Since I wanted conda to manage my CUDA version, I installed the cudatoolkit through conda env (python 3.6):



again, everything installs perfectly. When I run:

but using cuda fails. I get the following error message




I restarted, removed all irrelevant environment variables which may have caused issues (LD_LIBRARY_PATH), removed conda, reinstalled, tried cuda 9.2, but nothing works. I am not sure what the issue could be. Any ideas?

I searched a bit, and found [this](https://discuss.pytorch.org/t/found-no-nvidia-driver-on-your-system-but-its-there/35063) pytorch thread. Since I completely removed CUDA from my system this shouldn't be the problem, but I think somehow it may be related. 

**EDIT:**
It isn't surprising given my error, but following [this issue](https://github.com/pytorch/pytorch/issues/4546), I checked:
"
215,630,0,Add Peephole connections for LSTMs?. From this [paper](http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf). Peephole connections seem to help in learning precise timings of events.
433,27512,0,"[JIT] floordiv not bound to tensor. ## 🐛 Bug

floordiv is only bound for int, not tensor, which results in an incorrectly inserted cast..


cc @suo"
193,15866,0,"Casting to and from at::Half on CPU is not supported yet. Traceback (most recent call last):
  File ""/home/tcl/PycharmProjects/caffe2/mnist_caffe2.py"", line 278, in <module>
    workspace.CreateNet(train_model.net, overwrite=False)
  File ""/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py"", line 171, in CreateNet
    StringifyProto(net), overwrite,
  File ""/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py"", line 197, in CallWithExceptionIntercept
    return func(*args, **kwargs)
RuntimeError: [enforce fail at cast_op.cc:69] . Casting to and from at::Half on CPU is not supported yet
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x78 (0x7f234572e408 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libc10.so)
frame #1: <unknown function> + 0x1636612 (0x7f2346f74612 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #2: <unknown function> + 0x163e3c8 (0x7f2346f7c3c8 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #3: <unknown function> + 0x163e55e (0x7f2346f7c55e in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #4: std::_Function_handler<std::unique_ptr<caffe2::OperatorBase, std::default_delete<caffe2::OperatorBase> > (caffe2::OperatorDef const&, caffe2::Workspace*), std::unique_ptr<caffe2::OperatorBase, std::default_delete<caffe2::OperatorBase> > (*)(caffe2::OperatorDef const&, caffe2::Workspace*)>::_M_invoke(std::_Any_data const&, caffe2::OperatorDef const&, caffe2::Workspace*&&) + 0x23 (0x7f2348971803 in /usr/local/lib/python2.7/dist-packages/caffe2/python/caffe2_pybind11_state.so)
frame #5: <unknown function> + 0x13efc1c (0x7f2346d2dc1c in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #6: <unknown function> + 0x13f2aaa (0x7f2346d30aaa in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #7: caffe2::CreateOperator(caffe2::OperatorDef const&, caffe2::Workspace*, int) + 0x3b9 (0x7f2346d30f29 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #8: caffe2::SimpleNet::SimpleNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x295 (0x7f2346d27b75 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #9: <unknown function> + 0x13ebf2e (0x7f2346d29f2e in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #10: <unknown function> + 0x13ce563 (0x7f2346d0c563 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #11: caffe2::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x47d (0x7f2346cffc4d in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #12: caffe2::Workspace::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, bool) + 0xfd (0x7f2346d6e1dd in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #13: caffe2::Workspace::CreateNet(caffe2::NetDef const&, bool) + 0x91 (0x7f2346d6f371 in /usr/local/lib/python2.7/dist-packages/caffe2/python/../../torch/lib/libcaffe2.so)
frame #14: <unknown function> + 0x4e616 (0x7f234896a616 in /usr/local/lib/python2.7/dist-packages/caffe2/python/caffe2_pybind11_state.so)
frame #15: <unknown function> + 0x4e8dc (0x7f234896a8dc in /usr/local/lib/python2.7/dist-packages/caffe2/python/caffe2_pybind11_state.so)
frame #16: <unknown function> + 0x86831 (0x7f23489a2831 in /usr/local/lib/python2.7/dist-packages/caffe2/python/caffe2_pybind11_state.so)
frame #17: PyEval_EvalFrameEx + 0x695b (0x55e76d2878db in /usr/bin/python2.7)
frame #18: PyEval_EvalCodeEx + 0x6da (0x55e76d27ed0a in /usr/bin/python2.7)
frame #19: PyEval_EvalFrameEx + 0x5cb8 (0x55e76d286c38 in /usr/bin/python2.7)
frame #20: PyEval_EvalCodeEx + 0x6da (0x55e76d27ed0a in /usr/bin/python2.7)
frame #21: PyEval_EvalFrameEx + 0x567e (0x55e76d2865fe in /usr/bin/python2.7)
frame #22: PyEval_EvalCodeEx + 0x6da (0x55e76d27ed0a in /usr/bin/python2.7)
frame #23: PyEval_EvalCode + 0x19 (0x55e76d27e629 in /usr/bin/python2.7)
frame #24: <unknown function> + 0x12461f (0x55e76d2af61f in /usr/bin/python2.7)
frame #25: PyRun_FileExFlags + 0x82 (0x55e76d2aa322 in /usr/bin/python2.7)
frame #26: PyRun_SimpleFileExFlags + 0x18d (0x55e76d2a967d in /usr/bin/python2.7)
frame #27: Py_Main + 0x68b (0x55e76d2581ab in /usr/bin/python2.7)
frame #28: __libc_start_main + 0xe7 (0x7f2367bf4b97 in /lib/x86_64-linux-gnu/libc.so.6)
frame #29: _start + 0x2a (0x55e76d257a2a in /usr/bin/python2.7)"
360,11978,0,"[Caffe2] Attempting to install Caffe2 in Google Colab. ## Issue description

Hello, I'm trying to install Caffe2 in Google Colab so I can work with Detectron.
I've tried installing from Source however it takes too long (more than 2 hours) so it is impractical for Colab. Thus, I'm forced to install from binaries using Anaconda.

I install anaconda without much issues.



However, I'm trying to follow the [recommendation](https://caffe2.ai/docs/faq.html#why-is-caffe2-not-working-as-expected-in-anaconda) to install Caffe2 on a different environment than the base one.
I manage to create the environment just fine, however, trying to activate it does not work. In particular using:  seems to do nothing. So it seems that's out.

So, I go and install Caffe2 using the base anaconda environment. First, I have to add some code to add the anaconda binaries to the PATH, as it seems the usual way to assign environmental variables in Colab seem to not work.

Then installation as usual:


It installs without any issues. However, when trying to run the test code:

It returns:


I understand this is probably some PYTHONPATH [issue](https://github.com/facebookresearch/Detectron/blob/master/INSTALL.md#caffe2), however, I can't seem to find the ""build"" folder in caffe2.
If I search for caffe2 directory ()  after installing I only get the following results:

If I search in the directories of each result I get:

ls /root/anaconda2/pkgs/caffe2-cuda9.0-cudnn7-0.8.dev-py27_2018.08.26/lib/python2.7/site-packages/caffe2:

ls /root/anaconda2/pkgs/caffe2-cuda9.0-cudnn7-0.8.dev-py27_2018.08.26/include/caffe2:

ls /root/anaconda2/lib/python2.7/site-packages/caffe2:

ls /root/anaconda2/include/caffe2:

None of them has a build directory.

So finally I have some questions:

- **Was the Caffe2 installation successful?** I wonder given that I cannot find the 'build' directory.
- **Is this only a PYTHONPATH variable issue?** If so, to which folder should I point it?

I'm sorry if the question is noobish. But I'm a bit stuck. Thanks in advance!

### System Settings

- PyTorch or Caffe2: Caffe2
- How you installed PyTorch (conda, pip, source): conda
- Build command you used (if compiling from source):
- OS: Linux, whatever Colab's using.
- PyTorch version:
- Python version: 2.7
- CUDA/cuDNN version: 9.0 and 7.0
- GPU models and configuration:  Tesla K80
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
"
398,15764,0,"torch.argsort descends wrongly. ## 🐛 Bug
given a large amount of numbers (say, a list containing 32768 numbers), I found that the argsort behaves wrongly when setting the descend=False.
## To Reproduce
I want to calculate the similarity of word embeddings, and I managed to achieve that by batched-similarity calculation. I expect the most similarity to an embedding should be itself, thus the highest rank should be 1,2... accordingly, and vice versa. 
now say we have torch.tensor object  (in shape batch_size * 32768), I want to extract top similarities along the last dimension.

the code gives the result of:

> get_top = torch.argsort(similarity, dim=1)
> [20485, 30807, 27706,  ...,   117,     6,     **1**],
>  [29628, 16835, 23989,  ...,  6773, 10377,     **2**],
>    ...,
>  [30120, 23853, 24914,  ...,   118,    50,    **47**],
>  [23528, 23663, 25283,  ...,    78,    77,    **48**],
>  [31738, 17062, 29731,  ...,  1240,    79,    **49**]

while:

> top8 = torch.argsort(similarity, dim=1, descending=True)
> tensor([[**21820**, 21822, 21816,  ..., 10909, 10910, 32725],
>         [**32497**, 28158, 20713,  ..., 14889, 12632, 16177],
>         [**17110**, 17460, 18861,  ..., 14628, 12113,    18],
>         ...,
>         [**22335**, 21440, 26360,  ..., 14870,  8292, 13842],
>         [**20379**, 16479, 28973,  ..., 11770, 11327, 13454],
>         [**23388**, 19492, 16577,  ...,  7874,  5219, 13230]])
the bold part implys the main problems. and I assume it's not simply because of the accuracy of float numebrs.
## Expected behavior
> top8 = torch.argsort(similarity, dim=1, descending=True)
> tensor([
>         [**1**,  blablabla ],
>         [**2**, blablabla],
>         ...,
>         [**47**, blablabla],
>         [**48**, blablabla],
>         [**49**, blablabla]])
<!-- A clear and concise description of what you expected to happen. -->

## Environment
 - PyTorch Version: 1.0 stable
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (, , source): pip install
 - Python version: 3.6.3

"
160,25515,0,"ONNX export failed on ATen operator upsample_bilinear2d. ## 🐛 Bug

When I use torch.onnx.export , I met opset support problem:


## To Reproduce

Steps to reproduce the behavior:

1. pytorch inference file
2. change the output to onnx.export

## Environment

 - PyTorch Version (e.g., 1.0): 1.2
 - OS (e.g., Linux): Centos
 - How you installed PyTorch (, , source): Conda
 - Build command you used (if compiling from source): 
 - Python version: 3.7.3
 - CUDA/cuDNN version: 10/7.6
 - Any other relevant information: pytorch model is trained by pytorch 1.0.0
"
439,31248,0,"RuntimeError: cholesky_cuda: For batch 0: U(6,6) is zero, singular U.. ## 🐛 Bug when using 
Hi, I always get this RuntimeError during my training process:

##  Reproduce
There is a KL-loss term in my loss function, and I assume the two distributions are multivariate normal distributions, so I calculated it as follows:

## Environment
Here is my package version:
 - PyTorch 1.3.1  
 - Ubuntu 18.04
 - CUDA 9.2.148 with cuDNN 7.6.3_0 

## Additional 
I find my KL-loss falls in the range from 1e-6 to 1e-5.

 Any ideas to solve this problem? thx
"
222,18701,0,"[MSVC] Caffe2-Debug Static Size Too Big LNK1248. ## 🐛 Bug

pytorch-Debug-build\lib\Debug\caffe2-Debug.lib: fatal error LNK1248: image size (10007FE9E) exceeds maximum allowable size (FFFFFFFF)
## To Reproduce

Steps to reproduce the behavior:

1. Compile latest master on Windows as static

## Expected behavior

Links

## Environment

 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Windows
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): cmake -DUSE_CUDA=ON -DBUILD_SHARED_LIBS=OFF
 - Python version: 3.7
 - CUDA/cuDNN version: 10.1
 - GPU models and configuration: GTX 1060
 - Any other relevant information:

## Additional context

Did not have this issue in 1.0.1 but perhaps size has just gone over the threshold.

It appears MSVC has a 4GB code/data file size limit. I'm sure there's a workaround.
Although, Microsoft says to split up the library: https://developercommunity.visualstudio.com/content/problem/332991/static-lib-4gb-file-size-limit-lnk1248-image-size.html"
706,957,0,"Counter-intuitive behavior of expand and arithmetic op. I don't know if this is a bug (and it might have been reported already but I didn't find anything).
But it could at least be added to the documentation for the  operation on a tensor:

Even if you are aware that expand doesn't copy data, I believe most will find this result quite counter-intuitive. Of course, this can be fixed atm either by replacing  by  or  by , but that requires more memory. It would be awesome if the arithmetic ops like  had a special case when the tensor's  is 0 in some dimension

Edit: My torch version string is "
435,1829,0,"cuda out of memory with size 1 tensor. Hi, I have just built pytorch on OS X with CUDA support.

I'm having some trouble using CUDA features as it seems to run out of memory even with a size 1 tensor.


My laptop is fairly old, with an NVIDIA GeForce GT 650M 1024 MB. CUDA version is 8.0. Torch version is . OS X is 10.12.4. I am using Python 3.5.3 with Anaconda."
535,30400,0,"miniz is missing in .gitmodule. ## 🐛 Bug

miniz is missing in .gitmodule but there is miniz in the third_party

Steps to reproduce the behavior:

1.sync the lastest PyTorch code;
1.run build
1.report 'caffe2/CMakeList.txt missing miniz'

It seems that .gitmodule does NOT match third_party completely?
"
568,24958,0,"In Sparse Adam, initialize state['exp_avg']  and state['exp_avg_sq'] during optimizer initialization, not after step. ## 🚀 Feature
In the Sparse Adam optimizer, [ https://github.com/pytorch/pytorch/blob/master/torch/optim/sparse_adam.py ]

The 'exp_avg' and 'exp_avg_sq' are only initialized after the first step is taken. And then after each step, there is a check of if the state hasn't been initialized. Compare this to adagrad https://github.com/pytorch/pytorch/blob/master/torch/optim/adagrad.py ], where 'sum' is initialized when the optimizer is initialized. Is there any reason for this? 

## Motivation

For those doing manipulations of 'exp_avg' and 'exp_avg_sq', the code would be clearer if they didn't have to run the first step before coding the manipulators. 

Also, unless I am missing something, the code would seem more cleaner if the state was initialized during the optimizer init, instead of checking if it's been initiated already during each step, though there's probably no performance reduction in the way it's currently coded. 

## Pitch

Move 


to the  __init__ method. 


## Additional context

Here's a full code sample




If there is agreement on this, I already created a pull request, here 

https://github.com/pytorch/pytorch/pull/24960

cc @vincentqb"
516,14967,0,"ONNX - Pytorch-1.0 with Cuda-10.0. ONNX Export core dumps on GPU machine. ## 🐛 Bug

Pytorch-1.0-Cuda-10.0 with ONNX-1.2.1 and 1.3.0. Export onnx model core dumps on Ubuntu on GPU instance

## To Reproduce

 Steps to reproduce the behavior:
 
Spin up an Ubuntu with GPU instance like EC2 p2 or g3
 
1. install Cuda-10, cudnn-7.4.1, NCCL-2.3.7
2. anaconda
2. conda create -n pytorch_p27 python=2.7
3. conda install pytorch=1.0.0=py2.7_cuda10.0.130_cudnn7.4.1_1 torchvision=0.2.1 cuda100=1.0 -c pytorch
4. pip install -U onnx==1.2.1 or 1.3.0
 
example used to replicate:
https://github.com/onnx/tutorials/blob/master/tutorials/PytorchOnnxExport.ipynb
 
Error:
Illegal instruction (core dumped)



## Expected behavior

It should have exported a .onnx file with the model

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130
 
OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1
 
Python version: 2.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: Tesla M60
GPU 1: Tesla M60
GPU 2: Tesla M60
GPU 3: Tesla M60
 
Nvidia driver version: 410.79
cuDNN version: Probably one of the following:
/usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1
/usr/local/cuda-10.0/lib64/libcudnn_static.a
/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21
/usr/local/cuda-8.0/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so.7.3.1
/usr/local/cuda-9.0/lib64/libcudnn_static.a
/usr/local/cuda-9.2/lib64/libcudnn.so.7.3.1
/usr/local/cuda-9.2/lib64/libcudnn_static.a
 
Versions of relevant libraries:
[pip] Could not collect
[conda] blas                      1.0                         mkl 
[conda] cuda100                   1.0                           0    pytorch
[conda] mkl                       2018.0.3                      1 
[conda] mkl_fft                   1.0.6            py27h7dd41cf_0 
[conda] mkl_random                1.0.1            py27h4414c95_1 
[conda] pytorch                   1.0.0           py2.7_cuda10.0.130_cudnn7.4.1_1  [cuda100]  pytorch
[conda] torchvision               0.2.1                      py_2    pytorch

## Additional context

 complete list of packages
packages in environment:
 
Name                    Version                   Build  Channel
blas                      1.0                         mkl 
ca-certificates           2018.03.07                    0 
certifi                   2018.10.15               py27_0 
cffi                      1.11.5           py27he75722e_1 
cuda100                   1.0                           0    pytorch
freetype                  2.9.1                h8a8886c_1 
intel-openmp              2019.1                      144 
jpeg                      9b                   h024ee3a_2 
libedit                   3.1.20170329         h6b74fdf_2 
libffi                    3.2.1                hd88cf55_4 
libgcc-ng                 8.2.0                hdf63c60_1 
libgfortran-ng            7.3.0                hdf63c60_0 
libpng                    1.6.35               hbc83047_0 
libstdcxx-ng              8.2.0                hdf63c60_1 
libtiff                   4.0.9                he85c1e1_2 
mkl                       2018.0.3                      1 
mkl_fft                   1.0.6            py27h7dd41cf_0 
mkl_random                1.0.1            py27h4414c95_1 
ncurses                   6.1                  he6710b0_1 
ninja                     1.8.2            py27h6bb024c_1 
numpy                     1.15.4           py27h1d66e8a_0 
numpy-base                1.15.4           py27h81de0dd_0 
olefile                   0.46                     py27_0 
onnx                      1.3.0                     <pip>
openssl                   1.1.1a               h7b6447c_0 
pillow                    5.3.0            py27h34e0f95_0 
pip                       18.1                     py27_0 
protobuf                  3.6.1                     <pip>
pycparser                 2.19                     py27_0 
python                    2.7.15               h9bab390_4 
pytorch                   1.0.0           py2.7_cuda10.0.130_cudnn7.4.1_1  [cuda100]  pytorch
readline                  7.0                  h7b6447c_5 
setuptools                40.6.2                   py27_0 
six                       1.11.0                   py27_1 
sqlite                    3.25.3               h7b6447c_0 
tk                        8.6.8                hbc83047_0  
torchvision               0.2.1                      py_2    pytorch
typing                    3.6.6                     <pip>
typing-extensions         3.6.6                     <pip>
wheel                     0.32.3                   py27_0 
xz                        5.2.4                h14c3975_4 
zlib                      1.2.11               h7b6447c_3 

(opening this problem on behalf of a team member: @surajkota)
"
677,14078,0,"Build failing at torch/lib/c10d/ProcessGroupMPI.cpp. ## 🐛 Bug

PyTorch fails to finish building, with a possible bug (see below).



## To Reproduce


## Environment


 - PyTorch Version (e.g., 1.0): Master branch (1.0) 
 - OS (e.g., Linux): Debian:Stretch
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): see above
 - Python version: 3.6
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information: N/A

## Additional context

<!-- Add any other context about the problem here. -->
"
296,8837,0,"Inconsistency in implementation of _LRScheduler . I've noticed an odd behavior when attempting to write my own scheduler based on . 

If you write a custom get_lr() to work based on self.last_epoch, its impossible to differentiate the 
0th and the 1st epoch. 

Here is a minimal working example:



This results in the output



You can see the last epoch is asked to set the learning rate based on the last epoch being 0 twice. This LRScheduler class takes last_epoch as an argument, so it knows how to set the LR for the previous epoch. By default last_epoch=-1, because the first epoch is 0 and no epoch has run yet. On construction it then calls  with , which means the step function sets the learning rate for epoch 0. Then last_epoch is reset to -1 immediately after, so the next call to step also sets the learning rate for epoch 0. 

A fix would simply remove the + 1 from , but this might break existing implementations of  which wouldn't expect  being set to a negative number. 

I think a more intuitive implementation of this class might track the current epoch rather than the previous one. This would be a backwards incompatible change, but I think it would improve the overall quality of torch. I'm willing to give a re-implementation a shot if it sounds like a good idea to the maintainers. 

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @mruberry @jbschlosser @vincentqb"
282,20704,0,Remove unpack() in torch/csrc/nn/type_checks.h and its caller functions in the codebase. The  function in torch/csrc/nn/type_checks.h is not used anymore because we had already killed torch.legacy.nn and all the normal NN ops go through ATen bindings right now. A proper removal of the  function would also involve removing all of its caller functions.
637,24452,0,"model deployment ：Multiple serial models，using 3Dconv？？. ## ❓ Questions and Help

I have about 4 to 10 models to execute serially，every model is 3D-convolutional model，I have tried TensorRT,but TensorRT only support 2 D conv,
what else can I try?

The needs are: stable and fast

"
441,12133,0,"How to initialize caffe2::Tensor. ## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)

I don't know how to initialize  Tensor(const vector<int64_t>& dims,const vector<T>& values, BaseContext* context), cause I dont't know how to init context."
134,3124,0,"Does pytorch have Softmax3D ？. I am doing a 3d project and I found that pytorch has conv3d, pooling3d and batchnorm3d, but I didn't find the softmax3d. 
Does anyone have softmax3d layer? "
443,5918,0,Lint of .gitmodules and aten/.gitmodules are synchronized. see https://github.com/pytorch/pytorch/pull/5911
95,25878,0,"Model weights silently fail to load when model is on different gpu than when it was saved. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
 silently fails when the model is saved and loaded on different gpu-devices. As a result, the model remains in its randomly-initialized state.

This does not occur in PyTorch 1.1 but does occur in PyTorch 1.2.
 
## To Reproduce

Steps to reproduce the behavior:

The following code demonstrates this bug:

1. Define functions for creating a simple model. For loading the model and asserting that its loaded weights reflect the weights in the checkpoint



2. Create a model, place on gpu-0 and save its state-dictionary


3. Demonstrate that the model loads as-expected on gpu-0


4. Demonstrate that the model loads as-expected on cpu


5. Demonstrate that the model **fails to load** on gpu-1, and retains its randomly-initialized weights.


## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: Tesla V100-PCIE-16GB
GPU 1: Tesla V100-PCIE-16GB
GPU 2: Tesla V100-PCIE-16GB
GPU 3: Tesla V100-PCIE-16GB
GPU 4: Tesla V100-PCIE-16GB
GPU 5: Tesla V100-PCIE-16GB
GPU 6: Tesla V100-PCIE-16GB
GPU 7: Tesla V100-PCIE-16GB

Nvidia driver version: 418.67
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1

Versions of relevant libraries:
[pip] numpy==1.16.4
[pip] torch==1.2.0
[pip] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.0.2            py37h7b6447c_0  
[conda] mkl_fft                   1.0.14           py37ha843d7b_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] pytorch                   1.2.0           py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch
[conda] torchvision               0.4.0                    pypi_0    pypi
:

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519"
35,1253,1,"Memory usage increasing after first batch (possibly not freeing some potentially free memory). This may be related to issue #1184 but I believe there is something else to it:
I am working on version , using micro batches: split mini-batches into smaller batches and calculate/backprop loss in each micro batch, accumulating it over the mini batch. I noticed multiple times that my code goes out of memory after several micro batches. This seems strange to me as each micro batch should require the same amount of memory. Maybe there is something I don't fully understand here, so if anyone can explain it to me, I'd be happy !

Anyway, I tried to look into it a bit and attached is the smallest code snippet I came up with demonstrating my problem (around 100 lines).
Essentially, I am training a network on images, using micro-batches to reduce memory. However, after the first (and sometimes second) micro-batch, the memory usage increases and then stays the same after further micro-batches.

When using a functional approach for the mini and micro batches, the memory usage does not increase the same way, so I don't believe that this has to do with the fact that there may be 'lazy' memory freeing involved (freeing only when necessary).

[torch_memory.zip](https://github.com/pytorch/pytorch/files/919384/torch_memory.zip)"
496,16042,0,"grad_fn missing?. ## 🐛 Bug

Hi, Sorry if this is not a bug I tried to post in forums but am getting no response :( 

## To Reproduce

import torch

x = torch.randn(1,1,1,1).requires_grad_(True)

y = torch.nn.Conv2d(1,1,1)(x).mean()**2
conv_grad,  = torch.autograd.grad(y, x, create_graph=True, retain_graph=True)
print(""Conv grad"",conv_grad)

y = torch.nn.Linear(1,1)(x).mean()**2
linear_grad,  = torch.autograd.grad(y, x, create_graph=True, retain_graph=True)
print(""Linear grad"",linear_grad)

## Expected behavior

I think both gradient variables should have an associated grad_fn, however only the linear version does. Isn't the math the same for both here, the 1x1 convolution vs the 1x1 linear layer? Or maybe I am missing something :(
"
50,17703,1,"Training hangs when using DistributedDataParallel in two pod on two nodes . <!-- A clear and concise description of what the bug is. -->
Training hangs when using DistributedDataParallel in two pod on nodes.
The problem **occasionally** happens  and can not be reproduced each time. 
The pods were started by k8s using flannel network. The physical nodes are connected by 10Ge ethernet.

Training started normally, but hanged in the middle of training process.
![image](https://user-images.githubusercontent.com/41627739/53853400-4860ec80-4000-11e9-9929-678b7547f630.png)
Then there was no output again. The training process did not exit for servel hours.
We found that the GPU utilization  was always 100% on both two nodes.
![image](https://user-images.githubusercontent.com/41627739/53852922-a096ef00-3ffe-11e9-828c-6d38e8d5dd68.png)
After we added NCCL_DEBUG=WARN before the training command, we found the WARN as follows.
On on pod:
_worker0-0d9gnb:15:91 [3] include/socket.h 398 NCCL WARN Call to **write** failed : Connection reset  by peer.
worker0-0d9gnb:15:91 [3] transport.cu:153 NCCL WARN transport.cu:153 ->2 [Proxy thread error]_
On the other pod:
_worker0-0d9gnb:15:91 [3] include/socket.h 398 NCCL WARN Call to **recv** failed : Connection reset  by peer.
worker0-0d9gnb:15:91 [3] transport.cu:153 NCCL WARN transport.cu:153 ->2 [Proxy thread error]_
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Environment
Training code is pull from https://github.com/pytorch/examples/tree/master/imagenet.
Model is Resnet. We used 8 V100s to train (4 on each node).
Batch size was set 8, 32 or 128 per GPU.
The network rate is about 500 MB/s to synchronize the gradients.

 - PyTorch Version (e.g., 1.0): both 1.0 and 0.4.1
 - OS (e.g., Linux): Ubunut 16.04
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source): None
 - Python version: 3.6
 - CUDA/cuDNN version: CUDA 9.0/cuDNN 7.1.2
 - GPU models and configuration: V100 with NVLink
 - Any other relevant information:
"
682,5463,0,"ONNX export protobufs leak file paths from file system. This is bad for build reproducibility, since file paths may change between systems. The file paths come from SourceLocation which we started exporting in c7cb6a795e550ef2373d8127e120744db16b61cf

CC @jamesr66a "
100,19149,0,"Make operators like logsumexp and cumsum operate over dimension 0 by default (or at least for 1D arrays). It's pretty ridiculous that, for a 1D input like ,  still requires you to specify you're operating over dimension 0.

In particular  will error, saying that it needs a dim argument.  works.

My suggestion is that, at the very least for 1D inputs, the default dim you operate over is set to 0. This is a problem for  and , just to name two.

cc @jlin27 @mruberry @rgommers"
606,5486,0,"Tensors don't gracefully compare to NoneType. 
at "
652,4336,0,"Linking Error: ../libATen.so.1: undefined reference to functions from NNPACK and some other libs.... Hi, when I build the CPU version from source, I met the following issue. I am using Python 3.5, Ubuntu 16.04. Does anyone know how to fix it? Thanks a lot!

nnp_convolution_kernel_gradient'
> ../libATen.so.1: undefined reference to pthreadpool_create'
> ../libATen.so.1: undefined reference to nnp_convolution_inference'
> ../libATen.so.1: undefined reference to nnp_convolution_kernel_gradient'
> ../libATen.so.1: undefined reference to pthreadpool_create'
> ../libATen.so.1: undefined reference to nnp_convolution_inference'
> ../libATen.so.1: undefined reference to nnp_convolution_kernel_gradient'
> ../libATen.so.1: undefined reference to pthreadpool_create'
> ../libATen.so.1: undefined reference to nnp_convolution_inference'
> ../libATen.so.1: undefined reference to nnp_convolution_kernel_gradient'
> ../libATen.so.1: undefined reference to pthreadpool_create'
> ../libATen.so.1: undefined reference to nnp_convolution_inference'
> ../libATen.so.1: undefined reference to "
580,18873,0,Run shellcheck on OSS builds
170,19253,0,"torch.pow() in a script module produces an error. ## 🐛 Bug

The backward of torch.pow() in a traced module produces an error when the input is on cuda.

> RuntimeError:
Expected tensor to have CUDA Backend, but got tensor with CPU Backend (while checking arguments for CUDA_tensor_apply4) (checkBackend at ../aten/src/ATen/TensorUtils.cpp:202)

The backward is defined in torch/csrc/jit/symbolic_script.cpp.
I think the reason is that the backend of the first argument of torch.where() is always CPU.



## To Reproduce

The following model is the one in
https://github.com/pytorch/examples/blob/master/mnist/main.py
but I just inserted torch.pow().



## Environment
PyTorch version: 1.1.0a0+7e73783
Is debug build: No
CUDA used to build PyTorch: 9.2.88

OS: CentOS Linux release 7.5.1804 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
CMake version: version 3.12.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 9.2.88
GPU models and configuration:
GPU 0: Tesla P100-PCIE-16GB
GPU 1: Tesla P100-PCIE-16GB

Nvidia driver version: 396.26
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.15.4
[pip] numpydoc==0.8.0
[pip] torch==1.1.0a0+7e73783
[pip] torchvision==0.2.2.post3
[conda] blas                      1.0                         mkl
[conda] magma-cuda90              2.5.0                         1    pytorch
[conda] magma-cuda92              2.5.0                         1    pytorch
[conda] mkl                       2019.1                      144
[conda] mkl-include               2019.3                      199
[conda] mkl-service               1.1.2            py37he904b0f_5
[conda] mkl_fft                   1.0.6            py37hd81dba3_0
[conda] mkl_random                1.0.2            py37hd81dba3_0
[conda] torch                     1.1.0a0+7e73783           dev_0    <develop>
[conda] torchvision               0.2.2.post3              pypi_0    pypi

"
175,5826,0,"UserWarning: tensor1/other is not broadcastable to self, but they have the same number of elements. Falling back to deprecated pointwise behavior. - OS: Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-1038-aws x86_64)
- PyTorch version:  0.4.0a0+7f864bb (Source), and 0.3.1 (Pip).

- How you installed PyTorch (conda, pip, source): Both Pip and Source
- Python version: Python2 and Python3
- CUDA/cuDNN version: Cuda 9, cuDNN v7
- GPU models and configuration: Tesla K80        
- GCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)


When using the Pip version (Python2 & 3), I am getting these errors when using the  L-BFGS and Adam optimizers: 


Running optimization with ADAM:


 
Running optimization with L-BFGS:



This is my feval function: 



And this is how I run it: 




If I run the above code with the latest Github version, I get this error which happens in a completely different area: 




So I am not sure if the above error is still related to the Pip issues, if something got changed/broken, etc... "
386,10751,0,"[docs] Error in documentation for fft normalization. The primary fft equation in the documentation [here](https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5303) is 

However, from experimentation it seems the  1/N factor  should not be there for the forward transform. 

Here is the relevant line:
https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5303

The inverse transform equation (with the 1/N) is consistent with the behavior of the implementation.

Note, the documentation is accurate for , , but this only makes sense when if that 1/N factor isn't there in the main equation above. 

Note, the documentation for  is also accurate for , but again this only makes sense if the 1/N factor isn't there in the main equation above.

Lastly, the documentation for  is a mix of two problems. [This line](https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5493) says 

This says ""multiply"" instead of ""divide"" (as in fft). This is consistent with the primary equation, but it is not consistent with the implementation. It should be "
147,15916,0,"Where to download pytorch=0.2.0_4. I specifically need pytorch=0.2.0_4 to test a project built on this version (https://github.com/oawiles/X2Face) but can't find this anywhere on https://pytorch.org/get-started/previous-versions/, nor does  work. Is there any way I can install this specific version?"
515,23738,0,"Python 3.5 conda nightlies don't work. Reproducer in a Python 3.5 conda environment:

"
509,9987,0,"eigen submodule not found. Could you fix this submodule not found problem of caffe2?

remote: Repository not found.
fatal: repository 'https://github.com/RLovelett/eigen.git/' not found
fatal: clone of 'https://github.com/RLovelett/eigen.git' into submodule path '/home/ly/workspace/git/pose/caffe2/third_party/eigen' failed
Failed to clone 'third_party/eigen' a second time, aborting"
75,12683,1,"CPU triangular solve is slow. It would be nice to have a GPU version of triangular solve. It's needed for preconditioned stochastic gradient optimizer ([paper](https://ieeexplore.ieee.org/document/7875097/)), and it's possibly the reason why TF implementation runs 3x faster than PyTorch version

Here's a microbenchmark in milliseconds of 8000x8000 triangular solve:
https://github.com/yaroslavvb/newton/blob/master/benchmark_triangular.py



cc @VitalyFedyunin @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @ngimel"
695,907,0,"Fix cudnnHandle_t in Handles.cpp. There are two problems with the way we manage cudnnHandles in [Handles.cpp](https://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Handles.cpp):

1. We sometimes see SIGSEGV on exit when the handles are destructed
2. They don't work on non-default streams

We tried making the handles table thread-local, but that causes slow downs and deadlocks because the handles can be destructed after every forward pass in a data parallel module.

We should figure out a better solution."
711,4333,0,"Model behaves differently after saving and loading. Hi,

Recently I am working on a summarization project. During training, I saved the best model on the development set. However, loading the best model and testing again on the dev set gives me different ROUGE result (0.18218091939853281 -> 0.18217045231619222 ). Although the difference is small, it raises much concerns. And my colleagues told me that they have also encountered this issue (they observed about 2 points drop on their QA task). So I wrote a small [script](https://gist.github.com/magic282/94bdbb3b9ddef891d7aa40f0b0069a0f), and found that the parameters are identical, but the result is different after saving and loading.

And I also found this thread https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/21

The code on github gist runs on Windows (peterjc123’s 0.3.0 build). On linux (also 0.3.0 , I built a docker image myself by installing pytorch through conda) it raises an error that I don’t understand why:



The thread I posted https://discuss.pytorch.org/t/parameters-are-different-after-loading-model/11457"
94,17914,1,"Incorrect behaviour of min() and argmin(). ## 🐛 Bug

PyTorch's implementation of the argmin() function returns incorrect (maybe rather unexpected results) when using the argmin() and the min() function on tensors with dimensions where the minimal value appears multiple times. PyTorch picks the last(!) index with said value, whereas one would assume that the very first occurrence is reported. This is inconsistent with numpy's, Eigen's, C++ STL etc.

## To Reproduce

PyTorch



## Expected behavior

numpy



## Environment

PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Fedora release 27 (Twenty Seven)
GCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.1-6)
CMake version: version 3.11.2

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.14.6
[pip3] torch==1.0.0
[conda] Could not collect

## Additional context

See numpy's argmin() documentation: https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html
"
36,5351,1,"Wrong automatic gradient for Linear-Layer (Autograd/gradcheck). Minimal Example:

Output:

My Environment:
"
428,27255,0,"[JIT] list comprehensions over tensors does not work. ## 🐛 Bug

Following fails to compile:



List comprehensions over other iterables works; just not tensors.


cc @suo"
424,16288,0,"[JIT] Unable to append a tuple of tensor to a list. ## 🐛 Bug

JIT is unable to append a  to a 

## To Reproduce



"
474,6472,0,"Advanced indexing doesn't validate negative indexes (regression). Minimum example:

    torch.zeros(10)[torch.LongTensor([-11])] # returns bogus result on 0.4, error on 0.3.1

These all fail as they should:

    torch.zeros(10)[10]
    torch.zeros(10)[-11]
    torch.zeros(10)[torch.LongTensor([10])]

Works correctly on 0.3.1.post2.
Doesn't work on 0.4.0a0+df039e2.

Compiled with:
"
162,3981,0," Errors when computing second order gradients on gpu. Here is a piece of test code:



If the following conditions meet, the second order gradient cannot be computed:

1. you want to compute second order grad (not first order grad)
2. you network contains convolution (fc layer functions normally)
3. Computation on gpu (on cpu, everything was fine)

And the error is:

Add    won't fix anything."
417,20579,0,"Exception when adding custom scalars in tensorboard. ## 🐛 Exception when adding custom scalars in tensorboard

An exception is raised when adding custom scalars using 



## To Reproduce
Minimal code to reproduce:


Running this results in:


## Environment

Please copy and paste the output from our


## Additional context

The exception message is quite clear, the required type is  but we have a , this happens [here](https://github.com/pytorch/pytorch/blob/09f22d10a695bfba8ffb3327b9920fd3358c00ee/torch/utils/tensorboard/summary.py#L381):


Should be enough to remove the surrounding square brackets, PR is coming."
184,7857,0,"Feature Request: Logistic Distribution. Could we have logistic distribution in PyTorch?

https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/distributions/python/ops/logistic.py

cc @fritzo @neerajprad @alicanb @nikitaved @brianjo @mruberry"
203,6932,0,"Exception NameError: ""global name 'FileNotFoundError' is not defined"". 

This error is coming while implementing pytorch faster-rcnn repository. Any solutions please??"
319,24915,0,"Shared Dataset Functionality. ## 🚀 Feature

We want to build a unified data pipeline interface that offers building blocks for others to build on with the following objectives:
* Standardize datasets across domains.
* Offer flexible building blocks that can be combine to obtain other datasets.
* Enable datasets that do not fit in memory.
* Share code among domains.
* Facilitate parallel loading and processing of data.
* Decouple data loading and preprocessing/transformation.
* Offer static typing for datasets

## Motivation

* The Domains currently each have their own non-standard dataset structure that may also download the data. This duplicate efforts and adds complexity to the user.
* A common bottleneck when generating datasets is reading the data. We want to offer an interface that enables reading the data and running initial preprocessing while maximizing available computing resources utilization.
* We may want to leverage specialize libraries such as NVIDIA DALI.

## Additional Information

* [torch.utils.data](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/)
* [tf.data](https://www.tensorflow.org/beta/guide/data) (e.g. uses dictionary for data point iteration)
* fast.ai's [basic_data](https://docs.fast.ai/basic_data.html) and [data_block](https://docs.fast.ai/data_block.html)
* [tnt](https://github.com/pytorch/tnt/blob/master/torchnet/dataset/dataset.py)
* [torchnet](https://github.com/torchnet/torchnet/tree/master/dataset)
* [~~torchdata~~](https://pypi.org/project/torchdata/)

Datasets:
* pytorch/text#624 pytorch/text#610 pytorch/audio#303 new datasets in domains
* pytorch/vision#1193 wants to select which metadata to return
* Internal: [overview](https://fb.quip.com/vlWwA35cmq0t) [torchtext](https://fb.quip.com/LncwAsC1cUZt) [core](https://fb.quip.com/B0PeACndlZEE) [torchvision](https://fb.quip.com/WGsUApsce6xN)
* [safe datasets](https://github.com/msamogh/nonechucks)

Dataloader:
* [torchaudio background iterator](https://github.com/pytorch/audio/blob/master/torchaudio/datasets/utils.py#L314)
* #24915 wants to re-use worker processes
* [FastDataLoader](https://github.com/pytorch/pytorch/issues/15849#issuecomment-573921048)
* [python 3.8 shared memory](https://docs.python.org/3/library/multiprocessing.shared_memory.html)
* Internal: [torchdata](https://fb.quip.com/ekJJAsYqMG7X) [gil](https://docs.google.com/document/d/1InJP79dWTIYj-xGVU65Y2r-K2HeL6t2l1xGDfKTU4Rw/edit#) [experiment](https://fb.quip.com/imVLAOdyJfAI) [DataLoader+Iterable](https://fb.workplace.com/groups/2162019300778793/permalink/3398854433474998/)

Features:
* #12672 wants to move collate_fn functionality to datasets
* #26547 wants distributed random sampling
* #28743 for sampler for iterable datasets
* pytorch/vision#1315 wants to apply an instance of random transform sequence to many images

cc @SsnL @fmassa @zhangguanheng66 @vincentqb @mrshenli "
211,6164,0,"feature requests and future roadmap. PyTorch Feature Requests and Future RoadMap
--------------------------------
Hello everyone and thank your for this great project. I would like to point to some things that I find important for the future growth of pytorch. 

1. Easy name conventions throughout the library. I believe that some things should be renamed so that it resonates and are easier to remember. This will also lower the entry bar for new people exposed to pytorch.

Example:


The  or  doesn't mean anything to me and as soon as I use them my brain has already forgotten about it. A more sane naming good be  that seems to be a universal convention pretty much since other major libraries are using it.

I would also love to see something like ,  , , , , etc.

2. Model dissection. 
More often we find ourselves in a scenario where we would like to get the output of a complex model at different levels.

Example:


3. Dataset preprocessing and transformation pipeline switch between cpu/gpu
Many times we find ourselves the need to do the data preprocessing on the gpu especially in the scenario where there is an abundance. But the choice at the moment is inexistent. There should be a flag for the user to easily switch between cpu and gpu upon request in order to chose where the data preprocessing should take place.

4. Addition of negative indexing in Tensors.

5. Integration and applicability with already pre-existing tools.
Some tools that already pre-exist and are nice to have an out of the box support and integration with them. For instance tensorboard is great but at the moment we need to rely on third party developed plugins for this to work. Instead it would be better if pytorch had the ability to dump logs that can be consumed by tensorboard directly. 

5. Serving models is completely inexistent at the moment of writing.

For anyone else please feel free to add anything else you might think is important for future directions.
Basically I just watched the whole tf dev summit and the one advantage that pytorch might had against tf was the dynamic models but now that's gonna with eager mode. And since folks at tf have faster release cycles they can fail and iterate faster on concepts and ideas. A lot of people where nagging about the language choices which where horrific but since then they have iterated over a lot and brought it up to a sane level .

These things make me wonder about the future directions and where pytorch is heading?


Thank you!
"
572,15138,0,"Build simple c++ example-cpp using Libtorch fails on arm with undefined reference to c10::Error::Error. ## 🐛 Bug

<!-- After building PyTorch from source, to generate libs required to run https://pytorch.org/cppdocs/installing.html on arm, building example-cpp fails with following error. -->

## To Reproduce

Steps to reproduce the behavior:

1.Build PyTorch from source.

2.Replace libs folder in Libtorch (https://download.pytorch.org/libtorch/nightly/cu90/libtorch-shared-with-deps-latest.zip) with the pytorch/build/lib (lib folder created on arm after building pytorch).

3.build the example-cpp 

## Error Message

>  nvidia@tegra-ubuntu:~/uvaidya/LibTorch/example-app/build$ make
> [ 50%] Linking CXX executable example-app
> CMakeFiles/example-app.dir/example-app.cpp.o: In function c10::Error::Error(c10::SourceLocation, std::string const&)'
> example-app.cpp:(.text._ZN3c106DeviceC2ENS_10DeviceTypeEs[_ZN3c106DeviceC5ENS_10DeviceTypeEs]+0x1bc): undefined reference to at::TensorOptions::device() const':
> example-app.cpp:(.text._ZNK2at13TensorOptions6deviceEv[_ZNK2at13TensorOptions6deviceEv]+0x30): undefined reference to at::TensorOptions::requires_grad() const':
> example-app.cpp:(.text._ZNK2at13TensorOptions13requires_gradEv[_ZNK2at13TensorOptions13requires_gradEv]+0x38): undefined reference to c10::impl::getDeviceGuardImpl(c10::DeviceType)':
> example-app.cpp:(.text._ZN3c104impl18getDeviceGuardImplENS_10DeviceTypeE[_ZN3c104impl18getDeviceGuardImplENS_10DeviceTypeE]+0x11c): undefined reference to torch::autograd::make_variable(at::Tensor, bool)':
> example-app.cpp:(.text._ZN5torch8autograd13make_variableEN2at6TensorEb[_ZN5torch8autograd13make_variableEN2at6TensorEb]+0xbc): undefined reference to torch::jit::SourceRange::highlight(std::ostream&) const':
> example-app.cpp:(.text._ZNK5torch3jit11SourceRange9highlightERSo[_ZNK5torch3jit11SourceRange9highlightERSo]+0x1c0): undefined reference to c10::Error::Error(c10::SourceLocation, std::string const&)'
> example-app.cpp:(.text._ZNK5torch3jit11SourceRange9highlightERSo[_ZNK5torch3jit11SourceRange9highlightERSo]+0x41c): undefined reference to c10::Error::Error(c10::SourceLocation, std::string const&)' follow
> CMakeFiles/example-app.dir/example-app.cpp.o: In function c10::Symbol::fromQualString(std::string const&)'
> CMakeFiles/example-app.dir/example-app.cpp.o: In function c10::Error::Error(c10::SourceLocation, std::string const&)'
> collect2: error: ld returned 1 exit status
> CMakeFiles/example-app.dir/build.make:99: recipe for target 'example-app' failed
> make[2]: *** [example-app] Error 1
> CMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/example-app.dir/all' failed
> make[1]: *** [CMakeFiles/example-app.dir/all] Error 2
> Makefile:83: recipe for target 'all' failed
> make: *** [all] Error 2


## Environment

Collecting environment information...
PyTorch version: 1.0.0a0+db5d313
Is debug build: No
CUDA used to build PyTorch: 9.2.78

OS: Ubuntu 16.04 LTS
GCC version: (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.12.3

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.7.1.2
/usr/lib/aarch64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect



"
721,3798,0,"Can't Install from source if path contains a space. Hi everyone,

silly ""gotcha"" installing the latest master from source (on ubuntu 16)- if there is a space in the path you install from, it fails to compile the ""compiler simple test"", and throws some fairly obscure error messages about cc being an unrecognised compiler.

Perhaps just needs a line in the instructions, as it took digging through the compiler logs to find the issue (unless i missed something!).

Apologies if duplicated, but couldn't find a relevant issue in a quick search. Happy to contribute fix to docs etc if required."
477,16045,0,"Bug: fail to throw error when computing loss between tensors with shapes [n, 1] and [n]. ## Issue description
When computing the loss between  and  with shapes such as  and , an error should be thrown.  Indeed, my colleagues tell me this was the case in version 0.4, but lost in version 1.0.

## Code example

If you leave y.reshape commented out, the correct estimates of this straightforward linear regression problem are not found and the loss fails to decrease into a region near the global optimum of this convex problem.  Yet, no error is raised.

If you uncomment y.reshape, the correct estimates are found and the loss decreases in the anticipated way.


## System Info
Collecting environment information...
PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.5
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] Could not collect
[conda] blas                      1.0                         mkl  
[conda] mkl                       2017.0.3                      0  
[conda] torch                     1.0.0                     <pip>
[conda] torchvision               0.2.1                     <pip>
"
741,6384,0,"[Caffe2] [PyTorch] PyTorch to Caffe2 and Mobile using ONNX Tutorial: Segmentation Fault. Hello, 

I am trying to run [Transfering a model from PyTorch to Caffe2 and Mobile using ONNX](https://github.com/onnx/tutorials/blob/master/tutorials/PytorchCaffe2SuperResolution.ipynb) tutorial. However, I get segmentation fault error.

I think the segmentation error is triggered by the second part of the tutorial where loading ONNX model into Caffe2. While searching for what might cause this error I noticed that this tutorial has 2 different versions: one in Onnx Github Tutorial and the other in [PyTorch](http://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html) website. The onnx to caffe2 import for the two tutorials were different for the second part along with the implementation. One uses : import caffe2.python.onnx.backend and other import onnx_caffe2.backend. I tried with both tutorial but I get the same segmentation fault in both. Also, when I comment the onnx to caffe2 tutorial and the related import lines, the first part Pytorch to ONNX seems to work without any error.

Below, I will describe how I create my virtual env and the results from gdb after getting the Segmentation Fault:

Dependencies:
cuda/8.0 
cudnn/v6.0
opencv/3.4.1
nccl/2.0.5
caffe2/2018-03-02 


To run the code I used the following command (with 6GB RAM, 1 GPU with 6GB): 


Also, I upgraded the numpy and pyyaml versioons with:


Otherwise, I get:


After upgrading the numpy version, when I run the code I get the Segmentation Fault:


I tried to run the code in gdb as well to see where exactly the fault occurs:




In both tutorial, the segmantation fault occurs in the same place. I don't know how to fix this error. I didn't change anything in the code. I am not sure why I am getting this error. 

I really appreciate if you can help me to fix it.


**Note: I already opened an [issue ](https://github.com/onnx/tutorials/issues/29) on onnx repo but I want to cross-reference the issue since I noticed that I get the same error in [PyTorch Tutorial](http://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html) as well.** 





"
497,23823,0,"pytorch cannot be installed under Windows 10 if Python 3.7 was installed from Microsoft Store. ## Issue description

pytorch 1.1.0 cannot be installed under Windows 10 if Python 3.7 was installed from Microsoft Store.

The reason is a combination of
a) long Python installation path used by Microsoft Store installer
b) long filename path used within pytorch for collect_and_distribute_fpn_rpn_proposals_op_test.test_collect_and_dist.zip
c) by default, Windows 10 still refuses paths longer than 256 characters

## Code example


fails with an error message:

> 

The error is caused by the 256 character limit on Windows paths. With a file and pathname so long, no matter how short <username> is the path is 257 characters or longer.

## System Info
PyTorch version: 1.1.0
OS: Microsoft Windows 10 Pro, Version 1903 
Python version: 3.7

Versions of relevant libraries:
[pip3] numpy==1.17.0
[pip3] torch==1.1.0
[pip3] torchvision==0.3.0
"
189,25512,0,"Can't import torch after installing with Conda on Ubuntu. ## 🐛 Bug

I installed CUDA and CUDNN via the following tutorial: (https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html), then,
I installed pytorch on Ubuntu 18.04 according to the pytorch website, selecting stable (1.2), Linux, Conda, Python3.7, CUDA10.0, using a virtualenv.  Then, when I go to import:

>>> import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'torch'

I rebooted, made sure my PATH variable included anaconda:
 /home/dan/anaconda3/bin:/home/dan/anaconda3/envs/torch_1-2-0/bin:/home/dan/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda-10.1/bin:/usr/local/cuda-10.1/bin

I'm sure that it is pointing to the correct Python: alias python='/home/dan/anaconda3/bin/python'

Upon seeing some advice in a different ticket to ""conda upgrade cudatoolkit"", I did that, and didn't work.

I then decided maybe since my CUDA version is 10.1, I would alter the install command (conda install pytorch torchvision cudatoolkit=10.0 -c pytorch) to say cudatoolkit10.1.  That rolled pytorch back to 1.0, CUDA to 9.0, and generally didn't work, so I put everything back after that.

 - PyTorch Version (e.g., 1.0): 1.2
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (, , source): conda install pytorch torchvision cudatoolkit=10.0 -c pytorch (from the correct virtualenv)
 - Python version: 3.7.3, Conda version 4.6.11
 - CUDA/cuDNN version: 10.1 / 7.6.3
 - GPU models and configuration: NVIDIA 2080 TI RTX
 - Any other relevant information:  conda list:
# Name                    Version                   Build  Channel
cudatoolkit               10.1.168                      0  
pytorch                   1.2.0           cpu_py37h00be3c6_0  
torchvision               0.4.0           cuda100py37hecfc37a_0

Any help would be greatly appreciated."
420,22135,0,[dataloader] type annotation is broken. In #19228 I did not update the type annotations. We should fix this.
317,4181,0,"Fused RNN refactor plan. This ticket is to track our plan to refactor the fused RNN API (which makes use of the CuDNN implementation of RNNs).

**Why is this difficult?** There are a number of factors which make fused RNNs unusual, compared to most of the other differentiable operations in PyTorch

* It requires an unusually large, structured series of weights. Most differentiable operators have a fixed number of weights, but an RNN for an entire sequence must have weights for every layer of the RNN. To make matters worse, each layer needs not one but two tensors; the weight and the bias. No other operator in PyTorch behaves like this.

* CuDNN requires these weights and inputs to be packed in a particular way.  The required packing operations are frequently reported by users as an extremely confusing aspect of PyTorch.

* Not only do the weights vary depending on the type of RNN, so do the hidden tensors. If you are an LSTM, you have both hx and cx; for other RNNs, only hx is needed.

* RNN with dropout is a stateful API, which requires dropout descriptors to be passed between invocations to handle randomness.

**What do we want to do?** Here are the desired goals of the RNN refactor:

* Make CuDNN RNN available from ATen

**Design ideas.**

* The ATen API will take two tensors,  and , with  being undefined tensor for non-LSTM networks.

cc @csarofeen @ptrblck"
239,9356,0,"RuntimeError: cuda runtime error (8) : invalid device function at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THCUNN/generic/Threshold.cu:34. [forums](https://discuss.pytorch.org/).

If you are submitting a bug report, please fill in the following details.
I am using linux machine with ubuntu in it . 
Ubuntu Version 16.04 lts
NVIDIA  930mx
Nvidia driver version :  384.184

## Issue description
RuntimeError: cuda runtime error (8) : invalid device function at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THCUNN/generic/Threshold.cu:34
Provide a short description.

## Code example
 This code is is for the segnet , for detection of Vessel in retina.

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source): conda
- Build command you used (if compiling from source):
- OS:  Ubuntu 16.04
- PyTorch version: 0.3.1u
- Python version: 3.6
- CUDA/cuDNN version:7.5
![issue_pytorch](https://user-images.githubusercontent.com/24300927/42598377-a3d0db30-8579-11e8-9db3-886d6a9e498c.png)

- GPU models and configuration:
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
"
47,3869,1,"Why the Dropout2d and BatchNorm2d's model.eval() result is poor. Recent I'm doing a project using the elegant tool,pytorch. I use the dropout2d and batchnorm2d in my network.The trained model performance well in the test phase if I didn't using the model.eval(). However,when I use the model.eval(), the result is very pool. I really feel  confused.I sincerely hope the developer can help me solving this confusion. Thank you very much!"
165,4403,0,Pytorch docs：RNN weight_ih_l[k]  shape error. In pytorch docs，torch.nn.RNN parts show that variable **weight_ih_l[k]**  has shape of input_size x hidden_size，however it should be hidden_size x input_size.
8,31295,1,"nll_loss with weights: reduction 'mean' gives wrong result . ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:




1. on cpu


2. on gpu

tensor([1, 0, 4], device='cuda:0')

>>> reduction = ['none', 'sum', 'mean']
>>> for r in reduction:
...        m = F.log_softmax(i, dim=1)
...        loss = F.nll_loss(m, target, w, reduction=r)

none :  tensor([-0.0455,  0.1291,  0.8693], device='cuda:0', grad_fn=<NllLossBackward>)
sum :  tensor(0.9530, device='cuda:0', grad_fn=<NllLossBackward>)
mean :  tensor(2.3681, device='cuda:0', grad_fn=<NllLossBackward>)
>> loss = F.nll_loss(m, target, w, reduction='none')>> loss.mean()mean :  tensor(0.8193, grad_fn=<MeanBackward0>)`

<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.0.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration: 
GPU 0: TITAN X (Pascal)
GPU 1: TITAN X (Pascal)

Nvidia driver version: 430.50
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip3] numpy==1.13.3
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.3.0            py36he904b0f_0  
[conda] mkl_fft                   1.0.14           py36ha843d7b_0  
[conda] mkl_random                1.1.0            py36hd6b4f25_0  
[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchvision               0.4.2                py36_cu101    pytorch


cc @ezyang @gchanan @zou3519"
357,20117,0,"[FR] [RFC] add Sequential.append & .extend. A common pattern people use in  of a  is to build a  first and then feed it into  because  doesn't support many handy methods existing on . E.g., 



This is totally unnecessary if we can just provide  and  on ."
708,7722,0,"Error in backprop when using frozen LSTM layers (new with 0.4.0). ## Issue description

The general situation is that we have a pretrained Language Model, and during a first phase we only want to train the new embedding layer we added before fine-tuning the whole thing. This worked fine in 0.3 but now sends an error message during back propagation. Minimal reproduction is to just create a simple model with a linear layer and an LSTM, freeze this second layer (by applying require_grads=False to its parameters) and try to compute a back-propagation.

## Code example

See [here](https://github.com/sgugger/Deep-Learning/blob/master/Bug%20with%20frozen%20LSTM%20layer.ipynb)

## System Info
PyTorch version: 0.4.0                                                                                                  
Is debug build: No                                                                                                      
CUDA used to build PyTorch: 9.0                                                                                                                                                                                                                 OS: Microsoft Windows 10 Home                                                                                           
GCC version: Could not collect                                                                                         
CMake version: Could not collect                                                                                                                                                                                                                Python version: 3.6                                                                                                     
Is CUDA available: Yes                                                                                                  
CUDA runtime version: 9.1.85                                                                                            
GPU models and configuration: Could not collect                                                                         
Nvidia driver version: Could not collect                                                                                
cuDNN version: Could not collect                                                                                                                                                                                                                Versions of relevant libraries:                                                                                         
[pip] Could not collect                                                                                                 
[conda] Could not collect

Pytorch was installed with conda, the bug also appears on my linux instances.

Thanks for your help!"
86,12238,1,"Caffe2 is compiled without optimization passes.. ## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

Hi , I've compiled caffe2 for cpu inference on python 3, but I get the following warning  : 


so my question is how can I compile caffe2 with optimization passes ? 

~I've also noticed that during inference the model uses only one core of my cpu for inference , but I've compiled caffe2 with NNPACK enabled wich should make it use multi core~ (my bad i didn't set the correct engine to use for the convolution ops , that's why NNPACK wasn't used) 

The model used is faster-rcnn-FPN-101 converted to .pb from detectron, inference time reported on  cpu  for image of size 800x800 is 5.8 seconds 

thanks :)
"
503,8867,0,"[JIT] Normalize representation of traced and scripted functions/modules . Currently we have a mix of actual types when we script/trace a function/module:






This causes issues, for example, since we can only inline a  into a ScriptModule if it's an instance of GraphExecutor(https://github.com/pytorch/pytorch/blob/2b926aafb09575dd83ebf4fe2a76cbe239596f0b/torch/csrc/jit/script/init.cpp#L82). This causes problems in the case of calling Script/Traced modules from script functions -- they show up as s.

Let's make it so that all of {script,traced} {function,module} produce a Module, and re-work the inlining logic to merge modules in all cases. This will greatly simplify the logic and make things easier to understand and maintain"
129,3131,0,"cuda() methods on Module and Tensor/Variable are inconsistent. On , the keyword argument is named , but on Tensor/Variable, the keyword argument is just ."
249,16437,0,"cuDNN error when using half precision convolution. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

When using half precision convolution on PyTorch nightly (20190125), with torch.backends.cudnn.deterministic = True, CUDNN_STATUS_BAD_PARAM runtime error occurs

I found that when using latest PyTorch 1.0, or using fp32, or use torch.backends.cudnn.deterministic = False then this script runs without problem.

## To Reproduce

Steps to reproduce the behavior:





<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Run without error
<!-- A clear and concise description of what you expected to happen. -->

## Environment

`

PyTorch version: 1.0.0.dev20190125
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: Tesla V100-PCIE-32GB
Nvidia driver version: 410.79
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect"
171,13574,0,"dist.all_reduce([tensor]) throws confusing error message. 

It seems internally the input is wrapped in another list, but this error is confusing."
196,17882,0,"ProcessGroupNCCL error/timeout handling. As of NCCL 2.4 there are functions to detect I/O errors and abort running kernels. This is required to implement timeouts and force workers to raise an error or terminate when other workers fail.

See https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/api/comms.html and  and  and https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/ for an example."
749,15920,0,"How to let Batchnorm to use global mean in both training and inferance?. ## ❓ Questions and Help
**Observation:** 
(1) If I use the default bn layer and use model.eval() before infence.  Test result has a significant discrepancy between test and other test.
(2) If a set momentum to 0.0 and 0.1 in different tests. The results are also different. Is this discrepancy designed to be True? It doesn't make sense.
**Guess:** 
I guess this is because bn layer uses different mean and var in different tests. And model is senstive to these mean/var changes?
**Question:** 
(1) Is My guesses correct? If correct, I can I always use global mean and global var so that the result is same in both training and eval? 
(2) Is these bn parameters designed to changes with time? So different inference sequence causes diffferent result?
**Tried:**
 I tried set momentum to 0 or track_running_stats=False after load the model. Both doesn't work.
"
113,17632,0,"Caffe2 C++ script for classification/object_detection with CMakeLists.txt . ## ❓ Questions and Help

I was looking for caffe2 c++ script that supports latest version. But I couldn't find any good resource. Can anyone provide caffe2 c++ example script for classification. 

Thanks. "
452,11076,0,"[caffe2] testing, please ignore. If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

Provide a short description.

## Code example

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source):
- Build command you used (if compiling from source):
- OS:
- PyTorch version:
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
"
46,4893,1,"GPU Softmax over last dimension of 3D tensor is slow. Recently I profiled one of my models with  and was surprised to find that the softmax layers for an attention mechanism were the most expensive entries in the  column.

My original code looked something like this:


After some experimentation, I tried adding some transposes to the code:



This increased my overall model speed by around 10%, and now matrix multiplication is at the top of  (which is what I would expect).

Could this be considered a performance bug? I wonder if there is some way for the softmax cuda code to have comparable speed regardless of the softmax dimension.

(Sorry I don't have sample code at the moment; my actual code is deeply embedded in the current project I'm doing.)

System info
- OS: Linux
- PyTorch version: 0.3.0
- How you installed PyTorch (conda, pip, source): conda
- Python version: 3.6
- CUDA/cuDNN version: CUDA 9
- GPU models and configuration: K80"
530,17531,0,"JIT graph to ONNX Loop cond variable should be tensor(bool). ## 🐛 Bug

ONNX Loop produced by  and  does not conform to ONNX loop spec: The second input to ONNX loop--the conditional variable --should be , but I got . 

## To Reproduce

Run the following script (environment details below):



produces the following ONNX program:



 is tensor of type  (see the program print out). However,  is the second input to , which is required to be  by the [ONNX spec](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Loop). 

## Environment
PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.14.1
GCC version: Could not collect
CMake version: version 3.13.4

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.16.1
[pip3] torch==1.0.1.post2
[conda] Could not collect

"
135,12510,0,"[feature request] fix Windows cmake scripts to deal with \backlashes in paths . ## Issue description

Getting a bunch of Invalid escape sequence \N while compiling from source.
Coming from paths with unescaped backslashes in them.
Trying to resolve by myself in the meantime but its a real pain

## Code example

Invalid escape sequence \N                                                                                                                                                                                                                                                                                                                                                                                    Policy CMP0010 is not set: Bad variable reference syntax is an error.  Run                                                                                                                             ""cmake --help-policy CMP0010"" for policy details.  Use the cmake_policy                                                                                                                                command to set the policy and suppress this warning.                                                                                                                                                 This warning is for project developers.  Use -Wno-dev to suppress it.                                                                                                                                                                                                                                                                                                                                         CMake Warning (dev) at caffe2_gpu_generated_THCStorageCopy.cu.obj.Release.cmake:178 (execute_process):                                                                                                   Syntax error in cmake code at                                                                                                                                                                                                                                                                                                                                                                                   C:/Users/user/pytorchrc1/pytorch/build/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCStorageCopy.cu.obj.Release.cmake:178                                                                                                                                                                                                                                                      when parsing string                                                                                                                                                                                                                                                                                                                                                                                             C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin/nvcc.exe;-M;-D__CUDACC__;C:/Users/user/pytorchrc1/pytorch/aten/src/THC/THCStorageCopy.cu;-o;C:/Users/user/pytorchrc1/pytorch/bui   ld/caffe2/CMakeFiles/caffe2_gpu.dir/__/aten/src/THC/caffe2_gpu_generated_THCStorageCopy.cu.obj.NVCC-depend;-ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.11.   25503/bin/Hostx64/x64;-m64;-Dcaffe2_gpu_EXPORTS;-DCAFFE2_BUILD_MAIN_LIB;-DNOMINMAX;-D_CRT_SECURE_NO_DEPRECATE=1;-DUSE_MSC_ATOMICS=1;-DTH_BLAS_MKL;-D_OPENMP_NOFORCE_MANIFEST;-Xcompiler;,""/wd4267"",""   /wd4251"",""/wd4522"",""/wd4522"",""/wd4838"",""/wd4305"",""/wd4244"",""/wd4190"",""/wd4101"",""/wd4996"",""/wd4275"",""/EHa"",""-DONNX_NAMESPACE=onnx_torch"",""-openmp"",""/MP"",""/bigobj"",""-DHAVE_AVX_CPU_DEFINITION"",""-DHAV   E_AVX2_CPU_DEFINITION"",""/MD"",""/O2"",""/Ob2"",""/MP"",""/bigobj"";-DONNX_NAMESPACE=onnx_torch;-gencode;arch=compute_30,code=sm_30;-Xcudafe;--diag_suppress=cc_clobber_ignored;-Xcudafe;--diag_suppress=integ   er_sign_change;-Xcudafe;--diag_suppress=useless_using_declaration;-Xcudafe;--diag_suppress=set_but_not_used;-Xcompiler;-MD;--expt-relaxed-constexpr;--expt-extended-lambda;-Xcompiler;/wd4819;-Xcomp   iler;/wd4503;-Xcompiler;/wd4190;-Xcompiler;/wd4244;-Xcompiler;/wd4251;-Xcompiler;/wd4275;-Xcompiler;/wd4522;-Wno-deprecated-gpu-targets;--expt-extended-lambda;-gencode;arch=compute_30,code=sm_30;-   DCUDA_HAS_FP16=1;-D__CUDA_NO_HALF_OPERATORS__;-D__CUDA_NO_HALF_CONVERSIONS__;-D__CUDA_NO_HALF2_OPERATORS__;-DNVCC;-IC:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/include;-IC:/Users/yacin   e/pytorchrc1/pytorch/aten/src;-IC:/Users/user/pytorchrc1/pytorch/build;-IC:/Users/user/pytorchrc1/pytorch;-IC:/Users/user/pytorchrc1/pytorch/third_party/protobuf/src;-IC:/Program Files (x86)   /IntelSWTools/compilers_and_libraries/windows/mkl/include;-IC:/Users/user/pytorchrc1/pytorch/cmake/../third_party/eigen;-IC:/Users/user/pytorchrc1/pytorch/cmake/../third_party/pybind11/include   ;-IC:/Users/user/pytorchrc1/pytorch/cmake/../third_party/cub;-IC:/Users/user/pytorchrc1/pytorch/third_party/onnx;-IC:/Users/user/pytorchrc1/pytorch/build/third_party/onnx;-IC:/Users/user/p   ytorchrc1/pytorch/build/caffe2/aten/src/TH;-IC:/Users/user/pytorchrc1/pytorch/aten/src/TH;-IC:/Users/user/pytorchrc1/pytorch/aten/src/THC;-IC:/Users/user/pytorchrc1/pytorch/build/caffe2/aten   /src/THC;-IC:/Users/user/pytorchrc1/pytorch/aten/src/THCUNN;-IC:/Users/user/pytorchrc1/pytorch/aten/src/ATen/cuda;-IC:/Users/user/pytorchrc1/pytorch/build/caffe2/aten/src;-IC:/Users/user/p   ytorchrc1/pytorch/build/aten/src;-IC:/Users/user/pytorchrc1/pytorch/aten/src/THNN;-IC:/Users/user/pytorchrc1/pytorch/aten/../third_party/catch/single_include;-IC:/Users/user/pytorchrc1/pytor   ch/build/caffe2/aten/src/ATen;-IC:/Users/user/pytorchrc1/pytorch/aten/src/ATen/..;-IC:\NVIDIA\CUDA\9.0\cudnn-v7.3\include                                                                                                                                                                                                                                                                                     Invalid escape sequence \N                                                                                                                                                                                                                                                                                                                                                                                    Policy CMP0010 is not set: Bad variable reference syntax is an error.  Run                                                                                                                             ""cmake --help-policy CMP0010"" for policy details.  Use the cmake_policy                                                                                                                                command to set the policy and suppress this warning.                                                                                                                                                 Call Stack (most recent call first):                                                                                                                                                                     caffe2_gpu_generated_THCStorageCopy.cu.obj.Release.cmake:203 (cuda_execute_process)                                                                                                                  This warning is for project developers.  Use -Wno-dev to suppress it.                                                  

## System Info

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): python setup.py build -f
- OS: Microsoft Windows 10 Pro x64 Version	10.0.17134 Build 17134
- PyTorch version: 
- Python version: 3.6.5
- CUDA/cuDNN version: 9.0patch4/7.3
- GPU models and configuration: Nvidia Quadro K2100M
- GCC version (if compiling from source): N/A
- CMake version: the one from VS as described in compilation instruction
- Versions of any other relevant libraries:
"
584,3567,0,"undefined symbol: cudnnSetConvolutionGroupCount while running with cuDNN 7.0.3 and CUDA 9. Installed from source, tried various branches _(master, v0.3.0, soumith-patch-1)_, but still getting this error.
Full uninstall and reinstall didn't help either. 
Calling  (form torchvision module) works fine, creating variables on the device doesn't seem to provoke this error either.
I've also just ran  and got this [log](https://gist.github.com/VladislavZavadskyy/8b1a5abffe6298fe2908fe0e3bd6cbf9), which suggests that it searches for cudnn 6, but I have no idea why or how to fix it."
202,17560,0,"Please test and add the AdaBound optimizer into the next stable release. Now we have another powerful general purpose optimizer called AdaBound that trains as fast as Adam and as good as SGD, see [https://github.com/Luolc/AdaBound](https://github.com/Luolc/AdaBound).

It appears to work better than Adam in several tasks including NLP and CV, with slightly higher accuracy and smoothier learning curve.

![image](https://user-images.githubusercontent.com/10172392/53534724-08e75b80-3b3b-11e9-87b7-debe21b3908e.png)

I think this would be a great and useful addition, thanks!"
180,14373,0,[c10d] Barrier to synchronize with previously kicked off work in ProcessGroupGloo. In #14294 there is a failing test for this behavior (and it is therefore skipped). The stack that that PR is a part of removes deprecated code and does a few simplifications. It is easier to change the behavior to synchronize more strongly after the stack of #14294 is merged.
267,20591,0,"@ignore annotation for user defined type. ## 🚀 Feature
In user defined type, it would be nice to have a @ignore annotation similar to https://github.com/pytorch/pytorch/pull/16055

This allows the user to write python only methods that are only used in training / debugging (like )

cc @suo"
34,19163,1,"LayerNorm is very slow (almost frozen) in CPU of multiprocessing. The context of the use case is doing ES over small network in multiprocessing. 

It turns out that with  it becomes extremely slow on CPU.

To reproduce the effect, the code is attached below:


The benchmark is:

- **:**

- **:**

**Completely frozen after waiting a few minutes, and  shows all 80 cores CPU are 100% busy**


- **:**


- **:**



"
614,7276,0,"[feature request] Caffe2 model to PyTorch model. Do you think about converting Caffe2 model to PyTorch model for continue changing model, e.g fine-tuning, transfer-learning?

According to new feature of PyTorch 1.0, it seems possible, isn't it?"
188,27295,0,Documentation of which torch.tensor operations work on QTensors. Create a table listing which methods work on the quantized tensors (only a subset of the tensor methods work). 
604,2281,0,"Error installing branch v0.2.0 from source . I'm on ubuntu 16.04 with titan X. I am trying to install v0.2.0 branch from source.

I created new conda env  and followed the instructions on Readme.md.

python setup.py install fails with error which look like this:


I'm attaching complete log file: [log_file.txt](https://github.com/pytorch/pytorch/files/1196589/log_file.txt)


Sasank."
230,2351,0,"The Performance of v0.2.0 is 20X slower than v0.1.12. Hi, i use pytroch to train  all-cnn model on cifar-10 by [Entropy-SGD](https://github.com/ucla-vision/entropy-sgd/tree/master/python). 

The log of v0.1.12_2 is 
Train: [ 1] 0.9442 33.32% [13.62s]
Train: [ 2] 0.8139 28.38% [14.39s]
Train: [ 3] 0.7006 24.22% [13.76s]
Train: [ 4] 0.6254 21.68% [13.71s]
Train: [ 5] 0.5720 19.96% [13.63s]

and the log of v0.2.0_1 is
Train: [ 1] 0.2238 7.84% [294.03s]
Train: [ 2] 0.1349 4.66% [291.71s]
Train: [ 3] 0.0950 3.26% [291.67s]
Train: [ 4] 0.0715 2.41% [290.50s]
Train: [ 5] 0.0558 1.89% [295.84s]

All code is same but the only difference is the version of pytroch. Why the performance of the two has the gap?

P.S. Can you re-generate the pip resource for v.0.1.12 ?"
265,28884,0,"Why attn_mask is not 3D tensor in nn.MultiheadAttention?. ## 🚀 Feature
I think shoud add 3D attn_mask.
## Motivation

For example, if in a translation task,  I want to let every word in the target sentence focus different words in source sentence, "" attn_mask"" should  in shape (tgt_len, scr_len) , ok! That's fine, that's exactly what it is now. But in different sentences，the target word in same position may focus different source words, so the ""broadcast"" is unsuitable in this case and "" attn_mask"" should  in shape (batch_size, tgt_len, scr_len) . The same question is also in nn.Transfomers.
So I think it's necessary to fix.

cc @albanD @mruberry @jbschlosser @zhangguanheng66"
661,24612,0,"Migrate `polygamma` and `polygamma_` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
746,15958,0,"stable 1.0 with cuda 10 successfully, but cannot pip3 install torchvision!. ## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
I installed pytorch stable 1.0, linux, pip, python 3.6, successfully. However, when I pip3 install torchvision, it began downloading pytorch for cpu.
So I cancelled the installing torchvision!

Waiting for help!"
110,20009,0,"issue while exporting torch model to onnx format. ## 🐛 Bug

i am getting error while converting my saved torch model to onnx format

torch.onnx.export(trained_model, dummy_input, ""sentiment.onnx"")
TypeError: forward() missing 1 required positional argument: 'hidden' 

torch version - '1.0.1.post2'

## To Reproduce

Steps to reproduce the behavior:
from torch.autograd import Variable
import torch.onnx
# Load the trained model from file
net_save = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)
trained_model = net_save
trained_model.load_state_dict(torch.load('sentiment.pth'))

# Export the trained model to ONNX
dummy_input = Variable(torch.randn(1, 1, 28, 28)) # one black and white 28 x 28 picture will be the input to the model
torch.onnx.export(trained_model, dummy_input, ""sentiment.onnx"")

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

i should get the onnx format of my model but getting error

"
623,24856,0,"[jit] schema matching incorrectly types a call to append with an argument of type Scalar. Minimal Repro:



cc @suo"
414,18689,0,"distributed.all_gather function stuck when using NCCL backend . ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
I am trying to use distributed.all_gather to gather gradients in multi nodes. but I found the all_gather function stuck and no error throw
## To Reproduce

Steps to reproduce the behavior:

1. set up env variable  , save the following code as 

2. run the code with  and 

btw, when I execute this code manually in , I found the all_gather quickly go through, but it stuck when trying to print tensor

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

process will print a list of tensor
<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:

PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.10.2

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration:
GPU 0: Tesla K80
GPU 1: Tesla K80
GPU 2: Tesla K80
GPU 3: Tesla K80
GPU 4: Tesla K80
GPU 5: Tesla K80
GPU 6: Tesla K80
GPU 7: Tesla K80

Nvidia driver version: 384.145
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1

Versions of relevant libraries:
[pip3] numpy==1.15.3
[pip3] torch==1.0.1.post2
[pip3] torchvision==0.2.1
[conda] Could not collect

## Additional context

<!-- Add any other context about the problem here. -->
"
620,4031,0,"NVIDIA memory not deallocated after interupt. When I interrupt my Pytorch script using Ctrl-C occasionally GPU memory is not deallocated. Also threads related to my script may or may not still be running. If they are running I kill them using ""kill -9 PID"". However this does not deallocate memory on the GPU. 

> Tue Dec  5 14:12:11 2017       
> +-----------------------------------------------------------------------------+
> | NVIDIA-SMI 384.98                 Driver Version: 384.98                    |
> |-------------------------------+----------------------+----------------------+
> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
> |===============================+======================+======================|
> |   0  TITAN X (Pascal)    Off  | 00000000:02:00.0  On |                  N/A |
> | 31%   54C    P2    58W / 250W |   7829MiB / 12189MiB |      3%      Default |
> +-------------------------------+----------------------+----------------------+
>                                                                                
> +-----------------------------------------------------------------------------+
> | Processes:                                                       GPU Memory |
> |  GPU       PID   Type   Process name                             Usage      |
> |=============================================================================|
> |    0      1213      G   /usr/lib/xorg/Xorg                           166MiB |
> |    0      2355      G   compiz                                       249MiB |
> |    0     32412      G   ...-token=00DF6CCAB2487BD6F9AE70914F3A9358     8MiB |
> +-----------------------------------------------------------------------------+"
343,31557,0,"[docs] F.ctc_loss docs to warn clearly about invalid inf-causing inputs; zero_infinity to become enabled by default. **UPD** summary of all the long discussion for further discoverability:
1. F.ctc_loss will produce inf loss if presented with invalid unalignable examples
2. Such invalid examples may be generated by official usage code example if one is extremely unlucky or if one twists the dimension sizes a little bit
3. When presented with invalid examples, sum and mean reduction modes by default cause the whole batch loss to be **inf**

Proposals:
1. Have docs warn clearly about conditions on valid examples
2. Have docs warn clearly that the official usage example may produce invalid examples / fix the official code example
3. Enable zero_infinity = True by default or at least in reduction modes sum/mean

**BELOW IS THE ORIGINAL ISSUE DESCRIPTION**



Docs specify that targets can't be blank. If same consecutive labels are not supported, I think it should be explicitly mentioned in docs. And maybe docs should specify some workaround to encode consecutive same-valued targets (given that blank to separate them isn't allowed by docs).

Current docs: "
681,27450,0,"formatting issue in dynamic quantization function. <img width=""798"" alt=""formatting-issue-either-or"" src=""https://user-images.githubusercontent.com/45861273/66273196-95bb6b80-e826-11e9-8cd8-8fcb2a2759dd.png"">


cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100"
675,14057,0,"Unable to pickle torch dtype objects in Python 3.5. ## 🐛 Bug

When pickling a  object, Python 3.5 reports an obscure error ""can't pickle int objects"".

## To Reproduce

Steps to reproduce the behavior:



## Expected behavior

In Python 3.6 one can pickle torch dtypes successfully.

## Environment

"
461,19601,0,"TestJit.test_cpp broken on master. ## 🐛 Bug


https://circleci.com/gh/pytorch/pytorch/1423249?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link/console"
547,22171,0,"[jit] Compile only the relevant branch for `isinstance`. Similar to our thing where we only compile if a submodule is not None (#14533), we could skip compiling the non-taken branch for  checks (since their result is constant) to support use cases like this:




cc @suo"
79,2973,1,"broadcasting inconsistency?. when we multiply (or any binary operator) two tensor variables, i've noticed the weird behaviour depending on the shapes of those variables:

(a) when x.size()=(1,10) and y.size()=(10), (x\*y).size()=(1,10) (expected)
(b) when x.size()=(10) and y.size()=(1,10), (x\*y).size()=(1,10) (expected)
(c) when x.size()=(10,1) and y.size()=(10), (x\*y).size()=(10,10) (**unexpected**)
(d) when x.size()=(10) and y.size()=(10,1), (x\*y).size()=(10,10) (**unexpected**)
(e) when x.size()=(10,1) and y.size()=(1,10), (x\*y).size()=(10,10) (expected)

the cases (c) and (d) really easily throw off many (or perhaps only me) from debugging, when such cases happen in a loss function that reduces the output into a single scalar. 

my expectation is that broadcasting should first find a matching axis, and i wonder if the current implementation/behaviour is expected and was designed with a certain goal in mind.

"
710,30446,0,"Is there any plan for Adafactor optimizer?. ## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
official Adafactor optimizer
## Motivation
an efficient optimizer Adafactor is currently widely used in some big models, it saves a lot of memory due to its sub-linear running average of the gradient, sometimes result in a sigificant memory footprint reduce and larger batch size.
<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

## Pitch

<!-- A clear and concise description of what you want to happen. -->

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->


cc @vincentqb"
252,473,0,"Autograd IndexCopy is broken. This happens when the index tensor contains duplicate elements. If this is not allowed index_copy_ should raise an exception when this happens and we should fix the test. Otherwise we need to fix the autograd op.

Here's an example:





cc @ezyang @SsnL @albanD"
381,10746,0,"[Caffe2] Failed to build dispatch_test. Error LNK2001: unresolved external symbol. ## Issue description

Getting LNK2001 on Windows for .


## System Info
- PyTorch or Caffe2: C2
- How you installed PyTorch (conda, pip, source): src
- Build command you used (if compiling from source): cmake
- OS: Win10
- PyTorch version: master
- VS version (if compiling from source): 2017
- CMake version: 3.12"
714,16198,0,"ppc64le CI build failure (possibly due to #16006. ## 🐛 Bug

ppc64le CI build fails with psutil not found.

Running test_dataloader ... [2019-01-20 15:16:53.873206]
Traceback (most recent call last):
  File ""test_dataloader.py"", line 24, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'


pytorch version : pull from master for last 3 days.
here is the CI log:
https://powerci.osuosl.org/user/avmgithub/my-views/view/PyTorch/job/pytorch-linux-cuda92-cudnn7-py3-mpi-build-test-gpu/269/console

CC:  @SsnL 
"
117,1434,0,"no member named 'shared_ptr' in namespace 'std'. When building from source, I got this error. My system is OSX 10.12.4, conda, CUDA 8.0. Any thoughts? Thanks!

"
575,6462,0,"How to set batch_size when using DataParallel mode in pytorch?. I can't find document about the batch_size setting and DataParallel mode.
so How to set batch_size when using DataParallel mode in pytorch? Is it similiar with Keras?
Thanks

In Keras, the parallel_model( like DataParallel mode in pytorch), which document is very clear I think:
-------------------------------------------Keras document beginging------------------------------------------
E.g. if your batch_size is 64 and you use gpus=2, then we will divide the input into 2 sub-batches of 32 samples, process each sub-batch on one GPU, then return the full batch of 64 processed samples

This  call will be distributed on 8 GPUs.
 Since the batch size is 256, each GPU will process 32 samples.

parallel_model.fit(x, y, epochs=20, batch_size=256)
-------------------------------------------Keras document ended------------------------------------------"
185,28902,0,"C++ `torch::tensor` by default gives a double tensor, which is different from Python `torch.tensor` behavior. Example:
In C++:


In Python:


cc @yf225"
204,23366,0,"[PyTorch][Feature Request]Lookahead Optimizer. ## 🚀 Lookahead Optimizer
https://arxiv.org/pdf/1907.08610.pdf

In this paper they show new Optimizer can save time, improve accuracy than SGD and work both on CV/NLP.
I think it may worth to try.
"
197,1280,0,"Multi-GPU forward pass fails if first GPU id is not 0. I can specify multiple GPU IDs and train a network using them just fine. However, when it comes time to run a validation dataset through the trained network, pytorch throws an error when using a list of GPU IDs unless the GPU ID list starts with id 0.

Here's a sample function that will cause an error:



If I train the network using  the function above executes with no problem.  However, if I train the network using  it will throw an error."
449,7634,0,"cuda out of memory err:   when my gpu memory still has 4G left .  

## Issue description

i was running a trained model  from github ,i watched the gpu memory, found that the script exited when the memory usage got 2G.
my gpu is 1060 with 6G memory ,how did that come?


PS: torch might has the 2G limit,but i haven't install that 


 

## System Info
RuntimeError: cuda runtime err(2): out of memory at /opt/conda/conda-bld/pytorch_1501953625411/work/pytorch-0.1.12/torch/libTHC/THCstorage.cu:66

 

- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source):
- Build command you used (if compiling from source):
- OS: Ubuntu 14.04
- PyTorch version: pytorch 0.1.10
- Python version:  2.7
- CUDA/cuDNN version: 8.0/5.1 
- GCC version (if compiling from source): 4.9
 
 
"
276,14844,0,"Negative indexing for nn.Embedding inputs. ## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->

Negative indexing for nn.Embedding inputs

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

I found both
- 
- 

allow negative indexing for padding.

How about we allow negative indexing for  inputs as well?

## Example



cc @albanD @mruberry"
231,12013,0,"[Feature Request] Make nn layers accept empty batch size. Now that we have support for tensors with zero in its size, I believe it would be very handy to have support for accepting batches of size 0 in  functions.

A (non-exhaustive) list of functions that would be good supporting:
- [x] 
- [x] 
- [x] 
- [x] 

Handling the losses is a bit trickier, because it generally involves computing a , which results in  due to 0 / 0 division. I'd expect having a 0 loss for empty batches to make sense, but that's debatable so might be worth postponing this decision.

cc @ezyang @gchanan @zou3519 @albanD @mruberry"
192,27352,0,"torch quant docs all showing up in the index.html. When I checkout the [ branch](https://github.com/gottbrath/pytorch/tree/quantization_1_3_doc/docs), run make html from /docs, and look at the index.html, all the old quantization docs show up even though the [index.rst ](https://github.com/gottbrath/pytorch/blob/quantization_1_3_doc/docs/source/index.rst)lists quantization.rst. 

Did some testing and research and it looks like it's because in quantization.rst there are multiple H1 headers. We would need to decrease all the headers except the first by one level. 

![image](https://user-images.githubusercontent.com/8042156/66173258-1b130600-e604-11e9-914c-7f3e2b874aa7.png)
"
621,23236,0,"AttributeError: module 'torchvision.models' has no attribute 'detection'. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

Detection module is not present in torchvision.models

## To Reproduce

Steps to reproduce the behavior:

1. import detection from torchvision,models in a python script

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
733,24869,0,"Dictionary in C++. ## 🚀 Feature
Implement a dictionary ops in C++

## Motivation

The dictionary ops will replace the current vocab class in torchtext (or serve as a new class for the same purpose). It could also be applied for transforming audio script. I think a dictionary ops in pytorch core library makes it easier to maintain.

## Pitch

Some baselines should be included:
+ findWord()
+ addWord()
+ count_nwords()
+ count_ntokens()
+ count_nlables()
+ getWord()
+ readFromSequence()
+ readFromFile()
"
351,14659,0,"error: module multiprocessing.util' has no attribute '_flush_std_streams. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1. I followed the tutorial code: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
1. run without change anything, but error comes out. even I download the official code, error still exist.
1. seems like cannot enumerate trainloader. once do enumerate(trainloader), error appears: ""module multiprocessing.util' has no attribute '_flush_std_streams""

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 8.0.61

OS: Ubuntu 16.04 LTS
GCC version: (Ubuntu 4.9.3-13ubuntu2) 4.9.3
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: Tesla P100-PCIE-16GB
GPU 1: Tesla P100-PCIE-16GB
GPU 2: Tesla P100-PCIE-16GB
GPU 3: Tesla P100-PCIE-16GB
GPU 4: Tesla P100-PCIE-16GB
GPU 5: Tesla P100-PCIE-16GB
GPU 6: Tesla P100-PCIE-16GB
GPU 7: Tesla P100-PCIE-16GB

Nvidia driver version: 384.90
cuDNN version: Probably one of the following:
/usr/local/MATLAB/R2016b/bin/glnxa64/libcudnn.so.4.0.7

Versions of relevant libraries:
[pip] numpy (1.15.4)
[pip] torch (0.4.1)
[pip] torchvision (0.2.1)
[conda] cuda80                    1.0                  h205658b_0    pytorch
[conda] pytorch                   0.4.1           py36_cuda8.0.61_cudnn7.1.2_1  [cuda80]  pytorch
[conda] torchvision               0.2.1                    py36_1    pytorch

## Additional context

<!-- Add any other context about the problem here. -->
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-12-a647eac09f6e> in <module>
     21         plt.title('Batch from dataloader')
     22 
---> 23 for i_batch, sample_batched in enumerate(dataloader):
     24     print(i_batch, sample_batched['image'].size(),
     25           sample_batched['landmarks'].size())

~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __iter__(self)
    499 
    500     def __iter__(self):
--> 501         return _DataLoaderIter(self)
    502 
    503     def __len__(self):

~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __init__(self, loader)
    287             for w in self.workers:
    288                 w.daemon = True  # ensure that the worker exits on process exit
--> 289                 w.start()
    290 
    291             _update_worker_pids(id(self), tuple(w.pid for w in self.workers))

~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py in start(self)
    103                'daemonic processes are not allowed to have children'
    104         _cleanup()
--> 105         self._popen = self._Popen(self)
    106         self._sentinel = self._popen.sentinel
    107         # Avoid a refcycle if the target function holds an indirect

~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py in _Popen(process_obj)
    221     @staticmethod
    222     def _Popen(process_obj):
--> 223         return _default_context.get_context().Process._Popen(process_obj)
    224 
    225 class DefaultContext(BaseContext):

~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/context.py in _Popen(process_obj)
    275         def _Popen(process_obj):
    276             from .popen_fork import Popen
--> 277             return Popen(process_obj)
    278 
    279     class SpawnProcess(process.BaseProcess):

~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py in __init__(self, process_obj)
     15 
     16     def __init__(self, process_obj):
---> 17         util._flush_std_streams()
     18         self.returncode = None
     19         self._launch(process_obj)

AttributeError: module 'multiprocessing.util' has no attribute '_flush_std_streams'

===================================================================
I test the code on windows, still got the error: [Errno 32] Broken pipe"
470,9990,0,"Build fails in caffe2 git master with OpenCL ON. I'm getting the following compile error when trying to build caffe2 git master:


- Cmake command and output: [https://bpaste.net/show/f14fd917039e](https://bpaste.net/show/f14fd917039e)

- Caffe2 from pytorch stable release v0.4.1 builds fine with the same cmake options as above (OpenCL ON).

- Caffe2 git master builds fine when using the same cmake options as above and switching OpenCL to .

- The error seems be **not** related to the gcc version used. The same error occurs with gcc8, gcc7 and gcc5.4.

**System Information:**
**OS:** Arch Linux x86_64
**Compiler:** gcc 8.1.1 (as mentioned, the same error also occurs with gcc7 and gcc5.4)
**Caffe2:** git master
**OpenCL headers:** 2.2.20170516"
82,10681,1,"nn.DataParallel hangs with Pytorch 0.4.1 and CUDA 9.1.85 on TITAN V. Hi guys,
I tested a simple example with nn.DataParallel() to use multiple GPUs, but got a hang.


It hangs when I try to forward the data to the model. nvidia-smi gives


I have tried the solution in [this](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158), but it didn’t work.

I use

- CUDA 9.1.85
- Pytorch 0.4.1 (installed by pip)
- Python 2.7.13
- Debian 4.9.110-3+deb9u1 (2018-08-03) x86_64 GNU/Linux
- TITAN V cards

Any ideas to solve this issue? Or I should let NVIDIA’s folks see this issue?"
140,23641,0,"Hi, could you please tell me the different about deconvtranspose between pytorch and caffe?. ## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
"
600,12901,0,"torch.nn.L1Loss works incorrectly in certain situations. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
torch.nn.L1Loss()'s parameter *reduction* doesn't work if b = net(a).



## To Reproduce

Steps to reproduce the behavior:




<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Outputs



<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.2.148

OS: Debian GNU/Linux 9.5 (stretch)
GCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
CMake version: Could not collect

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: Tesla K80
Nvidia driver version: 396.44
cuDNN version: Probably one of the following:
/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1
/usr/local/cuda-9.2/lib64/libcudnn_static.a

Versions of relevant libraries:
[pip] intel-numpy (1.15.1)
[pip] numpy (1.15.1)
[pip] torch (0.4.1)
[pip] torchvision (0.2.1)
[conda] Could not collect


## Additional context
I have checked the source. In  F.l1_loss(), it use 

to convert reduction to an integer. However, it calls _pointwise_loss(), 
which assumes that reduction is still a str.

That's the reason of this bug.
<!-- Add any other context about the problem here. -->
"
619,11901,0,"[feature request] ability to nest ParameterList. In some cases, it is more convenient to arrange some parameters as 2d or 3d arrays rather than 1D list. One could solve it by computing a Nd to 1D offset array, but this could be avoided if the  objects could be nested

Example:


"
145,3195,0,"PyTorch throws an exception on import when denormals are flushed to zero. This happens, for example, when you import DyNet before importing PyTorch. The code from #3113 tries to take the log(0) which throws an exception.

My guess is that DyNet is setting FTZ or DAZ (or both). See:
https://software.intel.com/en-us/node/523328"
662,13,0,"Infinite recursion when indexing Variable. 


"
354,31591,0,"when i try to export a pytorch model to ONNX, got RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

/Users/duke/opt/anaconda3/bin/python /Volumes/无极山/project/code/pytorch2ONNX.py
/Volumes/无极山/project/code/models/anchors.py:24: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  image_shape = np.array(image_shape)
/Volumes/无极山/project/code/models/anchors.py:35: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  return torch.from_numpy(all_anchors.astype(np.float32))
Traceback (most recent call last):
  File ""/Volumes/无极山/project/code/pytorch2ONNX.py"", line 39, in <module>
    save2ONNX(args)
  File ""/Volumes/无极山/project/code/pytorch2ONNX.py"", line 27, in save2ONNX
    torch.onnx.export(model, x, export_ONNX_file)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 143, in export
    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 66, in export
    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 382, in _export
    fixed_batch_size=fixed_batch_size)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 249, in _model_to_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args, training)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py"", line 206, in _trace_and_get_graph_from_model
    trace, torch_out, inputs_states = torch.jit.get_trace_graph(model, args, _force_outplace=True, _return_inputs_states=True)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 275, in get_trace_graph
    return LegacyTracedModule(f, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/duke/opt/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 356, in forward
    torch._C._tracer_exit(tuple(out_vars))
RuntimeError: output of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.

Process finished with exit code 1


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
##Here is the code
import torch
import torch.onnx
from torch.autograd import Variable
from models.retinanet import resnet34, resnet50
from build_network import build_network
input_model = '../user_data/model_data/model_at_epoch_1.dat'
output_dir = 'torch_onnx_model.onnx'
dummy_input = torch.randn(2, 3, 800, 800)
checkpoint = torch.load(input_model)
model, _ = build_network(snapshot=None, backend='retinanet')
model.load_state_dict(checkpoint['state_dict'])
torch.onnx.export(model, dummy_input, output_dir)
print('Done')



## Expected behavior

The ONNX should be exported successfully.

## Environment
Python 3.7.4
pytorch '1.3.1'
macos 10.15




cc @suo"
524,19697,0,"torch.transpose doesn't shares it’s underlying storage with the original tensor. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
This concerns the method:
torch.transpose(input, dim0, dim1) → Tensor

According to the documentation, https://pytorch.org/docs/stable/torch.html :
> The resulting out tensor shares it’s underlying storage with the input tensor, so changing the content of one would change the content of the other.

However, transposed versions of the original tensor are not changed when the original tensor's data is changed via ""set_"" method (specifically).

## To Reproduce


>  t is: 
> tensor([[-1.,  2.]])
>  ttr is: 
> tensor([[-1.],
>         [ 2.]])
> --------------------
>  t is: 
> tensor([[5., 2.]])
>  ttr is: 
> tensor([[5.],
>         [2.]])
> The transposed versions were updated, as expected
> --------------------
>  t is: 
> tensor([[10., 20.]])
>  ttr is: 
> tensor([[10.],
>         [20.]])
> The transposed versions were updated, as expected
> --------------------
>  t is: 
> tensor([[12., 22.]])
>  ttr is: 
> tensor([[12.],
>         [22.]])
> The transposed versions were updated, as expected
> --------------------
>  t is: 
> tensor([[-1., -2.]])
>  ttr is: 
> tensor([[12.],
>         [22.]])

## Expected behavior
I expect that ""changing the content of one (via set_) would change the content of the other."", as per the documentation.

## Environment

PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.14.3
GCC version: Could not collect
CMake version: version 3.13.4

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.14.5
[pip3] torch==1.0.1.post2
[pip3] torchvision==0.2.2.post3
[conda] Could not collect


## Additional context

<!-- Add any other context about the problem here. -->
"
570,17569,0,"Is it possible to integrate jax into pytorch ?. # What is jax (just in case for other reader)
[jax](https://github.com/google/jax) is full numpy acceleration and autodiff with functional neural networks, and is essentially autograd 2.0. (from [Italian Association for Machine Learning)](https://iaml.it/blog/jax-intro-english)

# Feature Request
Is it possible to incorporate jax into implementation of Pytorch? It looks so promising to speed up machine learning.  
I would like to know if there is any plan about jax. Thanks."
236,23756,0,"[feature request] Core API for invertible and flow-like ops. Inplace BatchNorm seems to be developed by Mapillary here: https://github.com/mapillary/inplace_abn

This would be a very nice addition to core PyTorch (for memory savings).

cc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw @SsnL"
1,8710,1,"Incorrect calculation for torch.nn.MSELoss. ## Issue description
torch.nn.MSELoss seems to get inaccurate calculation when size_average=True and reduce=True

## Code example

For tensors of 0.1's and 0's, the desired output should be 0.01. (0.1 - 0)^2 = 0.01.
I can get correct result by code below:


However, if I do something like below, I get inaccurate results:


But when I turned off size_average and reduce and calculate mean manually, I get desired output:

Or:



I kind of know that you cannot get exactly number when you using float datatype, but 0.01 and 0.00983 seems to be too off for me.
I am not quite sure if it is intended to be like that or I am not using it correctly.

## System Info
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 2.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti

Nvidia driver version: 387.34
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a

Versions of relevant libraries:
[pip] numpy (1.14.5)
[pip] torch (0.4.0)
[pip] torchvision (0.2.1)
[conda] Could not collect
"
654,6287,0,"No .toDevice for ATen Tensors. As title says, might be nice to be able to put ATen Tensors on other GPUs easily in C++."
458,3225,0,"Subtracting ByteTensor doesn't work in this case.. This snippet used to work, but now throws an error:

"
674,13806,0,"OMP: Error about doubly-linked OpenMP when loading CIFAR10 dataset. I compile PyTorch from source (1.0.0a0+54e8623).
and when I attempt to load data I get this:


The crash occurs when I do this:


I'm using the CIFAR10 dataset here, the problem does not occur with MNIST.


The system is macOS 10.12.6. What could the problem be here?"
615,871,0,"GPU torch.multinomial produces an out-of-bounds index.  on the GPU can produce indices that are out of bounds.

Consider the following code:



Here's output from an example run:



Here, the sampled indices should be between 0 to 5, but one of the values is  .

The iteration at which this happens is random.

The following code uses probabilities closer to actual probabilities



but it has the same problem (the problem occurs much later on average though). Example output snippet:

"
55,23862,1,"nn.functional.conv2d is a factor 5 slower when using specific weight tensor on CPU.. ## 🐛 Bug

When using the torch conv2d I have a huge slowdown in speed using a specific weight tensor coming from a pretrained state_dict. By adding 1 and subsequently subtracting 1 from the tensor from the state_dict I get a huge improvement (factor 5 on my machine). 

In the code example below I compare the pretrained weight with itself after adding and subtracting 1. 

I also observed this problem in 1.0.1


generates the problematic tensor (There are many tensors like this in my pretrained state_dict, so it is not unique) 

## To Reproduce

To reproduce run the following code and read the printouts in the console.



## Expected behavior

I would expect the speed of conv2d to be identical whether using w1 or w2. 

## Environment

 - PyTorch Version: 1.1
 - OS: Ubuntu 18.04.2 LTS
 - How you installed PyTorch: 

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
certifi    2019.6.16
numpy      1.17.0   
pip        19.1.1   
setuptools 41.0.1   
torch      1.1.0    
wheel      0.33.4   "
440,24686,0,"Migrate `clamp` and `clamp_` from the TH to Aten (CPU). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
7,13886,1,"Get wrong with Pytorch derivative of division. I'm trying to compute the gradient of 1/x without using Pytorch's autograd. I use the formula grad(1/x, x) = -1/(x**2) but when I compare my result with the gradient given by Pytorch's autograd, they're different.
Here is my code

The output is

Can anyone explain why ?"
598,768,0,"Feature Request: Bitwise Operations. it seems that there is SOME support for bitwise operations in NumPy:
https://docs.scipy.org/doc/numpy/reference/generated/numpy.bitwise_xor.html
Bitwise operations are also supported by CUDA:
https://docs.nvidia.com/cuda/cuda-c-programming-guide/#warp-shuffle-functions
yet, the implementation of binary operation  in  is 

which takes two ByteTensor. My undertsanding is that this is inefficient, because only one Bit within every Byte of this tensor is actually encoding something. Please correct me if I'm wrong!!

for the record neither TensorFlow has bitwise operations
"
0,15872,1,"GPU memory usage is very high at the beginning of training.. ## 🐛 Bug

At the beginning of the training, the GPU memory usage is high, i.e. about 9GB. After some iterations, the consumption is stable at 1.4 GB. 

So I can't set a larger batch size.

## To Reproduce
Change crop size to  on  https://github.com/pytorch/examples/blob/0.4/imagenet/main.py#L122 can reproduce the problem to some extent, i.e. 7.5GB at beginning and 4.5GB at stable.

My training scripts only has a more complicated dataset process.

## Environment
"
26,30365,1,"TorchScript Performance: 150x gap between TorchScript and Native Python. ## 🐛 Bug

There's a 150x gap in performance for TorchScript ops versus straight Python / C++.
Looping over 100K numbers takes 2+ seconds instead of 18ms or better. 
Please see the benchmarks here: https://github.com/divyekapoor/ml-op-benchmarks

## To Reproduce
https://github.com/divyekapoor/ml-op-benchmarks

Steps to reproduce the behavior:
1. Clone the repo
1. make torchbench

See related TensorFlow issue for context:
https://github.com/tensorflow/tensorflow/issues/34500

## Expected behavior

FizzBuzz Iteration Counts | 100000 |   |   |  
-- | -- | -- | -- | --
  | Raw Latency (ms) | Per Run Latency (usec) | Python Multiplier | C++ Multiplier
PyTorch Python | 4007 | 40.07 | 222.61 | 23851
PyTorch TorchScript Python (from Loaded TorchScript) | 2830 | 28.3 | **157.22** | 16845
PyTorch TorchScript C++ (Native) | 255 | 2.55 | **14.17** | 1518
PyTorch TorchScript C++ (Native + ATen Tensors) | 252 | 2.52 | **14.00** | 1500
Raw Python | 18 | 0.18 | 1.00 | 107
Raw C++ | 0.168 | 0.00168 | 0.01 | 1

Performance similar to raw Python is the expected behavior.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:




 - PyTorch Version (e.g., 1.0): 1.3
 - OS (e.g., Linux): Mac OS X
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source): NA
 - Python version: 3.7
 - CUDA/cuDNN version: NA
 - GPU models and configuration: NA
 - Any other relevant information: See performance tables and github repo.

## Additional context

Code: 



cc @suo"
49,18722,1,"pytorch can't be as good as keras. I use the same data、 same data iter and same vgg16 network, but the pytorch's loss beame 12.3 while the keras's loss became 4.2:
pytorch loss log:
train Loss: 15.2963 val Loss: 8.0108
train Loss: 12.4626 val Loss: 9.5972
train Loss: 12.4143 val Loss: 6.9568
train Loss: 12.3993  val Loss: 8.3055
train Loss: 12.3956 val Loss: 6.8054

keras loss:
Epoch 1/20
402/402 [==============================] - 183s 455ms/step - loss: 11.6960 
Epoch 2/20
402/402 [==============================] - 181s 449ms/step - loss: 6.6525
Epoch 3/20
402/402 [==============================] - 181s 450ms/step - loss: 5.4451
Epoch 4/20
402/402 [==============================] - 186s 463ms/step - loss: 4.9515 
Epoch 5/20
402/402 [==============================] - 186s 462ms/step - loss: 4.6679 
Epoch 6/20
402/402 [==============================] - 186s 462ms/step - loss: 4.4298 

 The main code is as fallows:
pytorch:
model = nn.Sequential(*list(make_model('vgg16', num_classes=1, pretrained=True, input_size=INPUT_SIZE).children())[0][:],\
                      nn.AdaptiveAvgPool2d(1), 
                      Flatten(),nn.Sequential(nn.Linear(512, 1024, bias=True),nn.ReLU(), nn.Linear(1024, 1, bias=True))).cuda()
dataloaders = {'train':train_dl, 'val':valid_dl}
train_model(model, dataloaders, nn.L1Loss(), optim.Adam(model.parameters(),lr=0.001), 21)

keras:
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(input_shape[1], input_shape[0], 3))
    model = GlobalAveragePooling2D()(base_model.output)
    model = Dense(1024, activation='relu')(model)
    model = Dense(1, activation='linear')(model)

    model = Model(inputs=base_model.input, outputs=model)
    # model = base_model

    keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    patience=0, 
    verbose=0, 
    mode='auto')
    model.compile(keras.optimizers.Adam(0.001), 'mape', metrics=['acc', 'mae', 'mse'])
model.fit_generator(train_iter, len_train//BS-1, 20,validation_data=valid_iter, validation_steps=len_valid//BS, callbacks=[early_stop])   "
307,934,0,"dataloader parallels over elements vs over batches. With current design of , if we set , 8 batches of data will be prepared in advance, each worker works on one batch. This is not very efficient especially when one batch is large/expensive to process; and normally we only need 1-2 batches ahead of time . Why didn't we go with the design where all workers work on one batch at a time? 

cc @SsnL"
153,598,0,"Problem with backward hook function. Hi,

there is something strange in the  step (or maybe something I don't understand). If I define a Module that takes 3 inputs, the  has to be of size 3, right ? But this is not the case here (from the backward_hook point of view):



In that case, when I print grad_input throught the hook function, it is just composed of two elements... Could you tell me where am I wrong ? But  seem correctly computed

cc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen"
406,24718,0,"Migrate `l1_loss_backward` from the TH to Aten (CPU). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
324,24770,0,"Migrate `sort` from the TH to Aten (CPU). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
513,25320,0,"Compilation Error on Windows. 
#### ~~How to reproduce~~




## Configuration Log
<details><summary>Expand for detail</summary>
<p>



</p>
</details>

## First Error
<details><summary>Expand for detail</summary>
<p>



</p>
</details>

## Second Error
<details><summary>Expand for detail</summary>
<p>



</p>
</details>

## Third Error

The third one is same to the second one, but happens to target 

cc @peterjc123"
416,14179,0,"Failed to run 'bash ../tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn --use-qnnpack caffe2'. ## 🐛 Bug
-- Build files have been written to: /home/feng/pytorch/build
+ make install -j12
Scanning dependencies of target js_embed
Scanning dependencies of target benchmark
Scanning dependencies of target nccl_external
Scanning dependencies of target pthreadpool
Scanning dependencies of target clog
Scanning dependencies of target gtest
Scanning dependencies of target gloo
Scanning dependencies of target onnxifi_dummy
Scanning dependencies of target onnxifi_loader
Scanning dependencies of target libprotobuf-lite
Scanning dependencies of target libprotobuf
[  0%] Creating directories for 'nccl_external'
Scanning dependencies of target mkldnn
[  1%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/json_reporter.cc.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/js_embed.dir/__/src/google/protobuf/compiler/js/embed.cc.o
[  1%] Building C object confu-deps/clog/CMakeFiles/clog.dir/src/clog.c.o
[  1%] No download step for 'nccl_external'
[  1%] No patch step for 'nccl_external'
[  1%] No update step for 'nccl_external'
[  1%] No configure step for 'nccl_external'
[  1%] Performing build step for 'nccl_external'
[  1%] Building C object third_party/onnx/CMakeFiles/onnxifi_loader.dir/onnx/onnxifi_loader.c.o
[  1%] Building C object third_party/onnx/CMakeFiles/onnxifi_dummy.dir/onnx/onnxifi_dummy.c.o
[  1%] Building C object confu-deps/pthreadpool/CMakeFiles/pthreadpool.dir/src/threadpool-pthreads.c.o
make[3]: warning: jobserver unavailable: using -j1.  Add '+' to parent make rule.
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‘clog_vlog_fatal’:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:120:4: warning: ignoring return value of ‘write’, declared with attribute warn_unused_result [-Wunused-result]
    write(STDERR_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‘clog_vlog_error’:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:196:4: warning: ignoring return value of ‘write’, declared with attribute warn_unused_result [-Wunused-result]
    write(STDERR_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‘clog_vlog_warning’:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:272:4: warning: ignoring return value of ‘write’, declared with attribute warn_unused_result [-Wunused-result]
    write(STDERR_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‘clog_vlog_info’:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:348:4: warning: ignoring return value of ‘write’, declared with attribute warn_unused_result [-Wunused-result]
    write(STDOUT_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c: In function ‘clog_vlog_debug’:
/home/feng/pytorch/third_party/QNNPACK/deps/clog/src/clog.c:424:4: warning: ignoring return value of ‘write’, declared with attribute warn_unused_result [-Wunused-result]
    write(STDOUT_FILENO, out_buffer, prefix_chars + format_chars + CLOG_SUFFIX_LENGTH);
    ^
[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/algorithm.cc.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/arena.cc.o
Generating nccl.h.in                           > /home/feng/pytorch/build/nccl/include/nccl.h
Compiling  init.cu                             > /home/feng/pytorch/build/nccl/obj/init.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  1%] Building CXX object third_party/googletest/googletest/CMakeFiles/gtest.dir/src/gtest-all.cc.o
[  1%] Linking CXX executable ../../../bin/js_embed
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/arenastring.cc.o
[  1%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/string_util.cc.o
[  1%] Linking C shared library ../../lib/libonnxifi_dummy.so
[  1%] Linking C static library ../../lib/libpthreadpool.a
[  1%] Linking C static library ../../lib/libclog.a
[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/allgather.cc.o
[  1%] Linking C static library ../../lib/libonnxifi_loader.a
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive.cpp.o
[  1%] Built target onnxifi_dummy
[  1%] Built target js_embed
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/pooling.cpp.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/extension_set.cc.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o
[  1%] Built target onnxifi_loader
[  1%] Built target pthreadpool
[  1%] Built target clog
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arenastring.cc.o
Scanning dependencies of target python_copy_files
Scanning dependencies of target c10
[  1%] Building CXX object c10/CMakeFiles/c10.dir/DeviceType.cpp.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/engine.cpp.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/query.cpp.o
[  1%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o
[  1%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/allreduce.cc.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/softmax.cpp.o
[  1%] Building CXX object c10/CMakeFiles/c10.dir/Half.cpp.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/mkldnn_debug.cpp.o
[  1%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/batch_normalization.cpp.o
[  2%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set.cc.o
[  2%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/scratchpad.cpp.o
[  2%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/commandlineflags.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive_attr.cpp.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/Device.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/verbose.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/eltwise.cpp.o
[  3%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/sleep.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/memory_desc_wrapper.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive_iterator.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/lrn.cpp.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/allreduce_local.cc.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/Stream.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/memory.cpp.o
[  3%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/statistics.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/inner_product.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/OpSchema.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/KernelRegistration.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/DeviceId.cpp.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/broadcast.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/stream.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/convolution_relu.cpp.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/generated_message_util.cc.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/coded_stream.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/rnn.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/reorder.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/DispatchKey.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_util.cc.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/context.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/convolution.cpp.o
[  3%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/benchmark.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/primitive_desc.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/Dispatcher.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/OpSchemaRegistration.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/deconvolution.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/common/utils.cpp.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_conv_kernel_f32.cpp.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/zero_copy_stream.cc.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/gather.cc.o
[  3%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_batch_normalization_utils.cpp.o
init.cu:52:1: warning: ‘ncclNet’ initialized and declared ‘extern’
 ncclNet_t* ncclNet = NULL;
 ^
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/DispatchTable.cpp.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/core/dispatch/LayoutId.cpp.o
[  3%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/reduce.cc.o
[  3%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/zero_copy_stream_impl_lite.cc.o
[  3%] Building CXX object c10/CMakeFiles/c10.dir/impl/DeviceGuardImplInterface.cpp.o
[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/message_lite.cc.o
[  4%] Building CXX object c10/CMakeFiles/c10.dir/util/Type.cpp.o
[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/coded_stream.cc.o
[  4%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/repeated_field.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  4%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_lrn.cpp.o
[  4%] Building CXX object c10/CMakeFiles/c10.dir/util/Backtrace.cpp.o
[  4%] Building CXX object c10/CMakeFiles/c10.dir/util/Optional.cpp.o
[  4%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/scatter.cc.o
[  4%] Building CXX object c10/CMakeFiles/c10.dir/util/C++17.cpp.o
Compiling  ring.cu                             > /home/feng/pytorch/build/nccl/obj/ring.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/SmallVector.cpp.o
[  5%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/csv_reporter.cc.o
[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_gcc.cc.o
[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_msvc.cc.o
[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/LeftRight.cpp.o
[  5%] Building CXX object c10/CMakeFiles/c10.dir/util/flags_use_gflags.cpp.o
[  5%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/zero_copy_stream.cc.o
[  5%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_1x1_conv_kernel.cpp.o
[  5%] Linking CXX static library ../../../lib/libgtest.a
[  5%] Built target gtest
Scanning dependencies of target ATEN_CPU_FILES_GEN_TARGET
[  5%] Generating ../aten/src/ATen/CPUByteType.cpp, ../aten/src/ATen/CPUByteType.h, ../aten/src/ATen/CPUCharType.cpp, ../aten/src/ATen/CPUCharType.h, ../aten/src/ATen/CPUCopy.cpp, ../aten/src/ATen/CPUDoubleType.cpp, ../aten/src/ATen/CPUDoubleType.h, ../aten/src/ATen/CPUFloatType.cpp, ../aten/src/ATen/CPUFloatType.h, ../aten/src/ATen/CPUGenerator.h, ../aten/src/ATen/CPUHalfType.cpp, ../aten/src/ATen/CPUHalfType.h, ../aten/src/ATen/CPUIntType.cpp, ../aten/src/ATen/CPUIntType.h, ../aten/src/ATen/CPULongType.cpp, ../aten/src/ATen/CPULongType.h, ../aten/src/ATen/CPUShortType.cpp, ../aten/src/ATen/CPUShortType.h, ../aten/src/ATen/Declarations.yaml, ../aten/src/ATen/Functions.h, ../aten/src/ATen/NativeFunctions.h, ../aten/src/ATen/RegisterCPU.cpp, ../aten/src/ATen/RegisterCPU.h, ../aten/src/ATen/SparseCPUByteType.cpp, ../aten/src/ATen/SparseCPUByteType.h, ../aten/src/ATen/SparseCPUCharType.cpp, ../aten/src/ATen/SparseCPUCharType.h, ../aten/src/ATen/SparseCPUDoubleType.cpp, ../aten/src/ATen/SparseCPUDoubleType.h, ../aten/src/ATen/SparseCPUFloatType.cpp, ../aten/src/ATen/SparseCPUFloatType.h, ../aten/src/ATen/SparseCPUIntType.cpp, ../aten/src/ATen/SparseCPUIntType.h, ../aten/src/ATen/SparseCPULongType.cpp, ../aten/src/ATen/SparseCPULongType.h, ../aten/src/ATen/SparseCPUShortType.cpp, ../aten/src/ATen/SparseCPUShortType.h, ../aten/src/ATen/TypeDefault.cpp, ../aten/src/ATen/TypeDefault.h, ../aten/src/ATen/TypeExtendedInterface.h, ../aten/src/ATen/CUDAByteType.cpp, ../aten/src/ATen/CUDAByteType.h, ../aten/src/ATen/CUDACharType.cpp, ../aten/src/ATen/CUDACharType.h, ../aten/src/ATen/CUDACopy.cpp, ../aten/src/ATen/CUDADoubleType.cpp, ../aten/src/ATen/CUDADoubleType.h, ../aten/src/ATen/CUDAFloatType.cpp, ../aten/src/ATen/CUDAFloatType.h, ../aten/src/ATen/CUDAGenerator.h, ../aten/src/ATen/CUDAHalfType.cpp, ../aten/src/ATen/CUDAHalfType.h, ../aten/src/ATen/CUDAIntType.cpp, ../aten/src/ATen/CUDAIntType.h, ../aten/src/ATen/CUDALongType.cpp, ../aten/src/ATen/CUDALongType.h, ../aten/src/ATen/CUDAShortType.cpp, ../aten/src/ATen/CUDAShortType.h, ../aten/src/ATen/RegisterCUDA.cpp, ../aten/src/ATen/RegisterCUDA.h, ../aten/src/ATen/SparseCUDAByteType.cpp, ../aten/src/ATen/SparseCUDAByteType.h, ../aten/src/ATen/SparseCUDACharType.cpp, ../aten/src/ATen/SparseCUDACharType.h, ../aten/src/ATen/SparseCUDADoubleType.cpp, ../aten/src/ATen/SparseCUDADoubleType.h, ../aten/src/ATen/SparseCUDAFloatType.cpp, ../aten/src/ATen/SparseCUDAFloatType.h, ../aten/src/ATen/SparseCUDAIntType.cpp, ../aten/src/ATen/SparseCUDAIntType.h, ../aten/src/ATen/SparseCUDALongType.cpp, ../aten/src/ATen/SparseCUDALongType.h, ../aten/src/ATen/SparseCUDAShortType.cpp, ../aten/src/ATen/SparseCUDAShortType.h
[  6%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/types.cc.o
[  6%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_deconvolution.cpp.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_1x1_conv_kernel_f32.cpp.o
[  7%] Built target python_copy_files
Scanning dependencies of target common
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/bytestream.cc.o
[  7%] Building C object sleef/src/common/CMakeFiles/common.dir/common.c.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/common.cc.o
[  7%] Built target common
Scanning dependencies of target mkrename
[  7%] Building C object sleef/src/libm/CMakeFiles/mkrename.dir/mkrename.c.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/common/linux.cc.o
[  7%] Linking C executable ../../bin/mkrename
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/reporter.cc.o
[  7%] Built target mkrename
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/complexity.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/zero_copy_stream_impl_lite.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/Array.cpp.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_sum.cpp.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/Logging.cpp.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/int128.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/counter.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/message_lite.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/io_win32.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/once.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/sysinfo.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/status.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/colorprint.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/statusor.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/common/logging.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/mpi/context.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/Exception.cpp.o
Compiling  bootstrap.cu                        > /home/feng/pytorch/build/nccl/obj/bootstrap.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/repeated_field.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringpiece.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringprintf.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/timers.cc.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_convolution.cpp.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_conv_winograd_kernel_f32.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/context.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/benchmark_register.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_gcc.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/TypeList.cpp.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/strutil.cc.o
[  7%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark.dir/console_reporter.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/typeid.cpp.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/atomicops_internals_x86_msvc.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/file_store.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/hash_store.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/time.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/wire_format_lite.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/bytestream.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/TensorTypeId.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/prefix_store.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/Metaprogramming.cpp.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/TypeTraits.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/rendezvous/store.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/address.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/buffer.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/StringUtil.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/context.cc.o
[  7%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/common.cc.o
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/TensorTypeIdRegistration.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/device.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/pair.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  7%] Building CXX object c10/CMakeFiles/c10.dir/util/flags_use_no_gflags.cpp.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/unbound_buffer.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/address.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/buffer.cc.o
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/context.cc.o
[  7%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_inner_product.cpp.o
Compiling  transport.cu                        > /home/feng/pytorch/build/nccl/obj/transport.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[  7%] Linking CXX static library ../../../lib/libprotobuf-lite.a
[  7%] Built target libprotobuf-lite
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/device.cc.o
Scanning dependencies of target mkdisp
[  7%] Building C object sleef/src/libm/CMakeFiles/mkdisp.dir/mkdisp.c.o
[  7%] Linking C executable ../../bin/mkdisp
[  7%] Built target mkdisp
Scanning dependencies of target renamedsp256.h_generated
[  7%] Generating renamedsp256.h
[  7%] Built target renamedsp256.h_generated
Scanning dependencies of target dispavx.c_generated
[  7%] Generating dispavx.c
[  7%] Built target dispavx.c_generated
Scanning dependencies of target renameSSE2.h_generated
[  7%] Generating include/renamesse2.h
[  7%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/pair.cc.o
Generating renamesse2.h: mkrename 2 4 sse2
[  7%] Built target renameSSE2.h_generated
Scanning dependencies of target renameFMA4.h_generated
[  7%] Generating include/renamefma4.h
Generating renamefma4.h: mkrename 4 8 fma4
[  7%] Built target renameFMA4.h_generated
Scanning dependencies of target renameAVX2.h_generated
[  7%] Generating include/renameavx2.h
Generating renameavx2.h: mkrename 4 8 avx2
[  7%] Built target renameAVX2.h_generated
Scanning dependencies of target renameAVX2128.h_generated
[  7%] Generating include/renameavx2128.h
Generating renameavx2128.h: mkrename 2 4 avx2128
[  7%] Built target renameAVX2128.h_generated
Scanning dependencies of target renameSSE4.h_generated
[  8%] Generating include/renamesse4.h
Generating renamesse4.h: mkrename 2 4 sse4
[  8%] Built target renameSSE4.h_generated
Scanning dependencies of target ATEN_CUDA_FILES_GEN_TARGET
[  8%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/int128.cc.o
[  8%] Generating ../aten/src/ATen/CPUByteType.cpp, ../aten/src/ATen/CPUByteType.h, ../aten/src/ATen/CPUCharType.cpp, ../aten/src/ATen/CPUCharType.h, ../aten/src/ATen/CPUCopy.cpp, ../aten/src/ATen/CPUDoubleType.cpp, ../aten/src/ATen/CPUDoubleType.h, ../aten/src/ATen/CPUFloatType.cpp, ../aten/src/ATen/CPUFloatType.h, ../aten/src/ATen/CPUGenerator.h, ../aten/src/ATen/CPUHalfType.cpp, ../aten/src/ATen/CPUHalfType.h, ../aten/src/ATen/CPUIntType.cpp, ../aten/src/ATen/CPUIntType.h, ../aten/src/ATen/CPULongType.cpp, ../aten/src/ATen/CPULongType.h, ../aten/src/ATen/CPUShortType.cpp, ../aten/src/ATen/CPUShortType.h, ../aten/src/ATen/Declarations.yaml, ../aten/src/ATen/Functions.h, ../aten/src/ATen/NativeFunctions.h, ../aten/src/ATen/RegisterCPU.cpp, ../aten/src/ATen/RegisterCPU.h, ../aten/src/ATen/SparseCPUByteType.cpp, ../aten/src/ATen/SparseCPUByteType.h, ../aten/src/ATen/SparseCPUCharType.cpp, ../aten/src/ATen/SparseCPUCharType.h, ../aten/src/ATen/SparseCPUDoubleType.cpp, ../aten/src/ATen/SparseCPUDoubleType.h, ../aten/src/ATen/SparseCPUFloatType.cpp, ../aten/src/ATen/SparseCPUFloatType.h, ../aten/src/ATen/SparseCPUIntType.cpp, ../aten/src/ATen/SparseCPUIntType.h, ../aten/src/ATen/SparseCPULongType.cpp, ../aten/src/ATen/SparseCPULongType.h, ../aten/src/ATen/SparseCPUShortType.cpp, ../aten/src/ATen/SparseCPUShortType.h, ../aten/src/ATen/TypeDefault.cpp, ../aten/src/ATen/TypeDefault.h, ../aten/src/ATen/TypeExtendedInterface.h, ../aten/src/ATen/CUDAByteType.cpp, ../aten/src/ATen/CUDAByteType.h, ../aten/src/ATen/CUDACharType.cpp, ../aten/src/ATen/CUDACharType.h, ../aten/src/ATen/CUDACopy.cpp, ../aten/src/ATen/CUDADoubleType.cpp, ../aten/src/ATen/CUDADoubleType.h, ../aten/src/ATen/CUDAFloatType.cpp, ../aten/src/ATen/CUDAFloatType.h, ../aten/src/ATen/CUDAGenerator.h, ../aten/src/ATen/CUDAHalfType.cpp, ../aten/src/ATen/CUDAHalfType.h, ../aten/src/ATen/CUDAIntType.cpp, ../aten/src/ATen/CUDAIntType.h, ../aten/src/ATen/CUDALongType.cpp, ../aten/src/ATen/CUDALongType.h, ../aten/src/ATen/CUDAShortType.cpp, ../aten/src/ATen/CUDAShortType.h, ../aten/src/ATen/RegisterCUDA.cpp, ../aten/src/ATen/RegisterCUDA.h, ../aten/src/ATen/SparseCUDAByteType.cpp, ../aten/src/ATen/SparseCUDAByteType.h, ../aten/src/ATen/SparseCUDACharType.cpp, ../aten/src/ATen/SparseCUDACharType.h, ../aten/src/ATen/SparseCUDADoubleType.cpp, ../aten/src/ATen/SparseCUDADoubleType.h, ../aten/src/ATen/SparseCUDAFloatType.cpp, ../aten/src/ATen/SparseCUDAFloatType.h, ../aten/src/ATen/SparseCUDAIntType.cpp, ../aten/src/ATen/SparseCUDAIntType.h, ../aten/src/ATen/SparseCUDALongType.cpp, ../aten/src/ATen/SparseCUDALongType.h, ../aten/src/ATen/SparseCUDAShortType.cpp, ../aten/src/ATen/SparseCUDAShortType.h
[  8%] Building CXX object third_party/gloo/gloo/CMakeFiles/gloo.dir/transport/tcp/unbound_buffer.cc.o
[  8%] Linking CXX shared library ../lib/libc10.so
[  8%] Built target c10
Scanning dependencies of target mkrename_gnuabi
[  8%] Building C object sleef/src/libm/CMakeFiles/mkrename_gnuabi.dir/mkrename_gnuabi.c.o
[  8%] Linking C executable ../../bin/mkrename_gnuabi
[  8%] Built target mkrename_gnuabi
Scanning dependencies of target mkmasked_gnuabi
[  8%] Building C object sleef/src/libm/CMakeFiles/mkmasked_gnuabi.dir/mkmasked_gnuabi.c.o
[  8%] Linking C executable ../../bin/mkmasked_gnuabi
[  8%] Built target mkmasked_gnuabi
Scanning dependencies of target mkalias
[  8%] Building C object sleef/src/libm/CMakeFiles/mkalias.dir/mkalias.c.o
Scanning dependencies of target arraymap
[  8%] Building C object sleef/src/common/CMakeFiles/arraymap.dir/arraymap.c.o
[  8%] Linking C executable ../../bin/mkalias
[  8%] Built target mkalias
[  8%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/io_win32.cc.o
Scanning dependencies of target torch_shm_manager
[  8%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/once.cc.o
[  8%] Building CXX object caffe2/torch/lib/libshm/CMakeFiles/torch_shm_manager.dir/manager.cpp.o
[  8%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/status.cc.o
[  8%] Built target arraymap
Scanning dependencies of target c10_utils_gpu
[  9%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_gpu.dir/dummy.cpp.o
[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_barrier.cpp.o
[  9%] Built target c10_utils_gpu
Scanning dependencies of target c10_utils_hip
[  9%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_hip.dir/dummy.cpp.o
[  9%] Built target c10_utils_hip
Scanning dependencies of target c10_utils_cpu
[  9%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_cpu.dir/dummy.cpp.o
[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/simple_sum.cpp.o
[  9%] Built target c10_utils_cpu
Scanning dependencies of target cpuinfo
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/init.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/api.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/init.c.o
[  9%] Linking CXX static library ../../../lib/libbenchmark.a
[  9%] Built target benchmark
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/info.c.o
Scanning dependencies of target nnpack_reference_layers
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/convolution-output.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/vendor.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/uarch.c.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/convolution-input-gradient.c.o
[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ncsp_batch_normalization.cpp.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/name.c.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/convolution-kernel.c.o
[  9%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/statusor.cc.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/fully-connected-output.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/topology.c.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/max-pooling-output.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/isa.c.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/softmax-output.c.o
[  9%] Built target ATEN_CPU_FILES_GEN_TARGET
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/relu-output.c.o
[  9%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_inner_product.cpp.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/cache/descriptor.c.o
[  9%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack_reference_layers.dir/src/ref/relu-input-gradient.c.o
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/cache/init.c.o
[  9%] Linking C static library ../../lib/libnnpack_reference_layers.a
[  9%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/cache/deterministic.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/linux/init.c.o
[ 10%] Built target nnpack_reference_layers
Scanning dependencies of target gtest_main
[ 10%] Building CXX object third_party/googletest/googletest/CMakeFiles/gtest_main.dir/src/gtest_main.cc.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/x86/linux/cpuinfo.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/smallfile.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/multiline.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/current.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/cpulist.c.o
[ 10%] Building C object confu-deps/cpuinfo/CMakeFiles/cpuinfo.dir/src/linux/processors.c.o
Scanning dependencies of target benchmark_main
[ 10%] Building CXX object third_party/benchmark/src/CMakeFiles/benchmark_main.dir/benchmark_main.cc.o
[ 10%] Linking C static library ../../lib/libcpuinfo.a
[ 10%] Built target cpuinfo
Scanning dependencies of target onnxifi_wrapper
[ 10%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/stringpiece.cc.o
[ 10%] Building C object third_party/onnx/CMakeFiles/onnxifi_wrapper.dir/onnx/onnxifi_wrapper.c.o
[ 10%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/simple_concat.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 10%] Linking CXX executable ../../../../bin/torch_shm_manager
[ 10%] Linking C shared module ../../lib/libonnxifi.so
[ 10%] Built target onnxifi_wrapper
Scanning dependencies of target __aten_op_header_gen
[ 10%] Generating contrib/aten/aten_op.h
[ 10%] Built target torch_shm_manager
Scanning dependencies of target renameAVX.h_generated
[ 10%] Generating include/renameavx.h
Generating renameavx.h: mkrename 4 8 avx
[ 10%] Built target renameAVX.h_generated
[ 11%] Linking CXX static library ../../../lib/libgloo.a
Scanning dependencies of target renamedsp128.h_generated
[ 11%] Generating renamedsp128.h
[ 11%] Built target renamedsp128.h_generated
Scanning dependencies of target headers
[ 12%] Generating ../../../include/sleef.h
[ 12%] Built target gloo
Scanning dependencies of target dispsse.c_generated
[ 12%] Generating dispsse.c
Generating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__
Generating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__ sse2
Generating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__ sse4
Generating sleef.h: mkrename 4 8 __m256d __m256 __m128i struct\ {\ __m128i\ x,\ y;\ } __AVX__
Generating sleef.h: mkrename 4 8 __m256d __m256 __m128i struct\ {\ __m128i\ x,\ y;\ } __AVX__ avx
Generating sleef.h: mkrename 4 8 __m256d __m256 __m128i struct\ {\ __m128i\ x,\ y;\ } __AVX__ fma4
Generating sleef.h: mkrename 4 8 __m256d __m256 __m128i __m256i __AVX__ avx2
Generating sleef.h: mkrename 2 4 __m128d __m128 __m128i __m128i __SSE2__ avx2128
Generating sleef.h: mkrename 8 16 __m512d __m512 __m256i __m512i __AVX512F__
Generating sleef.h: mkrename 8 16 __m512d __m512 __m256i __m512i __AVX512F__ avx512f
[ 12%] Built target dispsse.c_generated
[ 12%] Built target headers
Scanning dependencies of target sleefsse2
Scanning dependencies of target sleeffma4
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefsse2.dir/sleefsimdsp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleeffma4.dir/sleefsimdsp.c.o
Compiling  misc/group.cu                       > /home/feng/pytorch/build/nccl/obj/misc/group.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 12%] Linking CXX static library ../../../lib/libbenchmark_main.a
[ 12%] Linking CXX static library ../../../lib/libgtest_main.a
[ 12%] Built target benchmark_main
Scanning dependencies of target sleefavx2
[ 12%] Built target gtest_main
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimdsp.c.o
Scanning dependencies of target sleefavx2128
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimdsp.c.o
[ 12%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/stringprintf.cc.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleeffma4.dir/sleefsimddp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimddp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimddp.c.o
[ 12%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefsse2.dir/sleefsimddp.c.o
[ 12%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_conv_kernel.cpp.o
[ 12%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_convolution.cpp.o
[ 12%] Built target sleeffma4
Scanning dependencies of target sleefsse4
[ 12%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/strutil.cc.o
[ 12%] Built target sleefavx2128
Scanning dependencies of target c10_utils_gpu_test
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefsse4.dir/sleefsimdsp.c.o
[ 12%] Linking CXX executable ../../bin/c10_utils_gpu_test
[ 12%] Built target sleefavx2
Scanning dependencies of target c10_utils_hip_test
[ 12%] Building CXX object caffe2/utils/CMakeFiles/c10_utils_hip_test.dir/dummy.cpp.o
[ 12%] Linking CXX executable ../../bin/c10_utils_hip_test
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 12%] Built target c10_utils_gpu_test
Scanning dependencies of target c10_utils_cpu_test
[ 12%] Linking CXX executable ../../bin/c10_utils_cpu_test
[ 12%] Built target c10_utils_hip_test
Scanning dependencies of target qnnpack
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefsse4.dir/sleefsimddp.c.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/init.c.o
[ 12%] Built target c10_utils_cpu_test
[ 12%] Generating src/x86_64-fma/2d-fourier-8x8.py.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/convolution.c.o
[ 12%] Built target sleefsse2
Scanning dependencies of target c10_registry_test
[ 12%] Building CXX object c10/test/CMakeFiles/c10_registry_test.dir/registry_test.cpp.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/deconvolution.c.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/fully-connected.c.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/sgemm/6x8-psimd.c.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8gemm/2x4c8-sse2.c.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8gemm/4x4c2-sse2.c.o
[ 12%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_pool_kernel_f32.cpp.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8conv/4x4c2-sse2.c.o
[ 12%] Built target sleefsse4
Compiling  misc/nvmlwrap.cu                    > /home/feng/pytorch/build/nccl/obj/misc/nvmlwrap.o
Scanning dependencies of target c10_OpSchema_test
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 12%] Building CXX object c10/test/CMakeFiles/c10_OpSchema_test.dir/dispatch/OpSchema_test.cpp.o
[ 12%] Building C object confu-deps/QNNPACK/CMakeFiles/qnnpack.dir/src/q8dw/9c8-sse2.c.o
[ 12%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/time.cc.o
[ 12%] Linking C static library ../../lib/libqnnpack.a
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/wire_format_lite.cc.o
[ 13%] Built target qnnpack
Scanning dependencies of target c10_InlineStreamGuard_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_InlineStreamGuard_test.dir/impl/InlineStreamGuard_test.cpp.o
[ 13%] Linking CXX executable ../../bin/c10_OpSchema_test
[ 13%] Built target c10_OpSchema_test
Scanning dependencies of target c10_StreamGuard_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_StreamGuard_test.dir/StreamGuard_test.cpp.o
[ 13%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_u8s8s32x_convolution.cpp.o
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any.cc.o
[ 13%] Linking CXX executable ../../bin/c10_registry_test
[ 13%] Built target c10_registry_test
Scanning dependencies of target c10_DeviceGuard_test
[ 13%] Linking CXX executable ../../bin/c10_StreamGuard_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_DeviceGuard_test.dir/DeviceGuard_test.cpp.o
[ 13%] Built target c10_StreamGuard_test
Scanning dependencies of target c10_TypeTraits_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_TypeTraits_test.dir/util/TypeTraits_test.cpp.o
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any.pb.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 13%] Generating src/x86_64-fma/2d-fourier-16x16.py.o
[ 13%] Built target ATEN_CUDA_FILES_GEN_TARGET
Scanning dependencies of target c10_Metaprogramming_test
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/api.pb.cc.o
[ 13%] Building CXX object c10/test/CMakeFiles/c10_Metaprogramming_test.dir/util/Metaprogramming_test.cpp.o
[ 13%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_1x1_convolution.cpp.o
[ 13%] Linking CXX executable ../../bin/c10_TypeTraits_test
[ 13%] Built target c10_TypeTraits_test
Scanning dependencies of target c10_logging_test
[ 13%] Building CXX object c10/test/CMakeFiles/c10_logging_test.dir/logging_test.cpp.o
[ 13%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/compiler/importer.cc.o
[ 13%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_gemm_f32.cpp.o
[ 14%] Linking CXX executable ../../bin/c10_InlineStreamGuard_test
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 14%] Built target c10_InlineStreamGuard_test
Scanning dependencies of target c10_Array_test
[ 14%] Building CXX object c10/test/CMakeFiles/c10_Array_test.dir/util/Array_test.cpp.o
[ 14%] Linking CXX executable ../../bin/c10_DeviceGuard_test
Compiling  misc/ibvwrap.cu                     > /home/feng/pytorch/build/nccl/obj/misc/ibvwrap.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 14%] Built target c10_DeviceGuard_test
Scanning dependencies of target c10_InlineDeviceGuard_test
[ 14%] Building CXX object c10/test/CMakeFiles/c10_InlineDeviceGuard_test.dir/impl/InlineDeviceGuard_test.cpp.o
[ 14%] Linking CXX executable ../../bin/c10_Metaprogramming_test
[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/compiler/parser.cc.o
[ 14%] Built target c10_Metaprogramming_test
Scanning dependencies of target c10_typeid_test
[ 14%] Linking CXX executable ../../bin/c10_Array_test
[ 14%] Building CXX object c10/test/CMakeFiles/c10_typeid_test.dir/util/typeid_test.cpp.o
[ 14%] Built target c10_Array_test
Scanning dependencies of target c10_flags_test
[ 14%] Building CXX object c10/test/CMakeFiles/c10_flags_test.dir/flags_test.cpp.o
[ 14%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_pooling.cpp.o
[ 14%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor.cc.o
[ 14%] Linking CXX executable ../../bin/c10_logging_test
[ 14%] Built target c10_logging_test
Scanning dependencies of target c10_TypeList_test
Skipping _th_multinomial Because of Arg: Generator * (Generator*) 
Skipping _th_normal Because of Arg: Generator * (Generator*) 
Skipping _th_normal Because of Arg: Generator * (Generator*) 
Skipping _th_normal Because of Arg: Generator * (Generator*) 
Skipping _th_tensor Because of Arg: Storage (Storage) 
Skipping _th_tensor Because of Arg: Storage (Storage) 
Skipping rrelu_with_noise Because of Arg: Generator * (Generator*) 
Skipping rrelu_with_noise_forward Because of Arg: Generator * (Generator*) 
[ 14%] Building CXX object c10/test/CMakeFiles/c10_TypeList_test.dir/util/TypeList_test.cpp.o
Skipping thnn_conv_transpose2d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv_transpose3d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv2d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv_depthwise2d_backward Because of Arg: std::array<bool,2> (std::array<bool,2>) 
Skipping thnn_conv3d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv_dilated2d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping thnn_conv_dilated3d_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping _cudnn_rnn_backward Because of Arg: std::array<bool,4> (std::array<bool,4>) 
Skipping _cudnn_init_dropout_state because it is a factory method
Skipping _fused_dropout Because of Arg: Generator * (Generator *) 
Skipping arange because it is a factory method
Skipping bartlett_window because it is a factory method
Skipping bernoulli Because of Arg: Generator * (Generator *) 
Skipping bernoulli Because of Arg: Generator * (Generator *) 
Skipping blackman_window because it is a factory method
Skipping clamp Because of Arg: c10::optional<Scalar> (Scalar) 
Skipping clamp Because of Arg: c10::optional<Scalar> (Scalar) 
Skipping _convolution_double_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping cudnn_convolution_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping cudnn_convolution_transpose_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping cumsum Because of Arg: ScalarType (ScalarType) 
Skipping cumprod Because of Arg: ScalarType (ScalarType) 
Skipping einsum Because of Arg: std::string (std::string) 
Skipping empty because it is a factory method
Skipping empty_like because it is a factory method
Skipping empty_strided because it is a factory method
Skipping eye because it is a factory method
Skipping full because it is a factory method
Skipping full_like because it is a factory method
Skipping hann_window because it is a factory method
Skipping hamming_window because it is a factory method
Skipping _cufft_set_plan_cache_max_size Because of Ret: void (void)
Skipping _cufft_clear_plan_cache Because of Ret: void (void)
Skipping linspace because it is a factory method
Skipping logspace because it is a factory method
Skipping log_softmax Because of Arg: ScalarType (ScalarType) 
Skipping mean Because of Arg: ScalarType (ScalarType) 
Skipping mean Because of Arg: ScalarType (ScalarType) 
Skipping mean Because of Arg: ScalarType (ScalarType) 
Skipping mkldnn_convolution_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping miopen_convolution_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping miopen_convolution_transpose_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping native_batch_norm_backward Because of Arg: std::array<bool,3> (std::array<bool,3>) 
Skipping ones because it is a factory method
Skipping ones_like because it is a factory method
Skipping rand because it is a factory method
Skipping rand_like because it is a factory method
Skipping randint because it is a factory method
Skipping randint_like because it is a factory method
Skipping randn because it is a factory method
Skipping randn_like because it is a factory method
Skipping randperm because it is a factory method
Skipping range because it is a factory method
Skipping rrelu Because of Arg: Generator * (Generator *) 
Skipping softmax Because of Arg: ScalarType (ScalarType) 
Skipping sum Because of Arg: ScalarType (ScalarType) 
Skipping sum Because of Arg: ScalarType (ScalarType) 
Skipping sum Because of Arg: ScalarType (ScalarType) 
Skipping prod Because of Arg: ScalarType (ScalarType) 
Skipping prod Because of Arg: ScalarType (ScalarType) 
Skipping prod Because of Arg: ScalarType (ScalarType) 
Skipping zeros because it is a factory method
Skipping zeros_like because it is a factory method
Skipping _standard_gamma Because of Arg: Generator * (Generator *) 
Skipping poisson Because of Arg: Generator * (Generator *) 
Skipping sparse_coo_tensor because it is a factory method
Skipping _sparse_coo_tensor_unsafe because it is a factory method
Skipping _sparse_coo_tensor_with_dims because it is a factory method
Skipping _sparse_coo_tensor_with_dims_and_tensors because it is a factory method
Skipping sparse_mask Because of Arg: SparseTensorRef (SparseTensorRef) 
Skipping to because it is a factory method
Skipping data_ptr Because of Ret: void* (void*)
Skipping multinomial Because of Arg: Generator * (Generator *) 
Skipping normal Because of Arg: Generator * (Generator *) 
Skipping normal Because of Arg: Generator * (Generator *) 
Skipping normal Because of Arg: Generator * (Generator *) 
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 14%] Built target __aten_op_header_gen
Scanning dependencies of target sleefavx
[ 14%] Building C object sleef/src/libm/CMakeFiles/sleefavx.dir/sleefsimdsp.c.o
[ 14%] Linking CXX executable ../../bin/c10_InlineDeviceGuard_test
[ 14%] Linking CXX executable ../../bin/c10_flags_test
[ 14%] Built target c10_flags_test
[ 14%] Built target c10_InlineDeviceGuard_test
Scanning dependencies of target dispsse_obj
Scanning dependencies of target dispavx_obj
[ 14%] Building C object sleef/src/libm/CMakeFiles/dispsse_obj.dir/dispsse.c.o
[ 14%] Building C object sleef/src/libm/CMakeFiles/dispavx_obj.dir/dispavx.c.o
[ 14%] Linking CXX executable ../../bin/c10_TypeList_test
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 14%] Built target c10_TypeList_test
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleefavx.dir/sleefsimddp.c.o
[ 15%] Linking CXX executable ../../bin/c10_typeid_test
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_lrn_kernel_f32.cpp.o
[ 15%] Built target c10_typeid_test
Compiling  misc/rings.cu                       > /home/feng/pytorch/build/nccl/obj/misc/rings.o
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_concat.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 15%] Built target dispsse_obj
[ 15%] Built target dispavx_obj
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_convolution.cpp.o
[ 15%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor.pb.cc.o
[ 15%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor_database.cc.o
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_i8i8_pooling.cpp.o
[ 15%] Built target sleefavx
Scanning dependencies of target sleef
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefdp.c.o
[ 15%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/nchw_pooling.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_reducer.cpp.o
[ 16%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefsp.c.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_gemm_f32.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/duration.pb.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_dw_conv_kernel_f32.cpp.o
Compiling  misc/utils.cu                       > /home/feng/pytorch/build/nccl/obj/misc/utils.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_reorder.cpp.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_engine.cpp.o
[ 16%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefld.c.o
[ 16%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefqp.c.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/dynamic_message.cc.o
[ 16%] Linking C static library ../../lib/libsleef.a
[ 16%] Built target sleef
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/empty.pb.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_convolution_winograd.cpp.o
Compiling  misc/enqueue.cu                     > /home/feng/pytorch/build/nccl/obj/misc/enqueue.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set_heavy.cc.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_lrn.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/field_mask.pb.cc.o
[ 16%] Generating src/x86_64-fma/2d-winograd-8x8-3x3.py.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_reflection.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven.cc.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/gzip_stream.cc.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_convolution.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/printer.cc.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_convolution_winograd.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/strtod.cc.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_1x1_convolution.cpp.o
[ 16%] Generating src/x86_64-fma/blas/s8gemm.py.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/tokenizer.cc.o
Compiling  transport/p2p.cu                    > /home/feng/pytorch/build/nccl/obj/transport/p2p.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 16%] Generating src/x86_64-fma/blas/c8gemm.py.o
[ 16%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_eltwise.cpp.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/io/zero_copy_stream_impl.cc.o
[ 16%] Generating src/x86_64-fma/blas/s4c6gemm.py.o
[ 16%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/map_field.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/message.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/reflection_ops.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/service.cc.o
[ 17%] Generating src/x86_64-fma/blas/conv1x1.py.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_pooling.cpp.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_conv_kernel_f32.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 17%] Generating src/x86_64-fma/blas/sgemm.py.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/source_context.pb.cc.o
[ 17%] Generating src/x86_64-fma/max-pooling.py.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/struct.pb.cc.o
[ 17%] Generating src/x86_64-fma/relu.py.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_dw_convolution.cpp.o
[ 17%] Generating src/x86_64-fma/softmax.py.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/mathlimits.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/stubs/substitute.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/text_format.cc.o
Compiling  transport/shm.cu                    > /home/feng/pytorch/build/nccl/obj/transport/shm.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/timestamp.pb.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_batch_normalization.cpp.o
[ 17%] Generating src/x86_64-fma/blas/sdotxf.py.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_inner_product.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/type.pb.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_reorder_utils.cpp.o
[ 17%] Generating src/x86_64-fma/blas/shdotxf.py.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/unknown_field_set.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/delimited_message_util.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_1x1_conv_kernel.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/field_comparator.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/field_mask_util.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/datapiece.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_batch_normalization.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/default_value_objectwriter.cc.o
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_sse42_1x1_convolution.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/error_listener.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/field_mask_utility.cc.o
Compiling  transport/net.cu                    > /home/feng/pytorch/build/nccl/obj/transport/net.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 17%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_u8s8s32x_wino_convolution.cpp.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/json_escaping.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/json_objectwriter.cc.o
[ 17%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/json_stream_parser.cc.o
[ 18%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/object_writer.cc.o
[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_convolution.cpp.o
[ 19%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_1x1_convolution.cpp.o
Scanning dependencies of target nnpack
[ 19%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/proto_writer.cc.o
[ 19%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/init.c.o
[ 19%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-inference.c.o
[ 19%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/protostream_objectsource.cc.o
[ 19%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/fully-connected-inference.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/pooling-output.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/relu-output.c.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/protostream_objectwriter.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/type_info.cc.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/softmax-output.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/fully-connected-output.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/relu-input-gradient.c.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-input-gradient.c.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/type_info_test_helper.cc.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-kernel-gradient.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/convolution-output.c.o
[ 20%] Building C object confu-deps/NNPACK/CMakeFiles/nnpack.dir/src/x86_64-fma/softmax.c.o
[ 20%] Linking C static library ../../lib/libnnpack.a
[ 20%] Built target nnpack
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/internal/utility.cc.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/cpu_reorder.cpp.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/json_util.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/message_differencer.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/time_util.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/util/type_resolver_util.cc.o
Compiling  transport/net_socket.cu             > /home/feng/pytorch/build/nccl/obj/transport/net_socket.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/wire_format.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/wrappers.pb.cc.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx2_1x1_conv_kernel_f32.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_common_conv_kernel.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/gemm_convolution_utils.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_uni_lrn.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_softmax.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_transpose_src_utils.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_rnn.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_eltwise.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/ref_convolution.cpp.o
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/nspc_batch_normalization.cpp.o
[ 20%] Linking CXX static library ../../../lib/libprotobuf.a
[ 20%] Built target libprotobuf
[ 20%] Generating ../../../../third_party/protobuf/src/google/protobuf/compiler/js/well_known_types_embed.cc
Scanning dependencies of target libprotoc
[ 20%] Building CXX object third_party/ideep/mkl-dnn/src/CMakeFiles/mkldnn.dir/cpu/jit_avx512_core_conv_winograd_kernel_f32.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/code_generator.cc.o
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/command_line_interface.cc.o
Compiling  transport/net_ib.cu                 > /home/feng/pytorch/build/nccl/obj/transport/net_ib.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 20%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_enum.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_enum_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_extension.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_file.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_generator.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_helpers.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_map_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_message.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_message_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_padding_optimizer.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_primitive_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_service.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/cpp/cpp_string_field.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_doc_comment.cc.o
Compiling  collectives/all_reduce.cu           > /home/feng/pytorch/build/nccl/obj/collectives/all_reduce.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_enum.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_enum_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_field_base.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_generator.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_helpers.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_map_field.cc.o
[ 21%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_message.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_message_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_primitive_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_reflection_class.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_repeated_enum_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_repeated_message_field.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_repeated_primitive_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_source_generator_base.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/csharp/csharp_wrapper_field.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_context.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_doc_comment.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum_field.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum_field_lite.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_enum_lite.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_extension.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_extension_lite.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_field.cc.o
Compiling  collectives/all_gather.cu           > /home/feng/pytorch/build/nccl/obj/collectives/all_gather.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_file.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_generator.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_generator_factory.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_helpers.cc.o
[ 22%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_lazy_message_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_lazy_message_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_map_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_map_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_builder.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_builder_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_field.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_message_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_name_resolver.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_primitive_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_primitive_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_service.cc.o
Compiling  collectives/broadcast.cu            > /home/feng/pytorch/build/nccl/obj/collectives/broadcast.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_shared_code_generator.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_string_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/java/java_string_field_lite.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_enum.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_enum_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_extension.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_field.cc.o
[ 23%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_file.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_generator.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_helpers.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_map_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_message.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_message_field.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/javanano/javanano_primitive_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/js/js_generator.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/js/well_known_types_embed.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_enum.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_enum_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_extension.cc.o
Compiling  collectives/reduce.cu               > /home/feng/pytorch/build/nccl/obj/collectives/reduce.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_file.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_generator.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_helpers.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_map_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_message.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_message_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_oneof.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/objectivec/objectivec_primitive_field.cc.o
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/php/php_generator.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 24%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/plugin.cc.o
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/plugin.pb.cc.o
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/python/python_generator.cc.o
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/ruby/ruby_generator.cc.o
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/subprocess.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotoc.dir/__/src/google/protobuf/compiler/zip_writer.cc.o
Compiling  collectives/reduce_scatter.cu       > /home/feng/pytorch/build/nccl/obj/collectives/reduce_scatter.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 25%] Linking CXX shared library ../../../../lib/libmkldnn.so
[ 25%] Built target mkldnn
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 25%] Linking CXX static library ../../../lib/libprotoc.a
[ 25%] Built target libprotoc
Scanning dependencies of target protoc
[ 25%] Building CXX object third_party/protobuf/cmake/CMakeFiles/protoc.dir/__/src/google/protobuf/compiler/main.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 25%] Linking CXX executable ../../../bin/protoc
[ 25%] Built target protoc
Scanning dependencies of target gen_onnx_proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/prof_dag.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/predictor_consts.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/torch.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/caffe2.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/hsm.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/metanet.proto
[ 25%] Running C++/Python protocol buffer compiler on /home/feng/pytorch/caffe2/proto/caffe2_legacy.proto
[ 25%] Running gen_proto.py on onnx/onnx.in.proto
Processing /home/feng/pytorch/third_party/onnx/onnx/onnx.in.proto
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch.proto
Compiling  all_reduce.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_reduce_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch.proto3
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx.pb.h
generating /home/feng/pytorch/build/third_party/onnx/onnx/onnx_pb.py
[ 25%] Running C++ protocol buffer compiler on /home/feng/pytorch/build/third_party/onnx/onnx/onnx_onnx_torch.proto
Scanning dependencies of target Caffe2_PROTO
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/caffe2_legacy.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/caffe2.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/predictor_consts.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/prof_dag.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/hsm.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/metanet.pb.cc.o
[ 25%] Building CXX object caffe2/proto/CMakeFiles/Caffe2_PROTO.dir/torch.pb.cc.o
[ 25%] Built target gen_onnx_proto
[ 25%] Running gen_proto.py on onnx/onnx-operators.in.proto
Processing /home/feng/pytorch/third_party/onnx/onnx/onnx-operators.in.proto
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch.proto
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch.proto3
Writing /home/feng/pytorch/build/third_party/onnx/onnx/onnx-operators.pb.h
generating /home/feng/pytorch/build/third_party/onnx/onnx/onnx_operators_pb.py
[ 25%] Running C++ protocol buffer compiler on /home/feng/pytorch/build/third_party/onnx/onnx/onnx-operators_onnx_torch.proto
Scanning dependencies of target onnx_proto
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx_proto.dir/onnx/onnx_onnx_torch.pb.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx_proto.dir/onnx/onnx-operators_onnx_torch.pb.cc.o
[ 26%] Linking CXX static library ../../lib/libonnx_proto.a
[ 26%] Built target onnx_proto
Scanning dependencies of target onnx
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/version_converter/convert.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/function.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/controlflow/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/data_type_utils.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/version_converter/helper.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/__/__/caffe2/onnx/torch_ops/schema.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/nn/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/__/__/caffe2/onnx/torch_ops/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/nn/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/logical/defs.cc.o
[ 26%] Built target Caffe2_PROTO
Scanning dependencies of target Caffe2_perfkernels_avx2
[ 26%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/embedding_lookup_avx2.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/logical/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/reduction/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/generator/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/generator/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/tensor/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/tensor/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/schema.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/traditionalml/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/math/defs.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/math/old.cc.o
[ 26%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/rnn/defs.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/rnn/old.cc.o
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/math_cpu_avx2.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/experiments/defs.cc.o
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/common_avx2.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/defs/experiments/experiments_functions.cc.o
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/typed_axpy_avx2.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/onnxifi_utils.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/ir_pb_converter.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/status.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/assertions.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/model_helpers.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/common/interned_strings.cc.o
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx2.dir/embedding_lookup_fused_8bit_rowwise_avx2.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/checker.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/shape_inference/implementation.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/pass_registry.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/optimize.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/pass_manager.cc.o
[ 27%] Building CXX object third_party/onnx/CMakeFiles/onnx.dir/onnx/optimizer/pass.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Scanning dependencies of target caffe2_protos
[ 27%] Linking CXX static library ../lib/libcaffe2_protos.a
[ 27%] Built target caffe2_protos
Scanning dependencies of target Caffe2_perfkernels_avx
[ 27%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx.dir/adagrad_avx.cc.o
[ 28%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx.dir/typed_axpy_avx.cc.o
[ 28%] Building CXX object caffe2/perfkernels/CMakeFiles/Caffe2_perfkernels_avx.dir/common_avx.cc.o
[ 28%] Built target Caffe2_perfkernels_avx
[ 28%] Built target Caffe2_perfkernels_avx2
[ 28%] Linking CXX static library ../../lib/libonnx.a
[ 28%] Built target onnx
Scanning dependencies of target caffe2
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
Compiling  broadcast.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/broadcast_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
ptxas warning : Too big maxrregcount value specified 96, will be ignored
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
Compiling  reduce.cu                           > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 28%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/DLConvertor.cpp.o
[ 28%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/TensorUtils.cpp.o
[ 28%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUGenerator.cpp.o
[ 28%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/TensorGeometry.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/UndefinedType.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/Utils.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/Context.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUGeneral.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUTypeDefault.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseTensorImpl.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/ExpandUtils.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/detail/ComplexHooksInterface.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/detail/CPUGuardImpl.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/detail/CUDAHooksInterface.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/cpu/FlushDenormal.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Storage.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/context_base.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/thread_pool.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/ATenCoreTest.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/StorageImpl.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Formatting.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/LegacyTypeDispatch.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/register_symbols.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/VariableHooksInterface.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Tensor.cpp.o
[ 29%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/interned_strings.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/TensorOptions.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Range.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/ivalue.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Allocator.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/ATenGeneral.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/UndefinedTensorImpl.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/Scalar.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/blob.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/intrusive_ptr.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/TensorImpl.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/UniqueVoidPtr.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/type.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/core/OptionsGuard.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Embedding.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/SoftMax.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/DispatchStub.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Dropout.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorCompare.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LinearAlgebra.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/GridSampler.cpp.o
[ 30%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/ReduceOps.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/UnaryOps.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Linear.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Indexing.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/PixelShuffle.cpp.o
Compiling  all_gather.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_gather_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorIterator.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LegacyBridge.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TypeProperties.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Copy.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Loss.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/EmbeddingBag.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Normalization.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/WeightNorm.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Resize.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Convolution.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/RoiPooling.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/PackedSequence.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LegacyDefinitions.cpp.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Distance.cpp.o
Compiling  reduce_scatter.cu                   > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_scatter_sum.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorFactories.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/ConvolutionTBC.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/SummaryOps.cpp.o
[ 31%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorTransformations.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Unique.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorIteratorReduce.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Pooling.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Distributions.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/SpectralOps.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Memory.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorProperties.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Scalar.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorShape.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/BinaryOps.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/Activation.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/LossCTC.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/RNN.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/ConstantPadNd.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/TensorConversions.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/BatchLinearAlgebra.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/sparse/SparseTensorMath.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/sparse/SparseTensor.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkl/LinearAlgebra.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkl/SpectralOps.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o
[ 32%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUByteType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUCharType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUCopy.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUDoubleType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUFloatType.cpp.o
Compiling  all_reduce.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_reduce_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUHalfType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUIntType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPULongType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/CPUShortType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/RegisterCPU.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUByteType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUCharType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUDoubleType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUFloatType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUIntType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPULongType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/SparseCPUShortType.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/TypeDefault.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THGeneral.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THAllocator.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THSize.cpp.o
[ 33%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THStorageFunctions.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensor.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorCopy.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorRandom.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorMath.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorMoreMath.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorEvenMoreMath.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorConv.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THTensorLapack.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THBlas.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THLapack.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THLogAdd.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THRandom.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THFile.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THDiskFile.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THMemoryFile.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/THVector.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/vector/AVX.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/TH/vector/AVX2.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/THNN/init.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp.AVX2.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/Activation.cpp.AVX2.cpp.o
[ 34%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/SoftMaxKernel.cpp.AVX2.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/GridSamplerKernel.cpp.AVX2.cpp.o
Compiling  broadcast.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/broadcast_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp.AVX2.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX2.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Compiling  reduce.cu                           > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp.AVX2.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/TensorCompareKernel.cpp.AVX2.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/Activation.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/SoftMaxKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/GridSamplerKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.AVX.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp.AVX.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/TensorCompareKernel.cpp.AVX.cpp.o
Compiling  all_gather.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_gather_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/Activation.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/SoftMaxKernel.cpp.DEFAULT.cpp.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/GridSamplerKernel.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp.DEFAULT.cpp.o
Compiling  reduce_scatter.cu                   > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_scatter_prod.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp.DEFAULT.cpp.o
[ 35%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/cpu/TensorCompareKernel.cpp.DEFAULT.cpp.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/mkldnn/Runtime.cpp.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/aten/aten_op.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/allgather_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/allreduce_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/barrier_ops.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/broadcast_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/common.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/common_world_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/context.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/reduce_scatter_ops.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/gloo/store_handler.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/script/lexer.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/contrib/script/compiler.cc.o
Compiling  all_reduce.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_reduce_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/init_intrinsics_check.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/module.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_dag_utils.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/event.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/graph.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/plan_executor.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/int8_serialization.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/tensor_int8.cc.o
[ 36%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/context.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/prof_dag_counters.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/blob_stats.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/qtensor.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_simple.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/memonger.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/transform.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/operator_c10wrapper.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/context_base.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/common.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/blob_serialization.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_async_tracing.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/workspace.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/tensor_impl.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/init_omp.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/operator.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/init.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/operator_schema.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/tensor.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_async_base.cc.o
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_simple_refcount.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 37%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/stats.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/types.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/allocator.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_dag.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/numa.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/net_async_scheduling.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/qtensor_serialization.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/db.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/proto_convert.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/proto_wrap.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/proto_utils.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/murmur_hash3.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/smart_tensor_printer.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/signal_handler.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/string_utils.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/threadpool/ThreadPool.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/cpuid.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/bench_utils.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/math_cpu.cc.o
Compiling  broadcast.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/broadcast_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/math_utils.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/thread_name.cc.o
[ 38%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/threadpool/pthreadpool.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/utils/threadpool/pthreadpool_impl.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/predictor/predictor.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/predictor/predictor_utils.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/predictor/predictor_config.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/nomnigraph/tests/test_util.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/core/nomnigraph/Representations/NeuralNet.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/db/create_db_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/db/protodb.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/file_store_handler.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Compiling  reduce.cu                           > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/file_store_handler_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/store_handler.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/distributed/store_ops.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/utils/ideep_register.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/dropout_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/pool_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/momentum_sgd_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/local_response_normalization_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/relu_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/elementwise_sum_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/squeeze_op.cc.o
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/utility_ops.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 39%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/conv_fusion_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/operator_fallback_ideep.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/concat_split_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/spatial_batch_norm_op.cc.o
Compiling  all_gather.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_gather_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/fully_connected_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/ideep/operators/conv_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/mpi/mpi_common.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Compiling  reduce_scatter.cu                   > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_scatter_min.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/mpi/mpi_ops.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/observers/time_observer.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/observers/runcnt_observer.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/backend.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/onnx_exporter.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/backend_rep.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/helper.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/onnxifi_init.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/onnx/device.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lpnorm_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gru_unit_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sparse_normalize_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/while_op.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/unique_ops.cc.o
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/feed_blob_op.cc.o
Compiling  all_reduce.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_reduce_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 40%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/index_ops.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/clip_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_div_gradient_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/glu_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reservoir_sampling.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/dropout_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/resize_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_rotated_gradient_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tensor_protos_db_input.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sparse_to_dense_mask_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pool_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/relu_n_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/instance_norm_gradient_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reciprocal_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/leaky_relu_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/distance_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ensure_cpu_output_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/locally_connected_op_util.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/partition_ops.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/selu_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/do_op.cc.o
[ 41%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_reducer_fused_8bit_rowwise_ops.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/free_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_top_k_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_bucketize_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/heatmap_max_keypoint_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/filler_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softsign_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op_shared.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/enforce_finite_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/local_response_normalization_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/moments_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sparse_to_dense_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/data_couple.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tt_linear_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/arg_ops.cc.o
Compiling  broadcast.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/broadcast_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/assert_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
Compiling  reduce.cu                           > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/feature_maps_ops.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/if_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/acos_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/relu_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/accumulate_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/spatial_softmax_with_loss_op.cc.o
[ 42%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/listwise_l2r_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sigmoid_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cast_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/integral_image_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/flexible_top_k.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_sub_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sequence_ops.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/remove_data_blocks_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/h_softmax_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/expand_squeeze_dims_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/slice_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_front_back_max_ops.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_gradient_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stats_put_ops.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/bbox_transform_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/collect_and_distribute_fpn_rpn_proposals_op.cc.o
Compiling  all_gather.cu                       > /home/feng/pytorch/build/nccl/obj/collectives/device/all_gather_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reshape_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/prepend_dim_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_gradient_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sqrt_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/segment_reduction_op.cc.o
[ 43%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_div_op.cc.o
Compiling  reduce_scatter.cu                   > /home/feng/pytorch/build/nccl/obj/collectives/device/reduce_scatter_max.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/byte_weight_dequant_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_sum_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/upsample_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ensure_clipped_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cosh_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/margin_ranking_criterion_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_pool_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tanh_gradient_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lp_pool_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pow_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rowmul_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rsqrt_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_rotated_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/bisect_percentile_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/percentile_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/normalize_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sin_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_moments_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lstm_unit_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/jsd_op.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
Compiling  functions.cu                        > /home/feng/pytorch/build/nccl/obj/collectives/device/functions.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pad_op.cc.o
[ 44%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sigmoid_gradient_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/string_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/create_scope_op.cc.o
ptxas warning : Too big maxrregcount value specified 96, will be ignored
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_reducer_rowwise_8bit_ops.cc.o
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).
nvlink fatal   : Internal error: reference to deleted section
Makefile:83: recipe for target '/home/feng/pytorch/build/nccl/obj/collectives/device/devlink.o' failed
make[5]: *** [/home/feng/pytorch/build/nccl/obj/collectives/device/devlink.o] Error 1
Makefile:45: recipe for target 'devicelib' failed
make[4]: *** [devicelib] Error 2
Makefile:25: recipe for target 'src.build' failed
make[3]: *** [src.build] Error 2
CMakeFiles/nccl_external.dir/build.make:110: recipe for target 'nccl_external-prefix/src/nccl_external-stamp/nccl_external-build' failed
make[2]: *** [nccl_external-prefix/src/nccl_external-stamp/nccl_external-build] Error 2
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/nccl_external.dir/all' failed
make[1]: *** [CMakeFiles/nccl_external.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_reducer_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/deform_conv_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gather_ranges_to_dense_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_transpose_op_mobile.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_matmul_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/crf_viterbi_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/thresholded_relu_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/replace_nan_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/mean_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stylizer_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/is_empty_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/channel_stats_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sinusoid_position_encoding_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/apmeter_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/key_split_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/square_root_divide_op.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_ops.cc.o
[ 45%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softmax_shared.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_mul_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/roi_align_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/minmax_gradient_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/deform_conv_gradient_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_front_back_sum_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/transpose_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/text_file_reader_utils.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elu_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduce_front_back_mean_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/workspace_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/text_file_reader.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/given_tensor_fill_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/utility_ops.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/communicator_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/find_duplicate_elements_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/abs_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cbrt_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_box_cox_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sqr_op.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/last_n_window_collector.cc.o
[ 46%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tanh_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/length_split_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gather_fused_8bit_rowwise_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/onnxifi_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/logit_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/boolean_mask_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stop_gradient.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/asin_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_sparse_to_dense_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/channel_shuffle_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/counter_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/sinh_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_mul_gradient_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ngram_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/accuracy_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reduction_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/matmul_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rank_loss_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softplus_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conditional_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/loss_op.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/boolean_unmask_ops.cc.o
[ 47%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/weighted_multi_sampling_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fused_rowwise_random_quantization_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reciprocal_gradient_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softmax_with_loss_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/numpy_tile_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/piecewise_linear_transform_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/space_batch_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/floor_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/log_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_logical_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_add_gradient_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ceil_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/index_hash_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/weighted_sample_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/atomic_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/one_hot_ops.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_ops_schema.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/merge_id_lists_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/shape_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/perplexity_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/normalize_l1_op.cc.o
[ 48%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/map_ops.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/concat_split_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/load_save_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/find_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/top_k.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/group_norm_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/spatial_batch_norm_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tile_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/box_with_nms_limit_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/tan_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/channel_backprop_stats_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fully_connected_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/zero_gradient_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stump_func_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fused_rowwise_8bit_conversion_ops.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/negative_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/softmax_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/half_float_ops.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ctc_beam_search_decoder_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/given_tensor_byte_string_to_uint8_fill_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/onnx_while_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/negate_gradient_op.cc.o
[ 49%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pool_gradient_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/flatten_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_ops_utils.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/summarize_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/minmax_ops.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_tile_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/im2col_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/generate_proposals_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/stats_ops.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/order_switch_ops.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/reverse_packed_segs_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/norm_planar_yuv_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/exp_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cos_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/atan_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/ctc_greedy_decoder_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/scale_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_transpose_gradient_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/instance_norm_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/prelu_op.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/batch_gather_ops.cc.o
[ 50%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/expand_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cosine_embedding_criterion_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pack_segments.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_add_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/multi_class_accuracy_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/copy_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/mod_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/dataset_ops.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/affine_channel_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/lengths_pad_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/spatial_batch_norm_gradient_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cross_entropy_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/locally_connected_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rmac_regions_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/variable_length_sequence_padding.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/hard_sigmoid_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/pack_rnn_sequence_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/fc_inference.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_op_eigen.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/gather_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/swish_op.cc.o
[ 51%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_sub_gradient_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/conv_transpose_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quant_decode_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/elementwise_linear_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/layer_norm_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/cube_op.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/layer_norm.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/batch_matmul.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/flatten.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/averaged_loss.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/add.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/batch_gather.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/filler.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/enforce_finite.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/expand_dims.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/stop_gradient.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/cast.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/mul.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/sigmoid.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/relu.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/fc.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/sparse_lengths_sum.cc.o
[ 52%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/concat.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/schemas/sigmoid_cross_entropy_with_logits.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/expand_dims_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/add_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/fc_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/concat_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/relu_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/batch_matmul_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/flatten_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/sparse_lengths_sum_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/sigmoid_cross_entropy_with_logits_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/sigmoid_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/filler_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/averaged_loss_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/cast_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/stop_gradient_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/batch_gather_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/enforce_finite_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/experimental/c10/cpu/mul_cpu.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rnn/recurrent_network_op.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rnn/recurrent_network_executor.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/rnn/recurrent_network_blob_fetcher_op.cc.o
[ 53%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/init_qnnpack.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_add_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_average_pool_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_channel_shuffle_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_concat_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_conv_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_conv_transpose_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_dequantize_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_fc_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_flatten_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_given_tensor_fill_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_leaky_relu_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_max_pool_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_quantize_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_relu_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_reshape_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_resize_nearest_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_roi_align_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_slice_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_sigmoid_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/operators/quantized/int8_softmax_op.cc.o
[ 54%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/distributed.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/optimize_ideep.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/distributed_converter.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/dead_code_elim.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/mobile.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/optimizer.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/sink.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/onnxifi_transformer.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/passes.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/annotations.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/converter.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/backend_cutting.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/fusion.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/opt/device.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/typed_axpy.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/fused_8bit_rowwise_embedding_lookup.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/adagrad.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/embedding_lookup.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/perfkernels/math_cpu_base.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/queue_ops.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/rebatching_queue_ops.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/rebatching_queue.cc.o
[ 55%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/blobs_queue.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/queue/blobs_queue_db.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/momentum_sgd_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/adam_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/learning_rate_adaption_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/clip_tensor_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/yellowfin_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/wngrad_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/gftrl_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/iter_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/learning_rate_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/ftrl_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/adadelta_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/lars_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/adagrad_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/sgd/rmsprop_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/share/contrib/nnpack/conv_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/share/contrib/depthwise/depthwise3x3_conv_op.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/single_op_transform.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/conv_to_nnpack_transform.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/pattern_net_transform.cc.o
[ 56%] Building CXX object caffe2/CMakeFiles/caffe2.dir/transforms/common_subexpression_elimination.cc.o
[ 57%] Linking CXX shared library ../lib/libcaffe2.so
[ 57%] Built target caffe2
Makefile:138: recipe for target 'all' failed
make: *** [all] Error 2
Failed to run 'bash ../tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn --use-qnnpack caffe2'
<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.
1.
1.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
683,827,0,"HalfTensor Training Needs non-Stateless Method in F.Linear. Hi Guys,

Currently trying to call backward() on a network trained in FP16 with a linear output results in an error:







This is easily fixed by replacing these calls with the non-stateless versions, as I've done [here](https://github.com/ajbrock/pytorch/blob/master/torch/nn/_functions/linear.py). I'm not sure if this replacement is in line with pytorch style (or if there are plans to implement stateless methods for HalfTensors which would render this irrelevant), but I can submit a PR if needbe.

Best,

Andy"
512,11232,0,[sparse autograd] create get_indices/values; allow backward via ctor
576,22586,0,"got nan when gumbel_softmax calculated in GPU. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce


got results: 
> GPU: nan 0.004% probability happen, tot 38
> CPU: nan 0.000% probability happen, tot 0

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

no nan happen

## Environment

 - PyTorch Version (1.1.0):
 - OS (Linux):
 - How you installed PyTorch ():
 - Python version: 2.7
 - CUDA/cuDNN version: 9.0
"
729,30578,0,"gradients of symeig not correct and gradcheck fails. ## 🐛 Bug

The off-diagonal elements of torch.symeig's gradients are off by a factor of 2. Further, checking the gradients with torch.autograd.gradcheck gives a RuntimeError.

## To Reproduce

The following code computes an eigenvalue decomposition of a symmetric matrix and corresponding derivatives with two different approaches. One approach uses torch.symeig and backward() to compute the gradient. The second approach uses torch.symeig and numerical differentiation to compute the gradient.
Further, torch.autograd.gradcheck is called to test torch.symeig and it gives a RuntimeError.


This prints

Some observations can be made:
First, since numerical differentiation is not implemented to take the symmetry of the matrix M into account, only an upper triangular matrix is computed. This is true for both the version implemented above and for the error message of torch.autograd.gradcheck. The two numerical differentiation versions are identical.
Second, the off-diagonal elements computed via backward() and those computed via numerical differentiation differ by a factor of 2.

## Expected behavior

The derivative computed via backward() should (at least in the upper triangular part) be identical to the numerical differentiation gradient. The reason why this is not the case is that the structure of inputs to torch.symeig (i.e., the symmetry of the inputs) is not taken into account in the backward pass of torch.symeig. Derivatives of functions of symmetric matrices need some extra care, as equation (138) in the [matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) highlights. As far as I can tell, only the [line of code which computes the return value of symeig's backward()](https://github.com/pytorch/pytorch/blob/dd52f50fc85e6710f020936c1fc5f14673508350/tools/autograd/templates/Functions.cpp#L1797) needs to be modified.
To demonstrate this, the following implements a custom gradient of symeig where the backward pass return value is changed slightly.

This prints

This agrees with numerical differentiation.

So, should the implementation of symeig_backward be changed?

## Environment

Output from environment collection script:



cc @vishwakftw @SsnL @jianyuh"
383,24675,0,"Migrate `_multinomial_alias_setup` from the TH to Aten (CPU). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
132,13494,0,"Throws error when backward through sum() twice. ## 🐛 Bug
 is special that we can backward() through it multiple times since it doesn't have saved variable, so nothing was really destroyed after the first backward.

This behavior is bad as we might silently backward through it multiple times without noticing, but the grad accumulates.

## To Reproduce


@zdevito told me briefly how to fix, I will send a PR for it soon. 
"
463,28868,0,"How to build caffe2 with ONNX opset version greater than 9?. ## ❓ Questions and Help

Hello,
I've currently worked with freshly merged feature pytorch/vision#1401 and won't able to find a way to make Caffe2 work with ONNX operation set 10?

Is there a way to build a Caffe2 from source with this opset?
 "
545,3610,0,"High GPU memory usage on master. Running cyclegan from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix on master takes huge amount of memory (>11G with default options). It only takes 3.5G on 0.3 and 0.2 branches. 

The memory usage increases as I run more iteration. It seems some tensors are not freed properly.

I remember running on a pre-ATen-merge version without such issue.

Repro command:


By the way, this doesn't happen on the other model in the repo, pix2pix"
308,1362,0,"Feature Request: noise contrastive estimation/negative sampling. There isn't a standard loss function implementing this, even though it's pretty common. I am perfectly willing to implement it myself, if nobody else feels like it. It shouldn't be terribly complicated. I would structure it something like:
(N, C)C = num_classes(N)0 <= targets[i] <= C-1
The API for tensorflow also uses a parameter  so that by setting it to  you can switch to a negative sampling objective from nce. Is this worth doing?

Is this something that already exists? Is this something worth implementing?"
393,3231,0,".numpy() conversion problem. Converting an ndarray to a buffer and back works as expected, but not if the ndarray was given by  where t is a 

This works:



This doesn't:

"
42,5388,1,"Perf regression: indexing 1-d tensor. After the Tensor/Variable merge we have a perf regression with indexing a 1-d tensor:

As of 6a2afe3



As of Tensor/Variable merge:



Note that indexing a Python list is even faster:



This affects things like:
https://github.com/pytorch/examples/blob/4ef2d4d0c8524372d0047e050065edcac665ce1a/word_language_model/data.py#L39-L46"
469,992,0,"loading GPU checkpoints on CPU with remapping is trying to load torch.cuda. As @jzbontar reports:
serialization storage remapping is trying to load somewhere in the codepath.

> I get an error when loading a cuda tensor on a CPU only machine.  Here is the code I use: https://gist.github.com/jzbontar/b50f8c9dd22e49ff68c7c91dad63166a. The error is: AssertionError('\nFound no NVIDIA driver on your system... Both machines use pytorch version 0.1.10."
558,2733,0,"Pytorch in multi-cpu cluster. Hi, 
I would like to use the distributed module to train a convolution net in a CPU cluster. Investigating your code, the function torch.cuda.device_count() is called in several places, and is used to populate the device_ids list. Since I don't have any GPU devices in my cluster, the method device_count() will always return 0 and any subsequent attempt to access device_ids[0] will result in an index exception.
Taking a naive path and changing device_count so that it always returns the number of nodes I intend to use then I get a different error: 

if not all(input.is_cuda for input in inputs):
   raise TypeError('Broadcast function not implemented for CPU tensors')

So I would like to ask you whether you have any plans to implement the distributed module to train networks in a multi-cpu cluster.

Many thanks"
726,16229,0,"test_dag_net_forking is flaky on ROCm. This was first surfaced in #15817 but I am filing a new bug for clarity.

Error looks like:



https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-devtoolset7-rocmrpm-centos7.5-test/5483/console

This seems to be specifically a ROCm failure. CC @bddppq @iotamudelta "
666,19366,0,"[ONNX] RNN activation function exported to ONNX with wrong string case. ## 🐛 Bug

If we export  module to ONNX, the  attribute is written is all lower case. Several backends assume these strings to be case-sensitive, where this can be a blocking issue during runtime. There is an open issue (https://github.com/onnx/onnx/issues/1934) in the ONNX repo to make this this explicit in the spec. PyTorch's ONNX exporter should also consider writing strings with correct case in  attribute. 

## To Reproduce

Steps to reproduce the behavior:

1. Create a simple model with  module.
1. Export this model to ONNX using 
1. Check the  attribute for the case of the string. As an example, the string will be 'tanh', where ideally it should be 'Tanh'.

## Expected behavior

The  attribute value should match the case specified in ONNX spec. 

## Environment
This can be seen in all environments."
352,17231,0,"testing.assert_allclose should assert tensors have the same devices. ## 🐛 Bug

This unit test should pass:

        ind = tor.LongTensor([110, 125, 235, 333, 404]).cuda()
        nd_shape = tor.LongTensor([10, 10, 10]).cuda()
        xy = array_to_nd_index(ind, nd_shape)
        result = [[1, 1, 0],
                  [1, 2, 5],
                  [2, 3, 5],
                  [3, 3, 3],
                  [4, 0, 4]]
        tortest.assert_allclose(result, xy)

However, I get:

        RuntimeError: expected type torch.LongTensor but got torch.cuda.LongTensor

Here's the solution I wrote in my own assert function, though it may be a bit permissive:

        if a.is_cuda and not b.is_cuda:
            b = b.cuda()
        elif b.is_cuda and not a.is_cuda:
            a = a.cuda()

## Environment

 - PyTorch Version (e.g., 1.0):    1.0.1
 - OS (e.g., Linux):    Windows
 - How you installed PyTorch (, , source):    pip
 - Python version:    3.6
 - CUDA/cuDNN version:    0.9
"
370,24145,0,"sccache crashes when building `Distribution.cu` on Windows. There are many occurences of this build error in Azure Pipelines.
https://dev.azure.com/pytorch/PyTorch/_build/results?buildId=3891
https://dev.azure.com/pytorch/PyTorch/_build/results?buildId=3901
https://dev.azure.com/pytorch/PyTorch/_build/results?buildId=3695

Any ideas, @yf225?"
118,22764,0,"[feature request] torch.hypot.  returns real and imaginary components as the last dimension. Two very frequent functions to consume it are:  and  (in NumPy). Abs is similar to regular , so meanwhile complex tensor support is not developed,  could do 's job.

NumPy also has a , although it accepts two arrays. We may have two versions: one accepting two arrays, another accepting only one array and a dim. Essentially,  is a version of , specialized for two element vectors only."
88,21462,1,"Slow convolution with large kernels, should be using FFT. ## 🐛 Bug

When using  with a large kernel size (1024 for instance) on gpu, the cudnn implementation is very slow and gets slower as I increase the kernel size. I thought it was using FFT but apparently not. If it were using FFT, the computation time should be independent of the kernel size, because the kernel is anyway padded to the length of the input.

I have tried benchmarking with both  set to  and . My implementation using the FFT is significantly faster especially when using a stride of 1. You will find hereafter the code both for the FFT based convolution implementation I use and the profiling. My implementation is within ~5e-5 of the reference implementation for random weights and input. For a kernel size of 1024, with 64 channels, a stride of 1 and an input of length 64000, the default implementation is about 20 times slower than the FFT based one. When using a kernel size of 2048, it is 40 times slower.


## To Reproduce

Steps to reproduce the behavior:

1. Copy the code below in 
2. Run 

torch.cuda.synchronize()

## Expected behavior

When using a stride of 1 and large kernel size, the FFT implementation is much faster than the default one. The FFT one takes 160ms whatever the size of the kernel, versus 3.3 seconds (resp 6.7) for the default one with a kernel size of 1024 (resp 2048).  For large strides, the cudnn implementation is competitive or faster as expected (the FFT only has an interest if we want the convolution for all positions).
I would expect cudnn to provide a fast implementation for large kernels with low stride, which can be especially useful in audio (filters implementation). When talking about this around me, most people were surprised as it has been announced that an FFT based implementation was added to cudnn.

## Environment

Collecting environment information...
PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.13.4

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 410.79
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.15.4
[pip3] torch==1.1.0
[pip3] torchvision==0.2.2
[conda] blas                      1.0                         mkl
[conda] mkl                       2018.0.3                      1
[conda] mkl_fft                   1.0.6            py37h7dd41cf_0
[conda] mkl_random                1.0.1            py37h4414c95_1
[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch
[conda] torchvision               0.2.2                      py_3    pytorch


cc @csarofeen @ptrblck @mruberry @peterbell10 @VitalyFedyunin @ngimel"
181,16027,0,"More readable error message for index error of nn.Embedding  in CUDA. ## 🚀 Feature
More readable error message for index error of nn.Embedding  in CUDA

## Motivation

If nn.Embedding received a tensor containing larger values than , raise an error as follows:



However, if nn.Embedding exists in CUDA, raises error message as follows:



This error message is too hard to know why the error is occurred.
I spent a lot of time to find out the reason, so I created this feature request."
28,160,1,"loss functions return wrong values. The following code outputs:
4, 0.4, 4
instead of
4, 0.4, 0.04

and there seem to be errors for other loss functions as well, like SmoothL1Loss.


"
106,2576,0,"CUDA multinomial is limited to 2^24 categories. As reported by @Moustapha6C. The following fails, because there are too many categories:



cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @ngimel @fritzo @neerajprad @alicanb @vishwakftw @nikitaved @vincentqb"
646,9264,0,"Dilated Conv3d segfaults (cpu). I have encountered a segfault when running  with dilation under pytorch 0.4.0 on the cpu for some specific tensor sizes. This is the code to reproduce the behavior:



And this is the stack trace from gdb:


And this is the environment information:


Thanks for your help!"
488,30717,0,"TestTorchDeviceTypeXLA: Could not start gRPC server. Excerpt from [this CircleCI build](https://circleci.com/gh/pytorch/pytorch/3768299) on the  branch:



cc @ezyang @gchanan @zou3519 @ailzhang"
507,30796,0,"How to Build pytorch with local protobuf rather than third_party/protobuf?. ## ❓ Questions and Help
I want to build pytorch with my own os built protobuf lib rather than third_part/protobuf, Which prefix to change, Can anyone help me?


### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
"
108,15617,0,"Feature request: transposed locally connected layer. ## 🚀 Feature

transposed version of locally connected layer, similar to the transposed version of convolution layer but without weight sharing.

## Motivation

Similar to the necessity of the transposed convolution layer,  the locally connected layer (issue #499, PR #1583) should also have a transposed version.



cc @albanD @mruberry @jbschlosser"
199,20101,0,"jit tracing error for nn.Sequential with nn.Conv2d in torch 1.1.0 . ## 🐛 Bug

 
when tracing nn.Sequential with nn.Conv2d in torch 1.1.0 

## To Reproduce

Steps to reproduce the behavior:



Raises the following error 


## Expected behavior

Expected to convert without issues

## Environment

PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.04.2 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.10.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
cuDNN version: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5

Versions of relevant libraries:
[pip3] numpy==1.16.2
[pip3] numpy-image-widget==2019.1.6
[pip3] torch==1.1.0
[pip3] torchfile==0.1.0
[pip3] torchvision==0.2.1
[conda] Could not collect
"
238,5302,0,"Slow first .cuda() call. I upgraded to v0.3.1 using conda and have since experienced really slow performance the first time I call .cuda(). I tried solutions listed [here](https://discuss.pytorch.org/t/model-cuda-takes-long-time/102/20) and [here](https://discuss.pytorch.org/t/model-cuda-takes-long-time/102/20) but no luck. I then tried installing the latest version from source but still experienced the slowdown. Any help would be hugely appreciated!

Info
- OS: CentOS 7.4.1708
- PyTorch version: tried 0.3.1 (conda) and 0.4.0a0+6279367 (source)
- How you installed PyTorch (conda, pip, source): conda, source
- Python version: 3.6.3
- CUDA/cuDNN version: 8.0.61, 7.0.2
- GPU models and configuration: 1080ti
- GCC version (if compiling from source): 7.2.0"
320,28245,0,"PyTorch RPC should expose critical metrics to the application.. ## 🚀 Feature

Context for Model Parallel: https://github.com/pytorch/pytorch/issues/23110

## Motivation

When applications are using complex distributed primitives like RPC, RRef and Distributed Autograd, debugging issues can be cumbersome. We should have a way of exposing metrics to applications. This could simply be a  API that returns information about various things. The full list of metrics needs to be decided, although a few examples could be number of owner rrefs, number of user rrefs, RPC latency, Distributed autograd latency etc.

cc @ezyang @gchanan @zou3519 @jerryzh168 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528"
633,30076,0,"LibTorch(C++) with Cuda is raising an exception. I am trying to create NN with LibTorch 1.3 and C++ using Cuda 10.1 and Windows 10. For the build I am using Visual Studio 2019.

So far I tried basic examples and [MNIST example][1] with CPU which is working. However I cannot run it with CUDA. I tried to move model to GPU as it is described [here][2], but it is not working. 

> To move your model to GPU memory, you can write model.to(at::kCUDA);. Make sure the inputs to a model are also living in CUDA memory by calling tensor.to(at::kCUDA), which will return a new tensor in CUDA memory.

So I tried just simple

    int main(){
        auto net = std::make_shared<Net>();
        net->to(torch::kCUDA); //crashes here
    }

Then I tried to move simple Tensors to gpu memory but it crashes as well.

    #include <torch/torch.h>

    int main() 
    {
	    torch::Tensor a = torch::ones({ 2, 2 }, torch::requires_grad());
	    torch::Tensor b = torch::randn({ 2, 2 });
	    a.to(torch::kCUDA);    //Here it crashes
	    b.to(torch::kCUDA);    //
	    auto c = a + b;
    }

and I got:

    Exception thrown at 0x00007FFB8263A839 in Resnet50.exe: Microsoft C++ exception: c10::Error at memory location 0x000000E574979F30.
    Unhandled exception at 0x00007FFB8263A839 in Resnet50.exe: Microsoft C++ exception: c10::Error at memory location 0x000000E574979F30.

In debug mode it is pointing to  to

    auto operator()(Parameters... args) -> decltype(std::declval<FuncType>()(std::forward<Parameters>(args)...)) {
      return kernel_func_(std::forward<Parameters>(args)...);
    }

Using  shows it can find cuda device. 

Therefore I decided to build it from source but I it's not working either. It's related to [this](https://github.com/pytorch/pytorch/issues/30075#issue-525005329) post.

  [1]: https://pytorch.org/cppdocs/frontend.html#end-to-end-example
  [2]: https://pytorch.org/tutorials/advanced/cpp_export.html#step-4-executing-the-script-module-in-c"
560,6512,0,"Scalar operations are traced incorrectly. This code:

prints

because it's not using the tensor overload. We should change the dispatch code to never use the scalar overloads when they really are tensors."
3,537,1,"Initial call to .cuda() very slow with Titan X. The first call to .cuda() takes more than one minute when run on a system with a titan X:




Swapping a GTX 980ti on the exact same system results in normal timings (although with a very slight delay for the first call). On my laptop (GTX 960M), it is almost instant. 

"
97,20230,0,"[jit] torch.tensor doesn't support list of tuples. ## 🐛 Bug

Cannot create a tensor using  from a list of tuples.

## To Reproduce

Steps to reproduce the behavior:



gives the following error:


## Expected behavior

Should not give an error. Works in pure python.

## Environment
Pytorch 1.1.0

## Additional context

A workaround is to use a list of lists instead.
"
21,23156,1,"Pruning off NaN values in the gradient graph still produces NaN gradients.. ## 🐛 Bug

This is a niche bug, but it might cause troubles in advanced users who like to use masking to filter out NaN losses. Simply put, when NaN losses are masked out using , performing  on the sum of the losses should produce valid gradients (assuming that the gradient graph is smooth everywhere except for the masked losses). When 

## To Reproduce

Steps to reproduce the behavior:

Define the environment as follows. We will be backpropagating gradients to a very simple  layer.



Performing a forward inference on the linear layer and propagating gradients from there works as intended if no  is involved.



Suppose that  has  in one of its rows.



Performing backwards computation on the sum of the matrix above should
produce  gradients. This is expected.



However, using  or  index slicing on the leaf nodes to mask out problematic gradient graphs should not.


We can check whether masking out final losses works in cases where no  is involved.



The accumulated gradient in the latter case is less than the former because index slicing () on the computation graph prevented the third row in  from affecting the gradient computation.

## Environment

Collecting environment information...
PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.14.5
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy==1.16.3
[pip] torch==1.0.0
[conda] torch                     1.0.0                     <pip>

## Additional context

None.
"
418,6318,0,"Bug of ByteTensor. Here is an example which I want to do:

But this code will encounter an issue, which raise

So how can I judge if a Tensor in a list?
"
40,27902,1,"Slowdown due to thread specific model caching. ## 🐛 Bug

PyTorch appears to have an issue with threads that causes it to not properly warm up models if executed by different threads  in succession. This is why when using multi-instances of the model on the same GPU, it does not provide a speedup and in some cases is even slower than a single instance.

## To Reproduce

**Steps to reproduce the behavior:**

I have a simple libtorch code snippet that reproduces this issue. In short all it does is run the same model (Resnet50 model from the torchvision  that was traced to produce a torchscript version of the same). The code was run on a Titian V with a batch size of 1.

1. Create model in main thread
2. Launch thread that runs model N times in a loop, reporting the runtime each time
3. Launch N threads that each run the model once, reporting the runtime each time


## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: No
CUDA runtime version: 10.1.243
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.16.4
[pip] torch==1.2.0a0+afb7a16
[pip] torchtext==0.4.0
[pip] torchvision==0.3.0a0
[conda] magma-cuda100             2.5.0                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] torch                     1.2.0a0+afb7a16          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.3.0a0                  pypi_0    pypi


cc @ezyang @gchanan @zou3519 @suo"
508,123,0,"0 indexed random. What about having torch.IntTensor(10).random_(4) returning integers between 0 and 3 instead of 1 and 4, to make it consistent with numpy.random.randint?
"
431,31611,0,"the example program using libtorch is not linked against torch_cuda when USE_CUDA is ON. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

Looks like https://github.com/pytorch/pytorch/issues/15992 is coming back again after the split of the torch library.

## To Reproduce

Steps to reproduce the behavior:

1. The following script throws ""Torch is not linked against CUDA"" and gets 0 as output.

2. The following one is working correctly and gets 1 as output.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

The behaviour keeps consistent in code snippet 1 and code snippet 2.

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Windows
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): python setup.py build
 - Python version: doesn't matter
 - CUDA/cuDNN version: doesn't matter
 - GPU models and configuration: doesn't matter
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @ngimel @peterjc123"
17,22127,1,"Cpu memory leak when only doing model forwarding in a loop. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce
Cpu memory leak when only doing model forwarding in a loop
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

define the model as simple as :

and write the test program as:


## Expected behavior
look into the system monitor, the related python process's memory should be stable. but in fact the memory increase over time.
<!-- A clear and concise description of what you expected to happen. -->

## Environment

 - PyTorch Version ( 1.1):
 - OS (Linux & OSX):
 - How you installed PyTorch ():
 - Python version: 3.6 & 3.7
 - Any other relevant information: when using gc.collect(), the increasing becomes slow, however it won't help since i want to use LSTM layers(quickly out of memory)

## Additional context

<!-- Add any other context about the problem here. -->
I am writing a reinforcement learning program, during every episode(in loops) I will do model forwardings. This memory leak problem is vital. "
361,31277,0,"nn.MultiHeadAttention with different similarity measures. ## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

## Pitch

<!-- A clear and concise description of what you want to happen. -->

Current nn.MultiHeadAttention uses matrix multiplication similarity, i.e., (Q@K.t()), but variants of this similarity are not available directly, for example, 

dot product similarity, i.e., 
, 

additive similarity, 
, 

general dot product similarity, 
.

These variants should also be in PyTorch.


## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->


cc @zhangguanheng66"
597,27626,0,"Cross entropy with sequence log_softmax not (almost) equal to 1. ## ❓ Is this a feature or a bug

When using the Cross entropy with K-target class in a sequence application, is it expected to squeeze before calling the loss or we are suppose to just past a vector of minibatch_size, sequence_length, K-tag ?

Because, in this line of code   the  step is giving me prob vector greater than 1 or not (almost) equal to 1. 
> After the exponantial application for sure.

Here an example of my situation, when 
"
141,3652,0,"Fix far-fetched Boolean ambiguity for byte tensor with one value in it.. Testing non-empty tensor in an if statement will unconditionally raise a RuntimeError stating that ""bool value of non-empty torch.ByteTensor objects is ambiguous"". But when there's only one value, it's bool value is pretty straightforward.
What do you think?"
659,2344,0,"Does CosineEmbeddingLoss support CUDA tensors?. Noticed this as I tried to use the CosineEmbeddingLoss with a model copied to the GPU.



My default assumption is that I'm using cuda() wrong in some way. Thoughts?"
401,20675,0,"Windows 10 CUDA 9 CUDNN 7.5 Pytorch 1.1 CUDNN_STATUS_EXECUTION_FAILED. ## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
Firstly I tested tensorflow-gpu 1.12, it works on CUDA GPU well. 
Then I followed pytorch tutorial to setup a simple CIFAR10 net, it works well on cpu. I tested cuda&cudnn is_available, both of them return True, however the gpu-enabled CIFAR10 net failed on gpu with exception CUDNN_STATUS_EXECUTION_FAILED.

Inputs is cudnn acceptable:  True
Traceback (most recent call last):
  File ""D:/Python/pytorch/tutorial/tut_4_classifier_gpu.py"", line 97, in <module>
    outputs = net(inputs)
  File ""D:\Python\Python36\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""D:/Python/pytorch/tutorial/tut_4_classifier_gpu.py"", line 62, in forward
    x = self.pool(F.relu(self.conv1(x)))
  File ""D:\Python\Python36\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""D:\Python\Python36\lib\site-packages\torch\nn\modules\conv.py"", line 320, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
"
697,11751,0,"Segfault in autograd using hook. ## Issue description

I found several errors in autograd:
* Segfault during / when using hook (e.g. through ) on non-reachable tensor, whose grad is implicitely calculated () because it is an output of a function in the gradient graph but is independent of the backprop root tensor. (See code and traceback below.)
* No hook is called if this non-reachable tensor is an output of an index operation (e.g.  while root only depends on  and complete  has .) (See code below.) That issue is not related to the others, but I encountered it in the same run, so I want to mention it here, too.
* If such a hook is called (e.g.  from ), the  argument is  but should be a tensor with -values, because that is the actually used value for the required  and therefore should be modifiable. (See code below.)

Here is the traceback for the Segfault: (Notice the lines . But I think, the actual source of the problem is, that in [this](https://github.com/pytorch/pytorch/blob/6f6b03566ba3c4828f6ee87a772f9d161be0bae7/torch/csrc/autograd/engine.cpp#L447) line, the inputs are not but should be initialized as variables with -values, as fallbacks if no function overrides them. Or am I wrong?)

I really would like to fix this, but to be honest, I'm not sure if I have enough expertise to do it right. So I open this issue for others. I hope, it helps.

## Code example

a0grad = 0a1a0a.grada0grad = None= 0a1a0Segmentation faultgradNone0.backward().grad()


## System Info

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): I tested two versions (same errors for both): current master from source, v0.4.1 via pip
- Build command you used (if compiling from source): 
- OS: Linux Mint 18.3 Sylvia
- PyTorch version:  I tested two versions (same errors for both): current master, v0.4.1
- Python version: 3.6.6
- CUDA/cuDNN version: None
- GPU models and configuration: No CUDA
- GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
- CMake version: version 3.12.0
- Versions of any other relevant libraries:
[pip] 18.0"
409,8282,0,"Loaded network with load_state_dict has different shape but works anyway. After it was verified on [discuss.pytorch](https://discuss.pytorch.org/t/loaded-network-has-different-shape-but-works-anyway/19398) that this is indeed unwanted behaviour, I am forwarding this to you:

## Issue description

I trained a model with among others had the following layer:

and then saved it to a file with state_dict and torch.save.

Then, when I wanted to load that model using load_state_dict, by accident the same layer was setup as follows:


Nevertheless, the model was loaded without error. It seemed that the weights were just duplicated 32 times, but I have not verified. So the question is how this is consistent with API documentation. I have not found a statement that says load_state_dict would somehow fix shape inconsistencies automatically. It seems you have a documentation vs reality mismatch here. (Now you need to decide which one to fix)

## Code example



Provided by discuss .pytorch user ptrblck

## System Info
pytorch 0.4 release

"
260,19126,0,"weight_norm doesn't support eta and returns nan for zero weights. ## 🐛 Bug

backprop on weights generated with torch._weight_norm that are zero filled yields nan gradients. I don't see a way to add an eta to the norm to prevent this.

## To Reproduce

Steps to reproduce the behavior:



I'm encountering nan's during backprop during training of a network with weight normalization. From this seemingly related thread it sounds like the advice is to add an eta to the norm, but in this case the norm is generated in pytorch's c++ implementation and I don't see an obvious way to do this.

## Expected behavior

I expect there to be a way to generate non-nan gradients for weight-norm weights that are zero filled

## Environment

 - PyTorch Version (e.g., 1.0): 1.0.0
 - OS (e.g., Linux): Mac and Centos
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.6
 - CUDA/cuDNN version: None
 - GPU models and configuration: None
 - Any other relevant information:

## Additional context



cc @albanD @mruberry"
472,12127,0,"how to use the checkpoint function. I  try to use the checkpoint function,when I train,gpu keeps the memory but don't work  and the train progress don't work either(without any error print). How can I make the checkpoint function work.
"
382,31657,0,"Use of Sequence collections for abstract classes in Dataset. ## 🚀 Feature
Using the sequence (or any other similar Python abstract class) for the  class in order to tackle the note in lines 30-32 of :

def __len__(self)__len__

By using the  abstract classes, we can see in the official Python documentation that they have the exact abstract methods that the  class expects, i.e.  and .

## Motivation

My motivation is three-fold: (1) tackling the note mentioned above left by the PyTorch contributors, (2) let the code be auto-documented—since accessing this code states explicitly that you need to define those two methods and, (3) using Python abstract classes instead of inheritting from object (which is a rather deprecated practice for when Python did not have abstract classes capabilities).

## Pitch

Implement the Dataset class using new Python abstract classes capabilities, or at least discuss about it.

## Alternatives

I currently do not have alternative proposals.

## Additional context

.


cc @SsnL"
342,23054,0,"ConcatDataset returns different error messages setting out of range plus index and minus index.. ## 🐛 Bug

ConcatDataset returns different error messages setting out of range plus index and minus index. 

## To Reproduce

## Expected behavior
I think it's better that x[100] and x[-100] have the same error message.

My way to solve this is to chage [these lines](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py#L197-L200) like this.


## Environment
 - PyTorch Version (e.g., 1.0): 1.1.0
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (, , source): pip
 - Python version: Python 3.7.3
 - CUDA/cuDNN version: None
 - GPU models and configuration: None

"
213,20855,0,"pytorch0.4 -> pytorch1.0.1 RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation. The code run well in pytorch  0.4 . But in pytorch1.0.1, I got the error  as following. How to locate the question and resolve it? Thanks very much.
 File ""G:/20190215-backup/bsandnielianToImage/optimizer_ExpAnglesplit_faceseg_splitBs_faceExpRecong.py"", line 215, in optimizer
    loss.backward()
  File ""E:\Program Files\Python35\lib\site-packages\torch\tensor.py"", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""E:\Program Files\Python35\lib\site-packages\torch\autograd\__init__.py"", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation"
278,15260,0,"MultiGPU for gru. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

During runtime of GRU under multi-GPU environment, there is a RuntimeError: Expected hidden size (3, 64, 12), got (3, 16, 12) where the first, second, and third arguments are the number of GRU layers, batch size and number of hidden units respectively. My model run well under single-GPU environment.

In Pytorch Forum, a question ""DataParallel LSTM/GRU wrong hidden batch size (8 GPUs)"" has been asked. One solution is to set batch_first to True for gru. But the error was still there after the setting. The correct solution is to store the hidden state inside the model rather than return it like the below code block(from user AnodyneCodeAsher Newcomer):




<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
My wrong code sample:

Please help to fix this bug. Thanks a lot.

## Environment
 - PyTorch Version 1,0:
 - OS Ubuntu 16.04:
 - How you installed PyTorch:
 - Python version: 3.6
 - CUDA/cuDNN version: 9.0





cc @zou3519"
724,14891,0,"Wrong recursive module::load. ## 🐛 Bug

Parameters and buffers must be checked recursively during the load procedure [module.cpp#L297](https://github.com/pytorch/pytorch/blob/8311bbee7f1dd33346f18c769cfeb8c5b5941874/torch/csrc/api/src/nn/module.cpp#L297). 
For example, like this:
"
173,3293,0,"Tensor constructor passed numpy ndarray does not check type.. 

According to @colesbury, however, it does look like ndarray passed into Tensor is doing the correct codepath.

CC @aszlam"
438,24568,0,"Migrate `ge` and `ge_` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
247,14365,0,"Assertion fails when using DataParallel with two nn.Embedding . ## 🐛 Bug
I'm using the nightly build: 1.0.0.dev20181123. This issue is very similar to #13569 .  When I instantiate two nn.Embedding, with DataParallel and with max_norm=1.0, I get the following assert


If I remove the self.lut_dummy, the issue disappears. 

## To Reproduce


## Output
1.0.0.dev20181123
/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py:25: UserWarning:
    There is an imbalance between your GPUs. You may want to exclude GPU 0 which
    has less than 75% of the memory or cores of GPU 1. You can do so by setting
    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES
    environment variable.
  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))
ok1
ok1
ok2
Traceback (most recent call last):
  File ""error_train.py"", line 25, in <module>
    main()
  File ""error_train.py"", line 22, in main
    output = model(src)
  File ""/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 479, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 143, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py"", line 153, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 83, in parallel_apply
    raise output
RuntimeError: output_nr_ == 0 ASSERT FAILED at /pytorch/torch/csrc/autograd/variable.cpp:196, please report a bug to PyTorch.

## Additional context
Observations
1. If I comment out the self.lut_dummy, the issue disappears. Though in my real model, I am using the .
2. If max_norm is removed, the issue disappears again.
3. If I set os.environ[""CUDA_VISIBLE_DEVICES""] to ""0"" or ""1"", again, the code works just fine.


cc @ezyang @gchanan @SsnL @albanD"
673,29293,0,"Using tensor cores. I would like to know if I build PyTorch on SM_70 or SM_75, will it automatically use tensor cores or I have to explicitly specify something during the compilation or before run.
Any comment?"
751,22697,0,"[docs] torch.lerp typo. ## 📚 Documentation

https://pytorch.org/docs/master/torch.html#torch.lerp



Should probably be  must be broadcastable at the end.
"
635,16501,0,"[sparse sum] Sparse sum over dimmension gives unexpected results.. ## 🐛 Bug

When summing over dimension 0 of tensor of 2 dimensions I'm getting a scalar, whereas summing over dimension -2 gives the correct answer. Is that expected?

## To Reproduce



## Expected behavior




## Environment


"
446,25171,0,"Relation between AVX and TH?. Hi, I'm studying PyTorch internals, especially trying to find AVX implementation. (not AVX2.)

I built PyTorch from source with

What I've figured out is the AVX intrinsic function that is called for simple tensor addition.


I debugged with gdb, and set some breakpoints and succeeded it. Below is the result of  at gdb.


The AVX intrinsic  is called at #0

Now, I'm trying to find if there exists AVX acceleration of CNN, so I've debugged really simple code at [PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) with same procedure as above. But I cannot find any AVX implementation.

So now I'm wondering if I've missed some options when I built PyTorch. I've googled a lot, and looking inside the CMake system, I haven't figured it out yet.
One I noticed it is there are AVX.cpp in TH directory.

And after searching TH, I also noticed that I don't have libTH.so file. In [a blog in PyTorch homepage](https://pytorch.org/blog/a-tour-of-pytorch-internals-2/#backend-torch-and-vendor-libraries), there is libTH.so.1 at torch/lib. Below is my  result at that directory.


In addition, in [this blog](https://apaszke.github.io/torch-internals.html), there is ""simd"" directory in , but I don't have it. Is there anyone who can tell me about what I'm missing?



"
467,5198,0,[feature request] support batch diag. Should be easy to implement. But we need to figure out a good name/API. We can't reuse  as it will be ambiguous when input is 2D.  sounds weird to me..
6,26165,1,"Memory leak in multithreading environment when loading checkpoint. ## 🐛 Bug

I have a problem when loading saved checkpoint for pytorch model in seperate thread. CPU memory keeps increasing and is never released.

Multi-threading version of script increases RAM usage with each iteration and ends with 978 RESV memory (htop output).
Single-threading version holds with 374 RESV memory (htop output).
When I randomly initialize model (without state_dict loading) both versions use the same (smaller) amount of memory.

## To Reproduce



## Expected behavior

I expect the RAM to be cleared as no reference is being held to created models.

## Additional info

I have traced the issue to  https://github.com/pytorch/pytorch/blob/33221b19acc3dcacb11c38fdbff65d9a6ce90866/torch/nn/modules/module.py#L775
Commenting out this line makes the multi-threading version behave as single-threading one.

## Environment

 - PyTorch Version: 1.2.0
 - OS: Ubuntu 18.04
 - How you installed PyTorch: pip
 - Python version: 3.5.7


cc @ezyang @gchanan @zou3519 @jerryzh168"
332,28938,0,"Cannot select version in the tutorials page. ## 📚 Documentation

The documentation pages can be viewed for different versions using the [versions page](https://pytorch.org/docs/versions.html). However, when navigating to the ""Tutorials"", the versions cannot be selected. This is a problem for the experimental parts (s.a. quantization), as we update the tutorials in master, which cannot be viewed."
586,27296,0,"document: torch.quantize_per_tensor and torch.quantize_per_channel. add quantized tensor creation functions on torch.tensor( ) documentation, add the torch.quantize_per_tensor() and torch.quantize_per_channel(). 


@raghuramank100 for visibility"
607,1228,0,"Import torch causes error. This might be relevant to #691. I have installed pytorch from source on Mac OS successfully, but  causes error:
> 
>
> ImportError                               Traceback (most recent call last)
> <ipython-input-1-c031d3dd82fc> in <module>()
> ----> 1 import torch
> 
> /Users/dqwang/Study/tools/anaconda2/lib/python2.7/site-packages/torch/__init__.pyc in <module>()
>      51 sys.setdlopenflags(_dl_flags.RTLD_GLOBAL | _dl_flags.RTLD_NOW)
>      52 
> ---> 53 from torch._C import *
>      54 
>      55 __all__ += [name for name in dir(_C)
> 
> ImportError: dlopen(/Users/dqwang/Study/tools/anaconda2/lib/python2.7/site-packages/torch/_C.so, 10): Symbol not found: __ZNSs4_Rep20_S_empty_rep_storageE
>   Referenced from: /Users/dqwang/Study/tools/anaconda2/lib/python2.7/site-packages/torch/_C.so
>   Expected in: flat namespace
>  in /Users/dqwang/Study/tools/anaconda2/lib/python2.7/site-packages/torch/_C.so
> 
The author of that thread got away with the issue by installing from conda package rather than source, which solved my case as well. But I have to install from the source because of I need CUDA."
139,31410,0,"can't close torch.utils.tensorboard.SummaryWriter in __del__. The program can not exit (print 'closed') when I try to close the  in .

The program can exit if the  is not in .

Python 3.7.5

Pytorch 1.3.1

tensorboard 2.0.0

OS: Ubuntu and Win10"
54,24373,1,"torch.as_tensor is very slow. ## 🐛 Bug
## To Reproduce

This code:

prints


I don't expect  to be 100x slower than .

## Environment


"
25,25243,1,"load pretrained model(trained in torch0.4) and got nan loss in torch1.2. Is it possible to cause Nan loss in pytorch1.2 when i load pretrained model trained in pytorch0.4?
Because pytorch1.2 support sync-bn while pytorch0.4 does not.
I have tested just random init and did not load pretrained model and the nan loss did not appear"
542,30321,0,"dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/sccache. All OS X builds are failing due to this https://app.circleci.com/jobs/github/pytorch/pytorch/3698524

CircleCI ticket: https://support.circleci.com/hc/en-us/requests/63093"
161,16392,0,"squeeze operation edge case . ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

I found an inconsistent behavior with 

> tensor.squeeze()

, it could be that it was intentional and if so, I can't understand the reason for it.

## To Reproduce

1 ) 



2) 



In the second example, I hoped to get


I'd be glad to know whether it was a bug, and if not, why.

Cheers!
Shiran"
471,2240,0,"Error building torch from source for python 2.7 on macOS. Hello,

Here is the error.
I'm running macOS 10.12.6 (16G29), python 2.7 on anaconda 3, CUDA 8.0.90, Apple LLVM version 8.0.0 (clang-800.0.42.1). I installed LLVM 8.0.0 specifically since it's the one listed on cuda docs.
I have successfully built pyTorch from source for python 3.6 on this computer before, so I'm clueless how this is happening.
I will just use python 3 for now. Hope this information is helpful to you.


"
672,24711,0,"Migrate `hardtanh_backward` from the TH to Aten (CPU). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
592,2754,0,"ppc64le test_cuda.py failures. I am using ppc64 Ubuntu 16.04. 
I compiled pytorch version 0.2.0 and am running into some problems with test_cuda.py.
To isolate the failures I commented out some of the types and float_types like so:

types = [
\#    torch.FloatTensor,
\#    torch.DoubleTensor,
\#    torch.LongTensor,
\#    torch.IntTensor,
\#    torch.ShortTensor,
    torch.CharTensor,
\#    torch.ByteTensor,
]

float_types = [
   torch.FloatTensor,
\#   torch.DoubleTensor
]  # TODO: add half...

Then I run ""python test_cuda.py""
I get a bunch of these errors:
/opt/pytorch/torch/lib/THC/THCTensorTopK.cuh:431: void gatherTopK(TensorInfo<T, IndexType>, IndexType, IndexType, IndexType, IndexType, TensorInfo<T, IndexType>, IndexType, IndexType, TensorInfo<long, IndexType>, IndexType) [with T = char, IndexType = unsigned int, Dim = 3, Order = true]: block: [64,0,0], thread: [9,0,0] Assertion  failed.

the only difference in each line are the block and thread data 

I also get a bunch of these for different tests:

  ERROR: test_CharTensor_tril (__main__.TestCuda)

----------------------------------------------------------------------
Traceback (most recent call last):
  File ""mytest_cuda.py"", line 358, in tmp
    gpu_tensor = to_gpu(cpu_tensor)
  File ""/opt/pytorch/test/common.py"", line 89, in to_gpu
    return obj.clone().type(t)
  File ""/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/torch/_utils.py"", line 35, in _type
    return new_type(self.size()).copy_(self, async)
RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/pytorch/torch/lib/THC/generic/THCTensorCopy.c:18

all failing at the same THCTensorCopy.c:18

All these failures only happen with type = torch.CharTensor

here is output of nvidia-smi
\# nvidia-smi
Fri Sep 15 21:54:54 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.66                 Driver Version: 384.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:03:00.0 Off |                    0 |
| N/A   45C    P0    59W / 149W |    426MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |
| N/A   30C    P8    30W / 149W |      1MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K80           Off  | 00000002:03:00.0 Off |                    0 |
| N/A   32C    P8    26W / 149W |      1MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K80           Off  | 00000002:04:00.0 Off |                    0 |
| N/A   25C    P8    29W / 149W |      1MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+

"
517,8388,0,"Running simultaneous DataParallels can potentially result in locked models. I observed that running simultaneous DataParallels might result in at least one of the models being unable to progress at all.



System configuration:
- PyTorch 0.4.0 stable release, installed using 
- EC2 P2.8xlarge (8 x K80s)
- CUDA 8
- Python 3.6



On a separate machine with 8 x Titan X, the following happened after I tried running multiple parallel models:


There's RAM that's somehow unaccounted for."
96,27094,0,"[jit] Document what types can be traced. Our  docs don't mention that  can be traced, nor that NamedTuples cannot be traced

cc @suo"
103,13618,0,test_dist_broadcast_coalesced_nccl timeout on CI
412,15328,0,"torch.gesv forward handles singleton dimension, but backward doesn't. ## 🐛 Bug

I'm running in to an issue where  accepts a  right hand side (with an  left hand side) in the forward pass, but the backward pass will fail. Unsqueezing the right hand side to  and then squeezing the output gets me the same size output (), but the backward pass also works.

## To Reproduce



## Expected behavior

Either (a) torch.gesv should require  to have the trailing singleton dimension in the forward pass (e.g. be  or (b) the backward pass should work if  doesn't have the singleton dimension.

## Environment

Reproducible on  installed via conda.
"
437,26893,0,"onnx export: einsum not supported. ## 🐛 Bug

Exporting models which use  in their  method does not work


## To Reproduce

This is a minimal example:


which gives 

s

cc @matthewfeickert @kratsg"
596,17537,0,"Cannot initial from_blob from std::vector.data(). ## 🐛 Bug


## To Reproduce




I end up getting giberrish tensor returned from this function. Once the function returns to Python
"
124,19201,0,"How do I edit a .t7 file?. I currently have a .t7 file which contains some arrays and raw text, but i want to reduce the number of entries in there. How do I edit the file?

So far, I can only display the file's contents using
"
227,4387,0,"cannot allocate memory. the problem arise when run after 1epoch, but works well in first train epoch as well as the  followed 1st validate epoch. batch_size is 8 and work_number is 2。why this happened ?
File ""/home/luhongchao/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataload
er.py"", line 301, in __iter__
    return DataLoaderIter(self)
  File ""/home/luhongchao/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataload
er.py"", line 158, in __init__
    w.start()
  File ""/home/luhongchao/anaconda2/lib/python2.7/multiprocessing/process.py"", line 130, 
in start
    self._popen = Popen(self)
  File ""/home/luhongchao/anaconda2/lib/python2.7/multiprocessing/forking.py"", line 121, 
in __init__
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory"
747,1738,0,"How do make batchnorm to maintain multiple pairs of running_mean and running_var?. Hi,

The current batchnorm implementation only has one pair of running_mean and running_var. However, some modules in the neural networks may be used twice or more in one network forward. Based on my experimental experience and also stated in paper https://arxiv.org/pdf/1603.09025.pdf , different running_mean and running_var should be computed for different iterations(or time steps). I have found in my experiments, for the same testing data, net.eval() performs much worse than net.train() (backward is turned off here). The only difference in this situation is different running_mean and running_var are used. So how to make the batchnorm to keep multiple running_mean and running_var and I can choose to use the first pair in the first iteration, the second pair in the second iteration, and so on?"
400,27769,0,"Nvidia cuda documentation link renders wrong. Some of our docs link to ""CUDA documentation"" render incorrectly:

![image](https://user-images.githubusercontent.com/5652049/66680833-414d2d00-ec26-11e9-8347-5da72167ccb5.png)


cc @ezyang @gchanan @zou3519"
312,30965,0,"JIT breaks with postponed annotations. Targetting the correct issue this time, sorry for the noise

## 🐛 Bug

As per [PEP 563 (Postponed Evaluation of Annotations)](https://www.python.org/dev/peps/pep-0563), typing annotations are not automatically evaluated as definition time starting with python 3.7 when using .

The solution is to avoid using   directly in [](https://github.com/pytorch/pytorch/blob/master/torch/jit/_recursive.py#L74) but to call [](https://docs.python.org/3.7/library/typing.html#typing.get_type_hints__future__` call will also be correctly evaluated. Should I make a PR?

## Testcase



This fails with traceback



cc @suo"
566,23826,0,[quantization] Enable quantization OSS tests. These weren't run before. In progress: https://github.com/pytorch/pytorch/pull/23718
518,17315,0,"Autograd inside hooks is disabled by default?. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

I was playing around  with the   and ""intermediate gradients"". And discovered strange behaviour (as for me).  Looks like by default  is disabled inside hooks (using  to extract intermediate gradients). But why? To avoid a kind of ""backward inside backward""?

## To Reproduce

### Plain hook works fine.


Output:


### Autograd inside hook fails.


Output:

Ok, it fails because  there is no  for . Looks like we need to turn on .

### Autograd inside hook with  seems to work fine.


Output:

Now it woks fine. In addition, outside of hooks with gradients it works as usual.

### Autograd outside of hooks works as expected.


Output:

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behaviour
I expected   to work as usual inside hooks ().
I am not sure is it a bug or smth. I'm just worrying, why does it behave in such way?
<!-- A clear and concise description of what you expected to happen. -->

## Environment
"
275,7569,0,"[Caffe2]How to set lr_mult and decay_mult in Conv layer?. In facebookresearch/detectron, I see a conv layer is add by:

conv_rpn_fpn = model.Conv(
         bl_in,
         'conv_rpn_fpn' + slvl,
         dim_in,
         dim_out,
         kernel=3,
         pad=1,
         stride=1,
         weight_init=gauss_fill(0.01),
         bias_init=const_fill(0.0)
 )

However, I don't find any parameter statement about lr_mult and decay_mult like that in a caffe conv layer. Could you please give me an example? Thanks a lot!







If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

Provide a short description.

## Code example

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source):
- Build command you used (if compiling from source):
- OS:
- PyTorch version:
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
"
305,7343,0,"[memory leak] [PyTorch] .backward(create_graph=True). 

leaks with  but not with .

Discovered when running code in #7270 

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved"
286,28323,0,"[quantization] fix the seed for hypothesis in CI. Currently the hypothesis is semi-random on CI. We need to fix the seed to something constant.

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100"
293,6998,0,"[pytorch] Not handling python reload properly. Trying to reload some modules cause crashes:

On python 2.7 (crash with ubuntu release, error messages from debug build).

type->tp_flags & (1L<<9)' failed.
Aborted (core dumped)

>>> from torch import autograd
>>> reload(autograd)
terminate called after throwing an instance of 'std::runtime_error'
  what():  generic_type: cannot initialize type ""ProfilerEvent"": an object with that name is already defined
Aborted (core dumped)
`

@colesbury can you see an obvious reason for this? Or should I look into more details?"
87,19407,1,"torch.max slow for some inputs. ## Issue description

It seems that  can be quite slow depending on input. For example, sorting a tensor before calling  can make it miserably slow. Note that this issue exists even for non-pathological cases where input is not fully sorted. This was originally observed in my beam search implementation that seemed to run faster than greedy decoding at times.

## Code example

Here is a minimal example highlighting the issue. It's framed in terms of a greedy decoding algorithm:



The following are timing results from a number of different GPU configurations. Note that when the input to  is sorted it is always slower than , otherwise it is faster. The speed difference is definitely architecture dependent. Additionally,  is much more stable in terms of speed regardless of sorting.

Timings for a Titan X Pascal:


Timings for a 1080ti:


And finally timings when running on Google Colab (Tesla T4):


## System Info
This is from the Google Colab system:

"
133,11737,0,"torch.utils.cpp_extension.load doesn't change device after moving the model. ## Issue description
After using torch.utils.cpp_extension.load, returned module always puts tensors at GPU 0 ('cuda:0')
It causes problems when model is moved to another GPU. 
In my example I used code from https://github.com/mapillary/inplace_abn
See also mapillary/inplace_abn#52

## Code example

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 8.0.61

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 4.9.4-2ubuntu1~16.04) 4.9.4
CMake version: version 3.12.0

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti

Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/local/cuda-8.0/lib64/libcudnn.so
/usr/local/cuda-8.0/lib64/libcudnn.so.6
/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21
/usr/local/cuda-8.0/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0/lib64/libcudnn_static.a
"
407,3001,0,"Trying to do advanced indexing using a Variable causes a hang. **Minimal test case:**

    import torch
    x = torch.autograd.Variable(torch.FloatTensor([[0.1, 0.2], [0.3, 0.4]]))
    idx = torch.autograd.Variable(torch.LongTensor([1, 0]))
    print(x[idx, :])

**Expected output:**

Either:

    Variable containing:
     0.3000  0.4000
     0.1000  0.2000
    [torch.FloatTensor of size 2x2]

(the above is what is printed if I replace the last line with )

...or an immediate exception traceback informing me that advanced indices must be a  and not a .

**Actual output:**

The program initially hangs. *After* I interrupt the program with ^C, the following exception is displayed:

    Traceback (most recent call last):
      File ""loop.py"", line 5, in <module>
        print(x[idx, :])
      File ""/home/.../lib/python3.5/site-packages/torch/autograd/variable.py"", line 76, in __getitem__
        return Index.apply(self, key)
      File ""/home/.../lib/python3.5/site-packages/torch/autograd/_functions/tensor.py"", line 16, in forward
        result = i.index(ctx.index)
    IndexError: When performing advanced indexing the indexing objects must be LongTensors or convertible to LongTensors

**More details:**

From some digging around in pdb, it looked like [](https://github.com/pytorch/pytorch/blob/master/torch/autograd/variable.py#L78) was tail-calling itself via C code. While I couldn't identify the full stack, it looks like it eventually made its way to [this attempt to convert the  index to a ](https://github.com/pytorch/pytorch/blob/master/torch/csrc/generic/Tensor.cpp#L785). Sure enough, this also hangs:

    torch.LongTensor(idx)

Perhaps the  constructor is trying to treat the  as a iterable, and getting stuck because iterating through a  returns a sequence of ? (Happy to dig further, if someone can point me to the logic for initializing a .)

Python 3.5.2, GCC 5.4.0 on Linux,  = '0.2.0_3', no CUDA"
31,20146,1,"CPU Memory leak when using weight_decay in libtorch. ## Bug
When using weight_decay in libtorch, CPU Memory usage is slowly increasing.

## To Reproduce
I used docker container ""nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04"", stable libtorch(1.1) for cuda9.0. This phenomenon can be reproduced using the mnist example in the pytorch/example repository.

rewrite examples/cpp/mnist/mnist.cpp l.148



Because the speed of increase is very slow, it may be better to increase the number of epochs. This happens with both CPU learning and GPU learning.

## Environment
- OS: Ubuntu 18.04.2 LTS
- GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CMake version: version 3.10.2
- CUDA runtime version: 9.2.148
- GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
- Nvidia driver version: 410.104
- cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1

## Additional context
This phenomenon happens also in cuda10.0 and libtorch nightly build for cuda10.0."
62,22866,1,"High CPU usage by torch.Tensor. ## 🐛 Bug

Pytorch >= 1.0.1 uses a lot of CPU cores for making tensor from numpy array if numpy array was processed by np.transpose. The bug is not appears on pytorch 1.0.0. Nightly build has the same bug.

## To Reproduce

Steps to reproduce the behavior:

1. Install Pytorch >= 1.0.1
2. Run following code 

3. Open htop and enjoy 2500% CPU utilization by the process that is running the code.


This code (without np.transpose) works just fine - not more than 100% CPU utilization


Same abnormal behavior with torch.permute if you call it before tensor.cuda().


torch.permute after tensor.cuda() works just fine  - not more than 100% CPU utilization


## Expected behavior

I'm expecting utilization not more than 100%. 

## Environment



## Additional context

I've tried the same code on other server. On this server CPU usage is not increased so drastically but still shows 200% utilization with np.transpose and not more than 100% without.
"
253,25775,0,"mean/sum(dtype) arg matching gives bad error message with positional dtype arg. ## 🐛 Bug

 is broken yet  works. The error message is confusing as well.

![Screenshot 2019-09-06 11 11 08](https://user-images.githubusercontent.com/5674597/64439027-1efc4880-d097-11e9-8c00-eb420962d78e.png)
"
375,3542,0,"Minor: docker build fails on submodule because of gitdir with absolute path name (problem with git v 2.7/2.8, fixed in git v 2.9). error message is 

Reason:  pytorch was cloned with --recursive (as recommended) into   and the submodule stores in file 

the absolute pathname as

But when 
 
is run to copy  everything into  the absolute pathname won't match - hence the error.

This is apparently a know problem in git, see https://lwn.net/Articles/691223/  and fixed in v 2.9 
So may be you can just put a note on the page telling people about this to save them time. 
"
618,30986,0,"Wrong result for CPU implementation (m,).addmv((m, 0), (0,)) when BLAS is not used. ## 🐛 Bug



gives



To be fixed in https://github.com/pytorch/pytorch/pull/30898"
263,31300,0,"torch runtime error when manual link libmkldnn.so. ## 🐛 Bug

I manual link libmkldnn.so, because I want to use some functions inside mkldnn, but pytorch get runtime error when I manual link libmkldnn.so

## To Reproduce

Steps to reproduce the behavior:


get runtime error


## Expected behavior

No runtime error

## Environment
PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.17.3
[pip3] torch==1.3.0
[pip3] torchvision==0.4.1
[conda] Could not collect


cc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh"
264,28249,0,"Add scopes to autograd profiler. ## 🚀 Feature

Add scopes within a torchscript model so you can get profile information per scope for both the forward and backward pass.

## Motivation

For distributed execution (model parallelism) we want to be able to measure the exact CPU time spent on different parts of a torchscript module so we can shard the model in a reasonable manner to different machines. We have high level ""components"" that represent shardable units of execution and want to use the autograd profiler to profile per component.

## Pitch

We want to add two custom methods into torch so we can describe the scope. Making them in C++ allows us to use them in torchscript as well in python.

TorchScript pseudocode


Python Sugar


These methods would add scope information to RecordFunction and thus provide it to the autograd Profiler. https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/record_function.h

The current RECORD_FUNCTION implementation appears to track the lineage from backwards functions to the forward pass via sequence_nr so it should be easy to walk the RecordFunction tree and extract scope information. https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/function.h#L104-L116 https://github.com/pytorch/pytorch/blob/master/tools/autograd/gen_variable_type.py#L238

One notable thing is that sequence_nr is currently thread_local. From what I've heard, the autograd/backward pass can be multithreaded and thus we will likely need to extend sequence_nr to include a thread ID as well as the current thread_local sequence_nr.

## Alternatives

There's ways to do this when running in the python environment w/ register_backward_hook + a custom nn.Module however there's no equivalent way to do this with torchscript since it doesn't support register_backward_hook. The autograd backward pass can also run multithreaded from my understanding so that would break that approach anyways."
738,1113,0,"Possibly a bug: Why the default_collate in dataloader will change the type of the data?. I have a float train data. When I send it into dataloader and get it with enumerate. I found the type of my data has been changed into torch.DoubleTensor. But I really need is my original type. I found this was caused in `torch/utils/data/dataloader.py' in the definition of the default_collate. Is this a bug? Or how could I make it keep my original datatype?


"
130,11504,0,"[JIT] Tracer throws runtime exception for torch.normal. ## Issue description

The normal distribution (and hence all distributions that use the  sampler) is throwing a runtime exception under JIT (this seems to be a recent regression). I have isolated the issue to the following code snippet which throws an exception on PyTorch master, and is breaking many PyTorch models. cc. @zou3519, @apaszke, @fritzo. 

## Code example

The following code that returns a sample from a normal distribution,



throws a Runtime Exception:


## System Info


"
80,21926,1,"Bug in saving indexed torch tensors makes it much slower than numpy.. ## 🐛 Bug

I am indexing tensors and storing each element of the batch separately so that they can be read individually from disk. The output of the model is a tensor of size (40,100,256,256). When I index one element from it and store the (100,256,256) dimensional tensor using torch.save() it takes the exact same time as it takes to save the whole (40,100,256,256) object.

On the other hand, if I do a .numpy() and do the same process in numpy it is much faster.

May be some thing to do with how torch handles indexed sub-tensors. Is it a separate object, or the same one? 

Dummy code below reproduces the issue with PyTorch 1.0.1.

## To Reproduce


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior
I'd expect torch and numpy to not be SO different.

## Environment
PyTorch version: 1.0.1
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 14.04.6 LTS
GCC version: (GCC) 5.2.0
CMake version: version 3.10.20171205-gd06b8

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.105
GPU models and configuration: 
GPU 0: TITAN Xp
GPU 1: TITAN Xp
GPU 2: TITAN Xp
GPU 3: TITAN Xp

Nvidia driver version: 418.56
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.15.2
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.1                      144  
[conda] mkl_fft                   1.0.10           py36ha843d7b_0  
[conda] mkl_random                1.0.2            py36hd81dba3_0  
[conda] pytorch                   1.0.1           cuda100py36he554f03_0  
[conda] torchfile                 0.1.0                     <pip>
[conda] torchnet                  0.0.4                     <pip>
[conda] torchvision               0.2.1                    py36_0

## Additional context

Saving indexed sub-tensors can be important when storing features of images etc. Due to large dataset sizes, things are usually done in batches, but it may be important to extract each element out of the batch it was processed in."
172,13409,0,"[jit] add suggestions in ""unknown builtin op"" error messages. As an example, the following:

will fail with ""unknown builtin op"", even though  (with the underscore) is bound.

We should suggest ops with similar names when we can't find a matching schema.

So the error message could look like:
aten::masked_fillaten:masked_fill_`?
@torch.jit.script
def foo(a):
    mask = torch.rand(1).byte()
    a.masked_fill(mask, 1)
    ~~~~~~~~ <--- HERE
    return a
"
631,2627,0,"Conv3D on CPU segfaults. Running the code


Results in a segfault. Running the same code on the GPU works flawlessly. 
[Output from gdb](https://gist.github.com/dchansen/6c616b73e65a68c027efa29082e777a1)


The issue was originally raised [here](https://discuss.pytorch.org/t/segmentation-fault-for-sequential-and-conv3d-on-cpu/6353), but I've encountered the same bug independently. "
735,8502,0,"[JIT] Don't support None in the script. Operator-level tests fail because we're using  in the script. Example failure:



Affected tests:

- 
- 
- 
- "
554,8987,0,"undefined reference to `onnx::GetEmptyStringAlreadyInited[abi:cxx11]()'. Hi, I met the following issue when build pytorch from source. I can build caffe2 successfully by using , but can not build pytorch by using . Does anyone know how to solve it? Thanks!

## Issue description

Failed to run 'bash tools/build_pytorch_libs.sh --use-nnpack caffe2 nanopb libshm gloo THD'

## Code example



## System Info
- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): 
- OS: Ubuntu 16.04
- PyTorch version: source code cloned from the master branch
- Python version: 3.5.2
- CUDA/cuDNN version: N/A
- GPU models and configuration: N/A
- GCC version (if compiling from source): 5.4
- CMake version: 3.11.0
- Versions of any other relevant libraries:

-Wall -Wno-unused -Wno-attributes -Wno-unused-result -Wno-psabi -ffp-contract=off -fno-math-errno -fno-trapping-mathonnx::GetEmptyStringAlreadyInited[abi:cxx11]()'
> /home/xxx/Project/svn-store/PyTorch/pytorch-git/build/lib/libcaffe2.so: undefined reference to onnx::GetEmptyStringAlreadyInited[abi:cxx11]()'
> /home/xxx/Project/svn-store/PyTorch/pytorch-git/build/lib/libcaffe2.so: undefined reference to "
292,13304,0,"ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1539602533843/work/aten/src/ATen/core/blob.h:79. ## 🐛 Bug
Hello Great programmers:
        When I was using FAIR's platform Detectron to do training with *e2e_mask_rcnn_R-101-FPN_3x_gn.yaml* config file, I faced this issue which indicated me to report one BUG to Pytorch.

<!-- A clear and concise description of what the bug is. -->
1generalized_rcnn
## Additional context

<!-- Add any other context about the problem here. -->
By the way, *e2e_mask_rcnn_R-50-FPN_1x.yaml* config works fine for me.

Waiting your response, thank you ."
256,22049,0,"Cannot update part of the parameters in DistributedDataParallel.. ## 🐛 Bug

When I use multiple GPU while the loss is calculated by only part of the parameters. I get the following errors. Use only one GPU works well.




## To Reproduce

Steps to reproduce the behavior:

Define a network in which the loss only depends on part of the parameters. We get:


find_unused_parameters=Truetorch.nn.parallel.DistributedDataParallelforwardforwardforward

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 1.2.0.dev20190620
CUDA used to build PyTorch: 9.0.176
OS: CentOS Linux release 7.5.1804 (Core)
GCC version: (crosstool-NG 1.23.0.449-a04d0) 7.3.0
CMake version: version 2.8.12.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce GTX 1080
GPU 1: GeForce GTX 1080
GPU 2: GeForce GTX 1080
GPU 3: GeForce GTX 1080

Nvidia driver version: 396.26
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] msgpack-numpy==0.4.3.2
[pip3] numpy==1.15.4
[pip3] pytorch-pretrained-bert==0.4.0
[pip3] torch==1.0.1.post2
[pip3] torchfile==0.1.0
[pip3] torchtext==0.4.0
[pip3] torchvision-nightly==0.2.1
[conda] pytorch-pretrained-bert   0.6.2                    pypi_0    pypi
[conda] torch-nightly             1.2.0.dev20190620          pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi"
9,1088,1,"GPU slower than CPU on a simple RNN test code. Hi,

I wanted to write an RNN from scratch using the pytorch cuda capabilities and I ran some preliminary tests to compare the speed of the CPU vs GPU. The task is very simple and consists of a for loop mimicking the update of the internal state x in an RNN with recurrent weight matrix J. I'm using a Quadro K620 with cuda 8.0.

When the size of x is N=1000 there seems to be a trade-off, with the GPU implementation consistently getting slower when the number of iterations increases (I ran some other tests with different sizes of the J matrix and this behaviour seems pretty systematic).

This is an example of running times I get when running the enclosed script (number of iterations are 100, 1000, 10000, 100000):

cpu: [0.010117292404174805, 0.058980703353881836, 0.45785975456237793, 4.512230634689331]
gpu: [0.0019445419311523438, 0.05474495887756348, 0.7503962516784668, 7.011191129684448]

I'd really appreciate some help on this. Thanks in advance.

The test script is the following:

"
376,26551,0,"No way to disable mse_loss broadcasting warning. ## 🐛 Bug

Hi, 

torch.nn.functional.mse_loss always throws a warning when using it with two tensors of different shapes.
There are legitimate reasons for wanting to use differently shaped tensor and taking advantage of the standard broadcasting behaviour of pytorch, so there needs to be a way to disable that warning.

## To Reproduce



## Expected behavior

There should be an optional parameter to disable the warning, like , .
As discussed here: https://github.com/pytorch/pytorch/issues/16045#issuecomment-476266780

## Additional context

There are legitimate cases for wanting to calculate the MSE between differently shaped tensors.
For example I need to calculate the difference between each sample and a subset of other samples. So I need to calculate the MSE between tensors shaped like:
(number of samples, dimension of a sample, 1) and (number of samples, dimension of a sample, number of samples in the subset).
I understand that safeguards need to be put in-place to avoid misleading people, as discussed in the original issue (https://github.com/pytorch/pytorch/issues/16045) but broadcasting is a Pytorch staple and users shouldn't have to re-implement basic functions like the MSE to make use of its full power.

Cheers
"
280,29842,0,"CI timeout after running test_async_grad_guard_with_grad (jit.test_async.TestAsync). pytorch_linux_xenial_cuda9_cudnn7_py3_test
https://app.circleci.com/jobs/github/pytorch/pytorch/3600910



@suo 




cc @ezyang @gchanan @zou3519 @jerryzh168 @suo"
456,5563,0,"Import Error : no module  named torch. PyTorch GitHub Issues Guidelines
--------------------------------

We like to limit our issues to bug reports and feature requests. If you have a question or would like help and support, please visit our forums: https://discuss.pytorch.org/

If you are submitting a feature request, please preface the title with [feature request].

When submitting a bug report, please include the following information (where relevant):
- OS:
- PyTorch version:
- How you installed PyTorch (conda, pip, source):
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):

In addition, including the following information will also be very helpful for us to diagnose the problem:
- A script to reproduce the bug. Please try to provide as minimal of a test case as possible.
- Error messages and/or stack traces of the bug
- Context around what you are trying to do

![issue1](**https://user-images.githubusercontent.com/19254992/36958144-c011b122-205f-11e8-82c2-fe0b0e0ad4f3.png**)
image error running from terminal is at link in Bracket
OS 16.04 Pytorch version 0.31 Latest , installed using conda, python version 3.5/3.6 .
Cuda 8.0 Cudnn 5.1 . GCC version 5.4.0
I just cloned this repository https://github.com/thstkdgus35/EDSR-PyTorch and trying to run demo.sh file which runs main.py file which imports PyTorch. so the problem is **i am able to import torch from home directory in python3 shell but whenever i go to any folder/directory and run pytorch code or import torch in python3 shell it gives me error** of no module named torch (see image for details). Also **i have installed torch(Luajit) in home or default root directory**. so it might be possible reason.Please help with reference to my code in image.
![issue1](https://user-images.githubusercontent.com/19254992/36958482-2c7fe048-2062-11e8-86e8-b10f25f471cc.png)
"
402,4896,0,"Second order derivative in neural network w.r.t. input encounter all zero. 









the values of y somehow are all zero
while in none-neural network case







the values of y are all normal
How to fix this? "
734,10858,0,"unexpected behaviour when mixing no_grad decorator with no_grad context manager. ## Issue description

Mixing torch.no_grad decorator with torch.no_grad context re-enables gradients as per torch.is_grad_enabled.

## Code example


#### Output:
> False
> True

### Expectation

It would be my expectation that within the no_grad context the gradients would always be disabled.
This expectation matches the output when using nested contexts as below.


#### Output:
> False
> False

## System Info

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.2.148

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 1080 Ti
Nvidia driver version: 396.54
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] cuda92                    1.0                           0    pytorch
[conda] pytorch                   0.4.1           py36_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch
[conda] torchvision               0.2.1                    py36_1    pytorch
[conda] warpctc-pytorch           0.1                       <pip>"
155,8369,0,"Something wrong with torch 0.4.0 in rnn?. If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description
I got something wrong with torch 0.4.0 in rnn. it's like there are errors about the RAM.


## Code example

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

/home/python3/bin/python3.6': double free or corruption (fasttop): 0x00007fe73000aa50 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x7c619)[0x7fe86b938619]
/usr/local/nvidia/lib64/libcuda.so.1(+0x1dedcf)[0x7fe860e4ddcf]
/usr/local/nvidia/lib64/libcuda.so.1(+0xf6ebb)[0x7fe860d65ebb]
/usr/local/nvidia/lib64/libcuda.so.1(cuStreamCreate+0x5b)[0x7fe860e9672b]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x3258786)[0x7fe801f1c786]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(+0x328dec4)[0x7fe801f51ec4]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN17RNNBackwardFilterIfffE4initEP12cudnnContextP14cudnnRNNStructi11PerfOptions+0x3b0)[0x7fe807792d10]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(cudnnRNNBackwardWeights+0xed1)[0x7fe807791f01]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native26_cudnn_rnn_backward_weightERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_+0xa7e)[0x7fe8000a0e5e]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZN2at6native19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x22f)[0x7fe8000a388f]
/home/python3/lib/python3.6/site-packages/torch/lib/libATen.so(_ZNK2at4Type19_cudnn_rnn_backwardERKNS_6TensorENS_8ArrayRefIS1_EElS3_S3_S3_S3_S3_S3_S3_lllbdbbNS4_IlEES3_S3_St5arrayIbLm4EE+0x9f)[0x7fe80030061f]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZNK5torch8autograd12VariableType19_cudnn_rnn_backwardERKN2at6TensorENS2_8ArrayRefIS3_EElS5_S5_S5_S5_S5_S5_S5_lllbdbbNS6_IlEES5_S5_St5arrayIbLm4EE+0x43b)[0x7fe824eead1b]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd9generated16CudnnRnnBackward5applyERKSt6vectorINS0_8VariableESaIS4_EE+0x6c6)[0x7fe824fb7f56]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine17evaluate_functionERNS0_12FunctionTaskE+0x3e2)[0x7fe824e46b62]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_mainEPNS0_9GraphTaskE+0xe5)[0x7fe824e47b75]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6Engine11thread_initEi+0x5e)[0x7fe824e4402e]
/home/python3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN5torch8autograd6python12PythonEngine11thread_initEi+0x2a)[0x7fe824e71a8a]
/lib64/libstdc++.so.6(+0xb52b0)[0x7fe859a602b0]
/lib64/libpthread.so.0(+0x7e25)[0x7fe86c38fe25]
/lib64/libc.so.6(clone+0x6d)[0x7fe86b9b434d]
======= Memory map: ========

ollecting environment information...
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 8.0.61

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)
CMake version: version 2.8.12.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 8.0.61
GPU models and configuration: 
GPU 0: Tesla P40
GPU 1: Tesla P40
GPU 2: Tesla P40
GPU 3: Tesla P40

Nvidia driver version: 390.30
cuDNN version: Probably one of the following:
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a

Versions of relevant libraries:
[pip3] numpy (1.14.2)
[pip3] torch (0.4.0)
[pip3] torchvision (0.2.1)
[conda] Could not collect
`

 by the way, The dataset size is about 300+G. 

Is the cuDNN error or something wrong with pytorch?
"
459,19920,0,CTC Decoder with KenLM Language Model. where is the implementation of ctc_decoder_with_kenlm in pytorch? I have found some 3rd party implementation but they are no longer supported by pytorch now?
532,9634,0,"Model only has gradients when ""x = x + bias"" and not when ""x += bias"". ## Issue description

I've built a simple model with a single bias layer (adding a set of constants to the inputs) as follows:


However, when I try training with this with an Adam optimizer, the weights of the bias layer do not change at all. Checking  shows that it is indeed in the optimizer, and checking  after running  returns a vector of all zeros.

As soon as I change the  to  in the forward() function, the model trains perfectly. The same issue also occurs with other operators such as . This model works:


It seems there is some difference with how the gradients of these are calculated which means that the gradients are all-zero when += is used and so training the model does not work. Is this intended behaviour?

## System Info
"
468,7544,0,"How to make dropout 'not scaling' during training?. Dropout always scale 1/(1-p) during training, but I want to get the original outputs. How to get it?"
478,10303,0,"cuda runtime error(59): device-side assert when running torch.topk. The code and dataset were the same as before, but I got the following error. However they could work well about a half mouth ago.
the pytorch's version is 0.3.1, BUT I find the version 0.4.0 also meeting with the same error
raceback (most recent call last):
File ""main.py"", line 341, in
main()
File ""main.py"", line 141, in main
train(train_loader, model, criterion, optimizer, epoch)
File ""main.py"", line 192, in train
prec1, prec5 = accuracy(output.data, target, topk=(1,5))
File ""main.py"", line 329, in accuracy
_, pred = output.topk(maxk, 1, True, True)
RuntimeError: invalid argument 5: k not in range for dimension at /pytorch/torch/lib/THC/generic/THCTensorTopK.cu:21
/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [0,0,0] Assertion t >= 0 && t < n_classes failed.
/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [1,0,0] Assertion t >= 0 && t < n_classes failed.
/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [2,0,0] Assertion t >= 0 && t < n_classes failed.
/pytorch/torch/lib/THCUNN/ClassNLLCriterion.cu:101: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [3,0,0] Assertion t >= 0 && t < n_classes failed.
THCudaCheck FAIL file=/pytorch/torch/lib/THC/generic/THCStorage.c line=184 error=59 : device-side assert triggered
terminate called after throwing an instance of 'std::runtime_error'
what(): cuda runtime error (59) : device-side assert triggered at /pytorch/torch/lib/THC/generic/THCStorage.c:184
Aborted (core dumped)"
104,27421,0,"[dataloader] Sampler abstract constructor API minor proposal. Currently the base abstract Sampler class [has](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py#L17):


The  is declared as a common argument but not saved to . Many samplers that do take  as argument indeed do that. And there exist samplers that do not even take  as argument. Two proposed suggestions: 1) add  to the abstract class; 2) remove  method from the base abstract class completely

cc @SsnL"
131,260,0,"define default GPU device. I think it would be useful to have a cuda.set_default_device in pytorch, so that the GPU 0 is not always the default one."
44,27926,1,"`num_batches_tracked` update in `_BatchNorm` forward should be a single scalar update on host regardless of the residence of the layer. ## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->

 is single scalar that increments by 1 every time  is called on the  layer with both  &  set to true. Our current implementation stores it as a single-element buffer that resides on the same device as with the rest of its parameters/buffers.

We request the update & storage of  to be moved to host, despite the residence of the rest of parameters/buffers.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

When we have a BN layer on accelerators (GPUs), every forward call that updates  triggers a single-element kernel launch, introducing unnecessary host overhead which could hurt our end-2-end perf in cpu bounded workload.

My last attempt to move  to the host from the device gives 0%~11% performance gain across some common problem sizes. #26550

## Pitch

<!-- A clear and concise description of what you want to happen. -->
We need a way that  would resides on device and be backward compatible for save/load modules. This involves relaxing some checks in python tests, which assumes that all values in  are buffers/parameters passed by reference.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

My implementation #26550 does not give full backward compatibility (failing tests and not loading  by reference) and I don't know how to do that easily without a big hammer (rewriting  in  inherited from ).
But it does support save/load module, as well as allow assignment using tensor/scalar."
194,8181,0,"Stop using ""from pip import main"". See https://github.com/pytorch/pytorch/pull/7411 for backstory"
378,23301,0,"Error while using Libtorch + OpenCV + Qt Creator. I have the following configuration in the .pro file

 

OpenCV works absolutely fine **without** ""QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=**0**"". With this, however, I get this following errors:

![Screenshot_2019-07-24_14-35-50](https://user-images.githubusercontent.com/17313248/61799072-b6307800-ae2a-11e9-9bb3-e8b8068c8a87.png)

OpenCV works fine **with** ""QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=**1**"" as well. But it throws a different set of errors:

![Screenshot_2019-07-24_14-38-10](https://user-images.githubusercontent.com/17313248/61799101-c183a380-ae2a-11e9-9da8-ff815f721dde.png)

Setting ""QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=**0**"" has been recommended for Libtorch in most of the forums to avoid the errors above.

What could be a solution or some solutions to work around this?
(I am a newbie to both Libtorch and Qt Creator.)"
323,15771,0,"Implicit conversion error in caffe2. Hello,

While compiling generated sources(caffe2.pb.cc)  from caffe2.proto we are getting the implicit conversion error
protobuf version=v3.5.2
compiler:
 CC=aarch64-linux-android-clang \
 CXX=aarch64-linux-android-clang++




Thank you,


"
725,30818,0,kthvalue/median with scalar and dim=1 inconsistent between CPU and CUDA
455,22542,0,"Bug in CosineAnnealingWarmRestarts (T_mul instead of T_mult). ## 🐛 Bug

The bug appears in PyTorch 1.1 in file lr_scheduler.py (line 686) for LR scheduler ""CosineAnnealingWarmRestarts"", where the parameter name is *T_mult* but in line 686 the name *T_mul* is used instead (python interpreter does not know variable *T_mul*).

## To Reproduce

The bug can be easily reproduced if the *T_mult* parameter of CosineAnnealingWarmRestart scheduler is not set properly (i.e. is < 1 or not an instance of int), causing Python to raise the error at line 686 (where *T_mul* is used instead of *T_mult*).

The following line will raise the error and cause the bug:


because 2.0 is not an instance of int.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Line 686 of lr_scheduler.py should be modified from

into 


## Environment

- PyTorch Version (e.g., 1.0): 1.1
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.7
 - CUDA/cuDNN version: 10
 - GPU models and configuration: 1080 Ti
 - Any other relevant information: N/A"
379,18776,0,"Value of torch.backends.cudnn.benchmark Baked into JIT-Traced Modules ( 150x slowdown on ConvTranspose2d() ) [jit] [libtorch] [cudnn] . ## 🐛 Bug

If you trace a module with  and load that script module in C++ via LibTorch, the resulting behavior in C++ depends on whether or not the  flag was set. Calls to  from the C++ API at runtime appear to have no effect.

## To Reproduce

**NOTE**: I was not able to verify this issue still exists on the latest nightly (20190402) because it appears the latest nightly (at least on Windows) cannot run JIT-traced models. Even the simplest model gives the following error:



1) Run the python script below:  or 
2) Compile + run the C++ code below.
3) Observe:
  a) Average time per call. I see ~0.8ms in the python script and either ~0.8 or ~120ms in C++ depending on the flag used in python. In either case, C++ sets benchmarking ON. (GTX 1080)
  b) Kernel run by CuDNN. w/either setting of the flag, the python code runs . With the flag ON, it runs  once (taking ~120ms) and then chooses the faster . If the flag was ON in python, C++ also chooses  but if the flag was OFF in python, it always chooses  regardless of the flag setting in C++.

I observed the choice of kernel using .

Python Script ():


C++ Code:


## Expected Behavior

I would expect that either: 1) the C++ setting of  should be respected (choosing the correct algorithm) or 2) at least print a warning that it is being overridden by the value of the flag at trace time.

## Additional Info
I printed the JIT graphs generated with benchmarking ON/OFF and got the following with the flag OFF:


The only change when the flag is ON is that register %17 is 1 instead of 0. I suppose this is where the ""hardcoding"" of the flag might be happening?

## Environment
Python code was run on Linux, C++ code was run on Windows

 - PyTorch Version (e.g., 1.0): 1.0.0.dev20190311 on linux, 2336f0ba0 on Windows
 - OS (e.g., Linux): Fedora 29, Windows 10 1809
 - How you installed PyTorch (, , source): conda (pytorch-nightly)
 - Python version: 3.7
 - CUDA/cuDNN version: CUDA 10, cuDNN 7.4.2
 - GPU models and configuration: Titan RTX (linux), GTX 1080 (windows)

cc @suo"
200,9261,0,"memory leak for PyTorch -> dlpack -> CuPy. Hi again,

for this one, I'm not sure whether it is a cupy or pytorch issue, either, but I don't think this should run out of memory:

 will be high. Again it could be a refcounting thing that things are not going out of scope properly. (In which case it would be more of a CuPy issue.)

Best regards

Thomas"
410,20296,0,"the capacity of memory when compiling pytorch from source. how large the capacity of memory is needed when compiling pytorch from source ?

I've got about 5GB available when compiling in ubuntu 18.04.  Each time I compile with **python setup.py install**, the compiling program will occupy all the capacity of memory. Then my screen is stuck. After a while,  the screen becomes dark and my laptop reboots.... 

Is there any way to setup some parameters to use less memory when compiling ? Many thanks."
676,22616,0,"Error when loading a sparse tensor parameter from a state_dict in pytorch 1.1.0. ## 🐛 Bug

In pytorch , if a  has a parameter which is a sparse tensor,  fails. For the mini snipped below it throws the following error:


On pytorch  there is no error.

## To Reproduce

Steps to reproduce the behavior:


## Expected behavior

The  loads the sparse tensor from the state dict.

## Environment



The virtual environment with torch  for which the above snipped throws **no** error:

"
704,6034,0,"how could I get pytorch0.4 for maskrcnn. i want to deal 0dim. pytorch 0.3 can't deal 0 dim 
i can't find 0.4
can u help me?



PyTorch GitHub Issues Guidelines
--------------------------------

We like to limit our issues to bug reports and feature requests. If you have a question or would like help and support, please visit our forums: https://discuss.pytorch.org/

If you are submitting a feature request, please preface the title with [feature request].

When submitting a bug report, please include the following information (where relevant):
- OS:
- PyTorch version:
- How you installed PyTorch (conda, pip, source):
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):

In addition, including the following information will also be very helpful for us to diagnose the problem:
- A script to reproduce the bug. Please try to provide as minimal of a test case as possible.
- Error messages and/or stack traces of the bug
- Context around what you are trying to do

"
489,15187,0,"complex extension does not work. ## 🐛 Bug

I was trying to build the [pytorch-complex](https://github.com/Roger-luo/pytorch-complex) which I haven't work on for 3 month with the following on Mac OS X Mojave.

Then I got the following error. 


And a bunch of others:



Then I tried to build the one in test (), with the same build script in pytorch-complex. I still got this.

## To Reproduce

Steps to reproduce the behavior:

1. git clone pytorch 1.0.0 or later
2. git clone , and run 

## Expected behavior

You will get that error.

## Environment



## Additional context

I did some search, I guess this might relate to https://github.com/martinmoene/gsl-lite/issues/63

However, I found it includes  in  which is strange...

cc: @ezyang "
229,6437,0,"[caffe2] Double precision for operators?. A lot of the operators are not defined for double precision. I am planning on experimenting with double precision calculations and will add them as necessary. Would there be interest in me submitting a PR request with them? Is there a reason they are not currently enabled? The only drawback is larger library file size and slower compilation time but I suppose not by much.

Thank you,
Svet"
315,18496,0,"Can't compile c10::optional values() or operator-> in .cu file. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

Our implementation of optional doesn't seem to always be compilable in .cu files.  Compiling this commit: https://github.com/gchanan/pytorch/commit/6666ff1083a55e90b230d2269c13a3d30af8c0f4

gives me the following error:


I did a bit of digging and the problem seems to be with the constexpr functions, e.g.:
https://github.com/pytorch/pytorch/blob/654e59fcac4a9d4bf0b48306e1d7f7be5b7e40b1/c10/util/Optional.h#L600-L604

basically, for nvcc to compile this in a .cu file,  has to be a .  This doesn't seem possible with the constraint that bad_optional_access derives from  because  does not have  constructors.

Note that this only applies to calls to .  Calls to  seem to fail for other reasons that I didn't dig into.  But  passes fine, so the solution is to use that until we fix the issue."
251,31698,0,"can't iter a dataSet imported by hdf5. ## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)

"
77,14231,1,"`index_select` on flat tensor faster than integer array indexing, even including reshaping. ## 🐛 Bug

Integer array indexing appears to be slower than applying  to the flattened tensor, using flattened indexes, and then reshaping to the expected output size.

## To Reproduce

Reproduced in this gist: https://gist.github.com/gngdb/3d4f5aa27ee5199b0d4b997ffe21a6b4

## Expected behavior

If this is a faster way to implement integer array indexing, it should be the default. It might not be though, there are other uses for integer array indexing where this speed difference might not hold.

## Environment

PyTorch version: 0.4.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Scientific Linux release 7.5 (Nitrogen)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
CMake version: version 2.8.12.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 8.0.44
GPU models and configuration: 
GPU 0: TITAN X (Pascal)
GPU 1: TITAN X (Pascal)
GPU 2: TITAN X (Pascal)
GPU 3: TITAN X (Pascal)

Nvidia driver version: 390.87
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy (1.15.2)
[pip] torch (0.4.1.post2)
[pip] torchvision (0.2.1)
[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch
[conda] torchvision               0.2.1                    py36_1    pytorch"
246,21004,0,"PTX JIT compilation failed when running FasterRCNN. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
Trying to run FasterRCNN newly released in torchvision 0.3 runs into PTX JIT compilation failed error. This fails on my Google Cloud instance using GPU, but does not fail using CPU

## To Reproduce

Steps to reproduce the behavior:

Simple code that fails


Error output:


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

To run

## Environment


## Additional context

<!-- Add any other context about the problem here. -->
"
450,2117,0,"Add dynamic infer shape for nn.Linear. It's quite inconvenient for user to calculate the shape of tensor manually. I think it would be better to entitle nn.Linear with the ability to infer shape from the input tensor.
One possible solution is like numpy's reshape function, which is nn.Linear(-1, hidden_size) or nn.Linear(None, hidden_size) means automatic inferring.

My preliminary implementation for this is at here:
https://gist.github.com/VoVAllen/1420e410e3dfd368b8dc9061ad0c206a

This implementation doesn't create any overhead during the training process. Also I think this kind of style can be used in other layers such as nn.Conv.
Feel free to add any comments. "
533,1984,0,"Different results for batch size 1 Variables. pytorch version: '0.1.12_2'

Run this snippet (The required serialised tensor has been included in the attachment.



The output

[issue.zip](https://github.com/pytorch/pytorch/files/1124350/issue.zip)
"
745,8519,0,"[JIT] Passed-in parameter is not a 1D LongTensor when expected. Operator-level tests fail. Example failure:


Affected tests:
- 
- 
- 
- 
- "
609,24587,0,"Migrate `log10` and `log10_` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
290,16316,0,"Please remove pytorch from pypi. ## 🐛 Bug

I tried to install pytorch using pipenv (see https://github.com/pypa/pipenv/issues/3476).

You have put in a fancy thing which is supposed to open the browser. It did not. Instead there was no output and I believed that the installation was broken.

## To Reproduce

Steps to reproduce the behavior:

1. pipenv install pytorch

You will see something like this:



## Expected behavior

If you don't want people installing it from pypi then don't put it there.
Don't break dependency management tools.

## Environment

Collecting environment information...
PyTorch version: N/A  
Is debug build: N/A                    
CUDA used to build PyTorch: N/A                
                                        
OS: Ubuntu 18.04 LTS            
GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0
CMake version: version 3.5.1   
                       
Python version: 3.7      
Is CUDA available: N/A                  
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect

## Additional context

I can understand your desire to be helpful. Trying to make a non interactive activity (dependency management) into an interactive activity is inevitably going to result in situations like this.

Please just make the installation fail promptly and noisily."
624,25176,0,"Tensor slicing with boolean numpy mask wrong. ## 🐛 Bug
Numpy arrays of dtype  should be interpreted as masks when slicing torch arrays, just like tensors of dtype  are. Insead, such arrays are interpreted as indices.

## To Reproduce
Steps to reproduce the behavior:


Output

## Expected behavior
The expected behaviour is that the result is the same as for slicing with torch tensors with the same dtype, where for the input

we get the expected output


## Environment
 - PyTorch Version: 1.2.0a0+0885dd2
 - OS: Ubuntu 16.04.6 LTS
 - How you installed PyTorch: Nvidia docker image ()
 - Python version: 3.6
 - CUDA/cuDNN version: Nvidia driver version: 384.183, 
 - GPU models and configuration: Tesla V100
"
123,2523,0,"cudnn bindings not respecting current stream. currently, some cudnnSetStream calls are missing in the bindings, fix them.
Context: https://discuss.pytorch.org/t/torch-nn-in-context-of-a-cuda-stream/6304/3"
527,8594,0,"PyTorch 0.4 hangs with nn.DataParallel . I have an issue that matches [https://github.com/pytorch/pytorch/issues/7019](url) but the suggested solution does not work for me.

This code never returns a value to y:

    import torch
    import torch.nn as nn
    from torch.autograd import Variable

    class NET(nn.Module):
        def __init__(self):
            super(NET, self).__init__()
            self.dense = nn.Linear(256, 512)

        def forward(self, input):
            return self.dense(input)

    if __name__ == '__main__':
        model = NET()
        model = nn.DataParallel(model).cuda()
        x = Variable(torch.rand(128, 256))
        y = model(x) ##### <<<<--- GETS STUCK HERE FOREVER

the solution was to delete  NCCL_SHM_DISABLE=1 and NCCL_P2P_DISABLE=1 from /etc/nccl.conf

I do not have a /etc/nccl.conf in Ubuntu 18.04 but I did unset those environment variables and I get this:

NCCL version 2.1.15+cuda9.0
rig:2637:2637 [0] INFO NET : Using interface enp0s31f6:192.168.85.32<0>
rig:2637:2637 [0] INFO NET/Socket : 1 interfaces found
rig:2637:2637 [3] INFO Using 256 threads
rig:2637:2637 [3] INFO Min Comp Cap 6
rig:2637:2637 [3] INFO NCCL_SINGLE_RING_THRESHOLD=131072
rig:2637:2637 [3] INFO Ring 00 : 0 1 2 3
rig:2637:2637 [0] INFO Ring 00 : 0[0] -> 1[1] via P2P/direct pointer
rig:2637:2637 [1] INFO Ring 00 : 1[1] -> 2[2] via P2P/direct pointer
rig:2637:2637 [2] INFO Ring 00 : 2[2] -> 3[3] via P2P/direct pointer
rig:2637:2637 [3] INFO Ring 00 : 3[3] -> 0[0] via P2P/direct pointer
rig:2637:2637 [0] INFO Launch mode Group/CGMD
^C gives:
File ""/home/minimumnz/anaconda3/envs/tacotron/lib/python3.6/threading.py"", line 1072, in _wait_for_tstate_lock
elif lock.acquire(block, timeout):

it seems stuck in some thread.

    Collecting environment information...
    PyTorch version: 0.4.0
    Is debug build: No 
    CUDA used to build PyTorch: 9.0.176

    OS: Ubuntu 18.04 LTS 
    GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0 
    CMake version: Could not collect

    Python version: 3.6
    Is CUDA available: Yes 
    CUDA runtime version: Could not collect
    GPU models and configuration:
    GPU 0: GeForce GTX 1080 Ti
    GPU 1: GeForce GTX 1080 Ti
    GPU 2: GeForce GTX 1080 Ti
    GPU 3: GeForce GTX 1080 Ti

    Nvidia driver version: 390.67
    cuDNN version: Probably one of the following:
    /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3
    /usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

    Versions of relevant libraries:
    [pip] numpy (1.14.5)
    [pip] torch (0.4.0)
    [pip] torchvision (0.2.1)
    [conda] cuda90 1.0 h6433d27_0 pytorch
    [conda] pytorch 0.4.0 py36_cuda9.0.176_cudnn7.1.2_1 [cuda90] pytorch
    [conda] torch 0.4.0 
    [conda] torchvision 0.2.1 py36_1 pytorch"
221,31528,0,"cuCtxGetDevice error and seg fault with DDP and OpenMPI. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
When using DistributedDataParallel with OpenMPI+UCX for certain tested models I'm getting this error in the DDP constructor (I believe it's during the model broadcast):



I observed this error using a resnet50 model but no error if I instead used a very small single-layer CNN.

## To Reproduce

The script is here:
https://github.com/sparticlesteve/nersc-pytorch-build/blob/911fc67b6667d3c6e3be972169e30e34c1a33af5/test_ddp.py

I submit via slurm a single-node job with 8 MPI ranks for 8 V100 gpus, something like:



My full log with stack trace is here:
https://gist.github.com/sparticlesteve/7307694f89329c277e16e452b524fefa

## Environment

PyTorch version: 1.3.1
Is debug build: No
OpenMPI: 4.0.1 with UCX 1.6
CUDA used to build PyTorch: 10.1.168
OS: openSUSE Leap 15.0
GCC version: (GCC) 7.3.0 20180125 (Cray Inc.)
CMake version: version 3.14.0
Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration:
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB
GPU 2: Tesla V100-SXM2-16GB
GPU 3: Tesla V100-SXM2-16GB
GPU 4: Tesla V100-SXM2-16GB
GPU 5: Tesla V100-SXM2-16GB
GPU 6: Tesla V100-SXM2-16GB
GPU 7: Tesla V100-SXM2-16GB

Nvidia driver version: 440.33.01
cuDNN version: Could not collect

cc @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"
636,28346,0,"Error for quantize_dynamic(resnet18) + torch.jit.trace + ios-demo-app/PyTorchDemo. ## 🐛 Bug

I'm getting an error while loading quantized resnet18 on iphone with [ios-demo-app/PyTorchDemo](https://github.com/pytorch/ios-demo-app/tree/master/PyTorchDemo).



## To Reproduce
Convert the model with:


Run the [ios-demo-app/PyTorchDemo](https://github.com/pytorch/ios-demo-app/tree/master/PyTorchDemo) with . Get the stacktrace above.
Here is the forked demo with coverted models: [mirth/ios-demo-app](https://github.com/mirth/ios-demo-app)

## Expected behavior
For default image demo with  i'm getting ~50ms per frame on iPhone 6s.
For  on same iPhone i'm getting ~250ms per frame.
For  i'm getting an error with stacktrace above.
I'm expecting no error.

## Environment

PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.15
GCC version: Could not collect
CMake version: version 3.11.1

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] Could not collect
[conda] mkl                       2018.0.0             hc285769_4
[conda] mkl-service               1.1.2            py36h7ea6df4_4
[conda] torch                     1.3.0                    pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchnet                  0.0.4                    pypi_0    pypi
[conda] torchsummary              1.5.1                    pypi_0    pypi
[conda] torchvision               0.4.0                    pypi_0    pypi


cc @suo @jerryzh168 @jianyuh @dzhulgakov @raghuramank100"
656,8330,0,"x.copy_(y) doesn't work with sparse x. 

There isn't really any reason this shouldn't work; internally we have all the pieces implemented already."
321,2001,0,"Implement similar PyTorch function as model.summary() in keras?.  in keras gives a very fine visualization of your model and it's very convenient when it comes to debugging the network. Can we try to implement something like it in PyTorch?

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @albanD @mruberry"
727,27569,0,"Export from TorchScript to ONNX: torch.onnx.symbolic_opset9.dim does not exist. ## 🐛 Bug

TorchScript -> ONNX conversion of a simple module fails

If one doesn’t jit-compile the model, everything works.

## To Reproduce



Output



## Expected behavior

I expected that a jit-compiled module consisting of just two  children should export to ONNX without hassle.

## Environment

Google Colab, I think at the moment it has PyTorch 1.2.0 and Python 3.6

## Additional context
I played around with TorchScript tracing / ONNX export of modules that work with namedtuples, got some errors. Trying to get a minimal example has led me to this code with no namedtuples.

cc @suo"
692,3243,0,"Compile error:  ‘__T0’ was not declared in this scope. When I build PyTorch from source following the [README directions][1], I get an error:



Machine details:

* nvcc release 7.5, V7.5.17
* gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC)
* gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC)
* Python 3.6.1 with Anaconda
* OS: Red Hat

I'm not sure if I have cuDNN. I am using Amazon's EC2 Deep Learning AMI, and they say they have cuDNN drivers: https://aws.amazon.com/amazon-ai/amis/

[1]:https://github.com/pytorch/pytorch#from-source"
573,4408,0,"torch.onnx.export fails on Linear when bias=False, torch.onnx.symbolic.matmul does not exist. Using  on a model that contains a linear layer with no bias, I receive an error: 



I've constructed a minimal working example that reproduces this


Environment Info:
"
219,23070,0,"adam.py KeyError: 'betas'. ## 🐛 Bug

File ""STS_main.py"", line 196, in <module>
    model, optimizer, loss = train(model, train_iter, optimizer, criterion)
  File ""STS_main.py"", line 72, in train
    optimizer.step()
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py"", line 85, in step
    beta1, beta2 = group['betas']
KeyError: 'betas'
"
340,17126,0,"Support callables in scripted functions. ## 🚀 Feature
Allow for callables (notably other scripted functions or ScriptModules) to be passed to scripted functions (Currently fails with ).

## Motivation

Attempting to tease out fusions from larger codebases (in my case maskrcnn_benchmark) where some blocks of code may not be scriptable, but subsets of those blocks may be. In a concrete example, I have a module which contains loop over sets of inputs & modules, where  is seen.  is itself a ScriptModule, and I'd like to be able to add the  to a fused group that's already being generated within that ScriptModule. However, there are several things that prevent the outermost module from being scripted. Pulling out the relevant code to a separate function like:


Should allow the desired fusion, but is currently not supported.

## Pitch

Allow callables (at least ScriptModules) to be passed as arguments to scripted functions and called.

## Alternatives

My specific use-case disappears as the scripting supports a certain level of python, but I would expect there to be other use-cases.

## Additional context

This is a proxy code I wrote to make sure that the error I was seeing wasn't from the larger application. This fails with . 




cc @suo"
272,21459,0,"RuntimeError: cublas runtime error . RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:411
My GPU is 2080ti,CUDA 10.0
How can I slove the problem?Thank you."
427,7396,0,[JIT][script] Implement `train` on ScriptModules
91,1509,1,"GPU memory consumption increases while training. Hello, all
I am new to Pytorch and I meet a strange GPU memory behavior while training a CNN model for semantic segmentation. Batchsize = 1, and there are totally 100 image-label pairs in trainset, thus 100 iterations per epoch. However the **GPU memory consumption increases a lot at the first several iterations while training**.

[Platform] GTX TITAN X (12G), CUDA-7.5, cuDNN-5.0

> torch.backends.cudnn.enabled = False
> torch.backends.cudnn.benchmark = False

Then GPU memory consumption is **2934M -- 4413M -- 4433M -- 4537M -- 4537M -- 4537M** at the first six iterations.

> torch.backends.cudnn.enabled = True
> torch.backends.cudnn.benchmark = True

Then GPU memory consumption is **1686M -- 1791M -- 1791M -- 1791M -- 1791M -- 1791M** at the first six iterations.

**Why GPU memory consumption increases while training, especially, increases so largely while no cuDNN? (In my opinion, GPU memory consumption won't increase while the CNN has been build and starts training)** 

Does anyone meet the same problem? Or could anyone give some help?

This is the code snippet:

"
502,3326,0,"nn.EmbeddingBag doesn't seem to support None offset for 2D tensors. Example

Gives
RuntimeError: expected a Variable argument, but got torch.LongTensor"
671,26922,0,"DistAutogradContainer should not expose references for DistAutogradContext. More Context here: https://github.com/pytorch/pytorch/pull/25527/files#r328763171

DistAutogradContainer's API currently exposes mutable references to DistAutogradContext. This could lead to 'use after free' scenarios where DistAutogradContainer cleans up the context, but there is another thread that continues to use the reference. We should modify the API to ensure it returns shared_ptrs to the user to avoid any sort of ownership issues.

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"
243,25481,0,"[feature request] symmetric matrix square root. This is needed when incorporating curvature information into optimization

There's PyTorch implementation of symmetric matrix square root op [here](https://github.com/msubhransu/matrix-sqrt) but they use PyTorch for backward pass only, and use scipy for forward pass

I've hacked something together by mirroring 




cc @vincentqb @vishwakftw @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @SsnL"
552,15202,0,"The font size of equations in tutorial is too small. The tutorial at: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients
![image](https://user-images.githubusercontent.com/1032377/49982017-a95cb500-ff28-11e8-9465-7b0da9ce39c4.png)
"
111,30139,0,"RuntimeError: isTensor() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue_inl.h:90, please report a bug to PyTorch. Expected Tensor but got Int (toTensor at /pytorch/aten/src/ATen/core/ivalue_inl.h:90). So, what I'm trying to do is to to convert torchscript model into onnx. Both scripting and tracing works during the creation of graph, but fails when the model is converted into onnx

Here's the error:

> 
Traceback (most recent call last):
  File ""yolo2script.py"", line 8, in <module>
    torch.onnx.export(model_scripted, dummy_input, ""yolov3.onnx"", example_outputs=model(dummy_input))
  File ""/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py"", line 143, in export
    strip_doc_string, dynamic_axes, keep_initializers_as_inputs)
  File ""/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py"", line 66, in export
    dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)
  File ""/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py"", line 382, in _export
    fixed_batch_size=fixed_batch_size)
  File ""/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py"", line 235, in _model_to_graph
    method_graph, params = model.forward._lowered_graph()
RuntimeError: isTensor() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue_inl.h:90, please report a bug to PyTorch. Expected Tensor but got Int (toTensor at /pytorch/aten/src/ATen/core/ivalue_inl.h:90)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7f83210f0813 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1876251 (0x7f82bdc7d251 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)
frame #2: torch::jit::script::Method::_lowered_graph() + 0x161 (0x7f82c0420771 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x5af698 (0x7f832229b698 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)
frame #4: <unknown function> + 0x2110f4 (0x7f8321efd0f4 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #6: python3() [0x4f88ba]
frame #8: python3() [0x4f6128]
frame #9: python3() [0x4f7d60]
frame #10: python3() [0x4f876d]
frame #12: python3() [0x4f6128]
frame #13: python3() [0x4f7d60]
frame #14: python3() [0x4f876d]
frame #16: python3() [0x4f6128]
frame #17: python3() [0x4f7d60]
frame #18: python3() [0x4f876d]
frame #20: python3() [0x4f6128]
frame #21: python3() [0x4f7d60]
frame #22: python3() [0x4f876d]
frame #24: python3() [0x4f6128]
frame #26: python3() [0x6415b2]
frame #31: __libc_start_main + 0xe7 (0x7f83267afb97 in /lib/x86_64-linux-gnu/libc.so.6)

Here's my system:

Collecting environment information...
PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.1.243

OS: Ubuntu 18.04.2 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.16.0-rc3

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX 1060 with Max-Q Design
Nvidia driver version: 418.87.00
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.2

Versions of relevant libraries:
[pip3] efficientnet-pytorch==0.4.0
[pip3] numpy==1.17.4
[pip3] torch==1.3.1
[pip3] torch2trt==0.0.2
[pip3] torchetl==0.3.9
[pip3] torchvision==0.4.2
[conda] Could not collect




If you need the model source code, I will provide it to you asap. Thank you !

cc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"
107,14320,0,"[caffe2] Fails to build with fbgemm enabled. ## 🐛 Bug

Caffe2 git master fails to build with fbgemm enabled (), giving the following error:

This error seems to be caused by the recent commit https://github.com/pytorch/pytorch/commit/fb8c3d62feacf5c3d39f6fb034db944a89a0bcf4. It was building fine some commits before it.

## To Reproduce

Steps to reproduce the behavior:

1. mkdir build
1. cd build
1. [cmake options](https://bpaste.net/show/ca5a7eb99368)
1. make -j1

## Expected behavior

A succesful build with fbgemm enabled.

## Environment

 - **PyTorch Version:** git master (currently at https://github.com/pytorch/pytorch/commit/f79fb58744ba70970de652e46ea039b03e9ce9ff)
 - **OS:** Arch Linux x86_64
 - **How you installed PyTorch:** source
 - **Build command you used (if compiling from source):** already shown above
 - **Python version:** 3.7.1
 - **CUDA/cuDNN version:** not enabled in this example
 - **GPU models and configuration:** not enabled in this example
 - **Compiler:** gcc 8.2.1
 - **Any other relevant information:** the same error occurs when enabling cuda and using gcc 7.3.1

## Additional context

It builds fine when disabling fbgemm ()."
325,27694,0,"Deployment training model at C + + end. When the model does not contain a custom layer, it can be deployed directly on the C + + side using JIT mechanism and libtorch library.

But my model contains a custom c++ and CUDA layer. Now that I'm deploying the model on the c++ side, do I need to compile the custom c++ and CUDA layers into the libtorch library?
Thank you!!!

cc @suo @yf225"
732,7496,0,"Train a model using multiple GPUs . ## Issue Description
I tried to train my model on multiple gpus. However, when I launch the program, it seems that the model is allocated to gpus, but no data is fed into model. Using nvidia-smi, i find hundreds of MB of memory is consumed on each gpu. I guess these memory usage is for model initialization in each gpu.

I am sharing 8 gpus with others on the server, so I limit my program on GPU 2 and GPU 3 by following command.


## Code example
I ran this official tutorial on my machine and the same thing happens again. https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html

## System Info
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 8.0.61

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 7.5.17
GPU models and configuration:
GPU 0: GeForce GTX TITAN X
GPU 1: GeForce GTX TITAN X
GPU 2: GeForce GTX TITAN X
GPU 3: GeForce GTX TITAN X
GPU 4: GeForce GTX TITAN X
GPU 5: GeForce GTX TITAN X
GPU 6: GeForce GTX TITAN X
GPU 7: GeForce GTX TITAN X

Nvidia driver version: 375.88
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10
/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.20
/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a
/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a
/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn.so.6

Versions of relevant libraries:
[pip3] msgpack-numpy (0.4.1)
[pip3] numpy (1.14.0)
[pip3] numpydoc (0.7.0)
[pip3] torch (0.4.0)
[pip3] torchtext (0.2.3)
[pip3] torchvision (0.2.1)
[conda] torch                     0.4.0                     <pip>
[conda] torchtext                 0.2.3                     <pip>
[conda] torchvision               0.2.1                     <pip>"
220,4894,0,"Inconsistent size error when using pin_memory() after torch.unbind(). - OS: Ubuntu 14.04
- PyTorch version: 0.3.0.post4
- How you installed PyTorch: conda
- Python version: 3.6.3
- CUDA version: 8

Script to reproduce the error:

"
237,145,0,"embeddings layer with IntTensor / cuda.IntTensor inputs. Would it be possible to feed the embedding layer with IntTensors rather than LongTensors only?
"
163,23393,0,"RuntimeError: set_storage is not allowed on Tensor created from .data or .detach(). ## 🐛 Bug Here is the issue while trying to execute the this bunch of code. 

<!-- A clear and concise description of what the bug is. -->

## My code is below..



   
   
    
 = 




 - PyTorch Version (1.0):
 - Linux
 - pip3 install torchvision:
 - Build command you used (if compil:
 - Python version 3.7


"
190,3395,0,"dlopen libnvrtc instead of dynamically linking against it. Starting from commit https://github.com/pytorch/pytorch/commit/50e51eaa7fa5c252c2f4508cb5984734050177f1 we are linking against nvrtc and libcuda on CUDA builds. 
This has two consequences:
- pytorch CUDA builds can only be done on GPU-enabled machines, because libcuda.so comes with the nvidia driver
- the pytorch binaries with libcuda.so link dependency cant be run on CPU-only machines.

We need to fix this, and instead dlopen nvrtc, so that we can ship the next set of binaries to run on CPU / CUDA machines (just like our previous releases)."
32,5812,1,"memory leaky on DataLoader. - OS: Ubuntu16.04
- PyTorch version: 0.3.1
- PyTorchNet version: 0.0.1
- How you installed PyTorch (conda, pip, source): conda
- Python version: 3.6.4
- CUDA/cuDNN version: CUDA 9.0.176/cuDNN 7.0.5.15

**The script to reproduce the bug:** 

**Error messages**

- The  of  is set to 64, then on epoch 1, before :
![screenshot from 2018-03-15 20-23-31](https://user-images.githubusercontent.com/9991443/37463453-30928920-2890-11e8-93d0-d0dbad043997.png)

- After , you could see the memory is grow up to about 3 times:
![screenshot from 2018-03-15 20-24-00](https://user-images.githubusercontent.com/9991443/37463507-5f8d2992-2890-11e8-9f7d-b6a754314c6a.png)

- Furthermore, if I change the  of  to 1000, then on epoch 1, before , you could see the memory use is just only **715MB**, it's strange:
<img width=""867"" alt=""qq20180315-225225 2x"" src=""https://user-images.githubusercontent.com/9991443/37471232-3815a042-28a4-11e8-89e5-b971a9e3af93.png"">

- After , you could see the memory haven't change:
<img width=""748"" alt=""qq20180315-225238 2x"" src=""https://user-images.githubusercontent.com/9991443/37471400-8e578920-28a4-11e8-8c51-34bb35c80e6c.png"">
"
634,15839,0,"MyPy Type Hinting Stub Files?. ## 📚 Documentation
### (should this be a feature request?)
<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->

Having  type annotated stub files ([PEP561](https://www.python.org/dev/peps/pep-0561/)) and type hinted source ([PEP484](https://www.python.org/dev/peps/pep-0484/)) would be beneficial for maintainability, new users, and making contributions easier. "
212,25172,0,"test_det_logdet_slogdet_batched (in test_cuda.py) fails on ppc64le. ## 🐛 Bug



https://github.com/pytorch/pytorch/blob/v1.2.0/test/test_cuda.py#L2224 fails for me on ppc64le.

On following the test through to https://github.com/pytorch/pytorch/blob/v1.2.0/test/test_torch.py#L6558 and then testing all the different possible combinations, the failures are seen for all combinations of  and  in both  and  (the other tests in there are fine).

I went through and commented out the assert statements and printed out the tensors that were being compared for each of the combinations:



## To Reproduce

Steps to reproduce the behavior:

1. Build from source on ppc64le
1. Run the tests

## Expected behavior

The test should pass

## Environment

PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.1.105

OS: Red Hat Enterprise Linux
GCC version: (GCC) 8.2.0
CMake version: version 3.13.3

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.1.105
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB

Nvidia driver version: 418.67
cuDNN version: 7.4.2.24
magma version: 7.5.1

Versions of relevant libraries:
[pip3] numpy==1.16.2
[pip3] torch==1.2.0
[conda] Could not collect

PyTorch built from source
Build command: PYTORCH_BUILD_VERSION=1.2.0 PYTORCH_BUILD_NUMBER=1 VERBOSE=1 LDFLAGS=""$LDFLAGS -ldl"" CFLAGS="""" CXXFLAGS="""" USE_FFMPEG=ON USE_GLOO_IBVERBS=1 USE_GFLAGS=ON USE_GLOG=ON CUDNN_LIB_DIR=$EBROOTCUDNN/lib64 CUDNN_INCLUDE_DIR=$EBROOTCUDNN/include TORCH_CUDA_ARCH_LIST=""3.5 6.0 7.0""  /rds/bear-apps/devel/2019a/branfosj-eb-pytorch/EL7/EL7-power9/software/Python/3.7.2-GCCcore-8.2.0/bin/python setup.py build

"
244,28444,0,"Problem installing from source on CentOS 6.5. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

I ran the following code:

1. conda create detectron2 python=3.7
2. conda activate detectron2
3. conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi typing
4. git clone --recursive https://github.com/pytorch/pytorch
5. cd pytorch
6. export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
7. python setup.py install

This gave the following stdout (sorry for the wall of text!):
-Wall -Wno-unused -Wno-attributes -Wno-unused-result -Wno-psabi -ffp-contract=off -fno-math-errno -fno-trapping-math/nethome/ebj26/apps/pytorch/third_party/nccl/nccl/src'
Compiling  misc/nvmlwrap.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/nvmlwrap.o
Compiling  misc/rings.cc                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/rings.o
Compiling  init.cc                             > /nethome/ebj26/apps/pytorch/build/nccl/obj/init.o
Compiling  misc/group.cc                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/group.o
Grabbing   include/nccl_net.h                  > /nethome/ebj26/apps/pytorch/build/nccl/include/nccl_net.h
Compiling  misc/ibvwrap.cc                     > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/ibvwrap.o
Compiling  collectives/reduce_scatter.cc       > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/reduce_scatter.o
Compiling  channel.cc                          > /nethome/ebj26/apps/pytorch/build/nccl/obj/channel.o
Compiling  collectives/all_gather.cc           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/all_gather.o
Compiling  transport/p2p.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/p2p.o
Compiling  enqueue.cc                          > /nethome/ebj26/apps/pytorch/build/nccl/obj/enqueue.o
Compiling  bootstrap.cc                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/bootstrap.o
Compiling  misc/argcheck.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/argcheck.o
Compiling  transport/shm.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/shm.o
Compiling  transport/net.cc                    > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/net.o
Compiling  misc/topo.cc                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/topo.o
Compiling  transport/net_ib.cc                 > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/net_ib.o
Compiling  collectives/broadcast.cc            > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/broadcast.o
Compiling  transport.cc                        > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport.o
Compiling  transport/net_socket.cc             > /nethome/ebj26/apps/pytorch/build/nccl/obj/transport/net_socket.o
Compiling  misc/utils.cc                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/utils.o
Compiling  misc/trees.cc                       > /nethome/ebj26/apps/pytorch/build/nccl/obj/misc/trees.o
Compiling  collectives/all_reduce.cc           > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/all_reduce.o
Compiling  collectives/reduce.cc               > /nethome/ebj26/apps/pytorch/build/nccl/obj/collectives/reduce.o
Generating nccl.h.in                           > /nethome/ebj26/apps/pytorch/build/nccl/include/nccl.h
Generating nccl.pc.in                          > /nethome/ebj26/apps/pytorch/build/nccl/lib/pkgconfig/nccl.pc
make[2]: Entering directory /nethome/ebj26/apps/pytorch/third_party/nccl/nccl/src/collectives/device'
make[2]: Entering directory /nethome/ebj26/apps/pytorch/third_party/nccl/nccl/src/collectives/device'
Linking    libnccl.so.2.4.8                    > /nethome/ebj26/apps/pytorch/build/nccl/lib/libnccl.so.2.4.8
make: *** [src.build] Segmentation fault (core dumped)
[200/3081] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/descriptor.cc.o
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File ""setup.py"", line 759, in <module>
    build_deps()
  File ""setup.py"", line 311, in build_deps
    cmake=cmake)
  File ""/nethome/ebj26/apps/pytorch/tools/build_pytorch_libs.py"", line 59, in build_caffe2
    cmake.build(my_env)
  File ""/nethome/ebj26/apps/pytorch/tools/setup_helpers/cmake.py"", line 334, in build
    self.run(build_args, my_env)
  File ""/nethome/ebj26/apps/pytorch/tools/setup_helpers/cmake.py"", line 142, in run
    check_call(command, cwd=self.build_dir, env=env)
  File ""/nethome/ebj26/apps/anaconda3/envs/detectron2/lib/python3.7/subprocess.py"", line 347, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '16']' returned non-zero exit status 1.

PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: CentOS release 6.5 (Final)
GCC version: (GCC) 5.5.0
CMake version: version 3.14.0

Python version: 3.7
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.17.2
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] mkl-service               2.3.0            py37he904b0f_0  
[conda] mkl_fft                   1.0.14           py37ha843d7b_0  
[conda] mkl_random                1.1.0            py37hd6b4f25_0
`

## Additional context

Any help would be greatly appreciated!
"
510,24918,0,"A lot of deprecation warnings in tests. Several test suites, for example test_indexing, throw deprecation warnings like this: 

"
641,27844,0,"torch.hub does not handle tags and branches with path separator (e.g. ""/"") in them. ## 🐛 Bug

 treats tag or branch names as file paths; if the tag or branch has a path separator (e.g. ""/"" on Linux) in it e.g. branch  as in

    torch.hub.list(""moabitcoin/ig65m-pytorch:issues/4"")

then torch hub integration fails e.g. see




## To Reproduce

Steps to reproduce the behavior:

1. add a simple hubconf.py to your repository
1. create a tag or branch with a path separator in its name
1. use torch hub functionality with that tag or branch

## Expected behavior

 works with arbitrary git tags and branches

## Environment

<details>
<summary>env</summary>



</details>


## Additional context

Workaround: create a tag or branch alias without the path separator in it.

cc @ailzhang"
482,29024,0,"Speed slows down after model quantization. my model is based on E-inception,after quantized  , i found it forward slower,
the original model run on CPU cost 200ms and 20ms on GPU , but the quantized model cost 290ms,

is it caused by the operation of layers concatenate?? 


cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"
240,28781,0," run run_test.py has error. ## ❓ Questions and Help
hi 
I am new at pytorch 

i try to install pytorch with ROCm and step by step following https://rocm-documentation.readthedocs.io/en/latest/Deep_learning/Deep-learning.html#building-pytorch-for-rocm 

output of ""PYTORCH_TEST_WITH_ROCM=1 python test/run_test.py --verbose"" should all correct ,but i got some error like this


test_lerp_tensor_weights,
test_unused_output_device_cuda were failed 

could anyone help me figure out please

my hardware :
os ubuntu 16.04
motherboard ROG CROSSHAIR VI HERO
cpu 2700x
gpu rx480 * 2


"
151,17699,0,"nn.Embedding broken for half. ## 🐛 Bug

Using the following script will break with error 



<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1. Run the script above

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

No error
<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): 1.0.1.post2
 - OS (e.g., Linux): Mac OSX 10.14.3
 - How you installed PyTorch (, , source): conda
 - Build command you used (if compiling from source):
 - Python version: 3.6
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
207,13929,0,"Maturing JIT Optimization Framework. ## 🚀 Feature
Taking a current look at the JIT optimization framework there are a couple of feature missing. 

We do not have a structure that captures post pass analysis of the optimization pass. Analysis are fundamental in optimization as they allow us to understand the necessary passes better. For example if PyTorch had a simple analysis per pass that calculated the number of nodes in the computational graph changed we could introduce features such as fixed point optimization. We could mature even further to Immutable passes (analysis passes) which outline in what order to apply passes to maximize performance. Currently we have a very trivial implementation of optimization which simply runs [passes in linear order](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/graph_executor.cpp#L458).

Another feature are pass managers. PyTorch implicitly does this by defining families of optimization passes (e.g. PeepholeOptimizeImpl implements a family of peephole optimization). An explicit OO implementation of pass managers will substantially clean up the code.

Simplifying the implementation of passes will increase agility. There's a lot of code reuse happening with respect to iterating the graph and checking for a pattern to apply a transform. If we could implement type's of passes (e.g. PredicatePass/PatternMatchPass, LoopPass), we could simplify code and reduce code reuse. 

## Motivation
I'm hoping by introducing these concepts to PyTorch's JIT we could mature the optimization framework and increase the performance of PyTorch in general.

## Pitch

This same approach and features I've implemented in the [ONNX optimization framework](https://github.com/onnx/onnx/tree/master/onnx/optimizer). If we agree that PyTorch needs these features I can start working on this."
601,1151,0,"Does pytorch implement SpatialUpSamplingBilinear layer?. Hi, I want to load a model trained by torch, which contains the layer ""nn.SpatialUpSamplingBilinear"", the convert code is simple as blow:



it will generate the exception:


And I find the  in pytorch,  does it function like the layer ? 
"
399,3573,0,"Mac OS X build with Python 3.6 fails. One representative error:



The cause of the problem is aa911939a328eff55c9b28b39ed3c43507ba8a2a:



It seems that on clang, changing the type parameter here is sufficient to cause template instantiation to fail.

Maybe the easiest way to fix this is to write a more portable version of PyInt_FromLong (and friends) which always returns ."
602,16558,0,"Pytorch Import:  failed to map segment Error . I installed Pytorch .3.1 using pip3 for python3.5, and the installation was successful.
However, I am unable to load the package in python

import torch
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""user/.local/lib/python3.5/site-packages/torch/__init__.py"", line 56, in <module>
    from torch._C import *
ImportError: libgcc_s.so.1: failed to map segment from shared object


Can anybody help, how to resolve this?

Best"
557,27820,0,"IterableDataset should be added into dataset.pyi. ## 🐛 Bug
trying to import  in  creates an error in PyCharm. look at the following picture:
![图片](https://user-images.githubusercontent.com/44257865/66712058-9c6b5500-edc9-11e9-9492-64c416939e7b.png)

It says that , where in , we can see that:
![图片](https://user-images.githubusercontent.com/44257865/66712088-f9670b00-edc9-11e9-9ad4-417bc791c207.png)

It's obviously that there is no  but only its super class  exists.
I believe it should be added.


## To Reproduce
Steps to reproduce the behavior:

1. Open PyCharm
2. Write 
3. Error

## Expected behavior

No unresolved reference hinted.

## Environment


## Additional context


cc @SsnL @ezyang"
743,29805,0,"I use Torch Script to convert  has error. ## ❓ Questions and Help
attribute 'use_res_connect' of type 'bool' is not usable in a script method (did you forget to add it __constants__?):
at d:\PyCode\FeatherNetB\models\FeatherNet2.py:77:15
    @torch.jit.script_method
    def forward(self, x):
        if str(self.use_res_connect) == ""True"":
               ~~~~~~~~~~~~~~~~~~~~ <--- HERE
            return x + self.conv(x)
        else:
            if self.downsample is not None:
                return self.downsample(x) + self.conv(x)
            else:
                return self.conv(x)
### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
"
394,28775,0,"[feature request] Whether or not any planning to support mips64 arch environments. 

## 🚀 Feature
pytorch can be compiled in  arch successfully, and then utilize related API to develop :+1: 

## Motivation
I'm working on compiling pytorch in mips64 arch, but it doesn't work


I have tried add  in  and , same error happened.

## Pitch
pytorch can be compiled in  arch

<!-- A clear and concise description of what you want to happen. -->

## Alternatives
1) When I see warning info in  for , I plan to fake match rule in , for example:

The ultimate result will also failed since  . So, whether or not  support  arch, too? Thanks!
"
366,3158,0,"[Feature Request] Sparse-Dense elementwise Multiplication . Hi, thanks again for the great work.

I would like to raise an issue to discussion how to implement the following features:
Dense[m,1] * Sparse[m,n] -> Sparse[m,n]
Sparse[m,n] * Dense[1,n] -> Sparse[m,n]
Sparse[m*n] * Dense[b,1,n] -> Sparse[b,m,n]

these features are fully supported by the [cuSPARSE Level 3](https://developer.nvidia.com/cusparse), as for the CPU side, I'm not quite sure weather MKL was satisfied. 

I would like to help on the implementation if needed, could anyone give me a brief guideline that what have be done and what still need to be do?



 

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @aocsa @nikitaved @pearu @mruberry @vincentqb"
328,11980,0,"[Enhancement] Increase user-friendliness of dataset.random_split. ## Issue description

Currently, when using the [random_split function](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py)  the parameters that need to be given are:
- dataset
- list that contains the lengths of splits to be produced

This means a user has to calculate these upfront and add them to the function as parameters like this:




Wouldn't it be better if the second parameter was a list that contains the percentages in which the users wants the split to happen, like this:
 .

The result of this change would be cleaner code for the user and also I believer in a more natural way of creating the splits.

I would like to pick this up as a pull request!





cc @VitalyFedyunin @ejguan"
125,15028,0,"License unavailable in the installed package. Hi. I noticed that, after pytorch is installed, the PKG-INFO file says the license is unknown. Our tools rely on this file to obtain the correct license information. Is it possible to put correct license information in it?

For example,  has:
"
198,21922,0,"Segmentation fault using all_reduce with cuda:1 (MPI). ## 🐛 Bug

Using  under cuda-aware MPI with a cuda device other than  causes segmentation fault. I managed to bypass this for a very specific use case by setting  and then using  within pytorch.
 
## To Reproduce



Steps to reproduce the behavior:

> mpirun -np 4 --oversubscribe -host 127.0.0.1 python test.py

Segmentation fault:

## Expected behavior

No crash

## Environment

PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: Tesla P100-PCIE-16GB
GPU 1: Tesla P100-PCIE-16GB

Nvidia driver version: 418.67
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.16.4
[pip3] numpydoc==0.8.0
[pip3] torch==1.1.0
[pip3] torchvision==0.3.0a0+c94a158
[conda] blas                      1.0                         mkl  
[conda] libmklml                  2018.0.3                      0  
[conda] magma-cuda100             2.5.0                         1    pytorch
[conda] mkl                       2019.4                      243  
[conda] mkl-dnn                   0.14                 h6bb024c_0  
[conda] mkl-include               2019.4                      243  
[conda] mkl-service               2.0.2            py36h7b6447c_0  
[conda] mkl_fft                   1.0.12           py36ha843d7b_0  
[conda] mkl_random                1.0.2            py36hd81dba3_0  
[conda] torch                     1.1.0                    pypi_0    pypi
[conda] torchvision               0.3.0a0+c94a158          pypi_0    pypi


MPI 3.0.0 - cuda aware"
154,27703,0,"FAIL: test_torch (__main__.TestDocCoverage). ## 🐛 Bug

after our v1.3.0 doc changes i am seeing a test failure

FAIL: test_torch (__main__.TestDocCoverage)


## To Reproduce

Steps to reproduce the behavior:

1. git checkout upstream master
2. python ./test/test_docs_coverage.py

AssertionError: Items in the second set but not the first:
'real'
'imag'
'conj'
'angle' : 
The lists of tensor methods documented in tensors.rst and in python are
different. Did you forget to add a new thing to tensors.rst, or whitelist
things you don't want to document?

======================================================================
FAIL: test_torch (__main__.TestDocCoverage)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""./test/test_docs_coverage.py"", line 73, in test_torch
    don't want to document?''')
AssertionError: Items in the first set but not the second:
'promote_types'
'result_type'
'can_cast'
Items in the second set but not the first:
'quantize_per_tensor'
'quantize_per_channel' : 
The lists of functions documented in torch.rst and in python are different.
Did you forget to add a new thing to torch.rst, or whitelist things you
don't want to document?

----------------------------------------------------------------------
Ran 2 tests in 0.003s

FAILED (failures=2)

## Expected behavior


git checkout pr-fix-doc-test-failure

gottbrath@ubuntu:~/pytorch-for-doc$ python ./test/test_docs_coverage.py 
..
----------------------------------------------------------------------
Ran 2 tests in 0.003s

OK




cc @ezyang"
612,10660,0,"Inconsistent output of torch.mm when called in two mathematically equivalent way. Hi,

 I tired to use torch.mm to get matrix multiplication, but found it returned slightly different results when I called it in two equivalent way.

Assume we have two matrix  matrix_a and matrix_b and want to get the matrix multiplication matrix_a@matrix_b
1. First, I use matrix_a and matrix_b as input directly, torch.mm(matrix_a, matrix_b)
2. On the other hand, I call torch.mm with each row of matrix_a together with matrix_b as inputs, and then concatenate the results.

The two results are expected to be exactly the same, but they are still slightly diferent. So can somebody explain the reason?

Thanks,


Here is my test code.



Here is the output.

>  2018-08-18 21:32:11,294 INFO 87997 [test.py:81] difference between two values: tensor(1.00000e-06 *
>        1.3113)
> 2018-08-18 21:32:11,295 INFO 87997 [test.py:83] batch id: 0
> 2018-08-18 21:32:11,295 INFO 87997 [test.py:84] [-0.89934576  0.11556479 -0.48109883 -0.3961889 ]
> 2018-08-18 21:32:11,295 INFO 87997 [test.py:85] [-0.89934576  0.11556476 -0.4810989  -0.39618894]
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:83] batch id: 1
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:84] [ 2.6500921   0.5928108  -2.213797    0.39320117]
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:85] [ 2.6500921   0.59281075 -2.2137969   0.3932012 ]
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:83] batch id: 2
> 2018-08-18 21:32:11,296 INFO 87997 [test.py:84] [ 1.3148142  3.5158207 -2.2286756 -2.1263554]
> 2018-08-18 21:32:11,297 INFO 87997 [test.py:85] [ 1.3148141  3.515821  -2.2286754 -2.1263554]
> 2018-08-18 21:32:11,297 INFO 87997 [test.py:83] batch id: 3
> 2018-08-18 21:32:11,297 INFO 87997 [test.py:84] [-0.25188264  1.0855039   1.4054772  -0.47265998]
> 2018-08-18 21:32:11,297 INFO 87997 [test.py:85] [-0.25188267  1.085504    1.4054773  -0.47265998]
> 2018-08-18 21:32:11,298 INFO 87997 [test.py:83] batch id: 4
> 2018-08-18 21:32:11,298 INFO 87997 [test.py:84] [ 1.250929   3.790292  -1.7040166 -2.443583 ]
> 2018-08-18 21:32:11,298 INFO 87997 [test.py:85] [ 1.250929   3.790292  -1.7040166 -2.443583 ]"
723,29211,0,"""multinomial_kernel_cuda"" not implemented for 'Half'. It appears that even after the  fixes,  no longer works with  dtypes

## To Reproduce







## Environment


## Additional context
This is not blocking us because we can convert to fp32 before sampling"
713,15129,0,"ResNet -101 does not run when converted to sequential (returns dimension mismatch). ## 🐛 Bug

I wanted to obtain convolutional features from ResNet-101. I had already figured out how to do it, but I wanted to run a simple test to check if I am applying the correct preprocessing steps, so instead of omitting the last layer, I transformed the whole network into a nn.Sequential(), only to get an error:
RuntimeError: size mismatch, m1: [2048 x 1], m2: [2048 x 1000] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:940

## To Reproduce

Here is the code:


When I run resnet_eval with a valid image path, I get the following output:



Of course, transforming the whole net into a sequential model is non-sense, we can just replace the first line (after the docstring) of resnet_eval with

and get the expected output


Another approach is to remove the final layer, reshape the output tensor and then add the last layer again, like this:

and once again, this works as expected, giving the same output as when running


If I print the tensor values, they are identical in the two last cases. So why do I come accross the mismatch error in the first case (or equivalently, why is the last case valid)?

## Expected behavior

The first case should run smoothly, giving the same output as the second one.

## Environment

Tested with Python3.5 and 
torch==1.0.0
torchvision==0.2.1
"
396,5949,0,"Bug Report re-initializing weights after layer initialization . 
Why is this allowed? considering the fact that it changes the entire layer 




This gives a dimension mismatch error
"
722,11121,0,"Running torch.cuda.is_available() before import from torch._C import *. ## Issue description

Using a NFS, sometimes I find myself on machines without _Cuda_, although _PyTorch_ was installed on a _Cuda_ enabled machine. _PyTorch_ fails to load and crashes instead.

## Code example



## System Info

- PyTorch or Caffe2: _PyTorch_
- How you installed PyTorch (conda, pip, source): , on a _Cuda_ enabled machine
- OS: _CentOS_ 7
- PyTorch version: 
- Python version: 
- CUDA/cuDNN version:  or "
127,3462,0,"sort() and  topk() behave weirdly when given large numbers. 

In the above, a matrix of large numbers, where every element is the same, is added to a matrix of small numbers (this shouldn't affect the ordering of the small matrix). Hence  should all be the same. However,  is different from the rest, meaning adding a large numbers to the input makes  behave weirdly. So does , presumably for the same reason.

I've been getting weird results with my beam search implementation in PyTorch, and this was the culprit. In beam search you have to sort a list of large negative numbers (log probabilities of candidate translation sentences). Can anyone suggest a quick workaround? Thanks!"
226,2677,0,"Require_grad = True, but printed as ""None"". My code is

x = Variable(torch.rand(8, 1, 5, 5), requires_grad=True)
conv = nn.Conv2d(1, 1, 3)
y = conv(x)
final = torch.sum(y)
print('y.grad', y.grad)
final.backward()
print('y.grad', y.grad)
print('y.grad', y.requires_grad)

However, the output is 
y.grad None
y.grad None
y.grad True

If y.requires_grad==True, shouldn't the second y.grad output some gradients instead of None? For x, the gradient showed normally after the backward()"
505,19162,0,"Denormalize option in torchvision.utils.save_image(). ## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
A function to denormalize an image based on mean and standard deviation.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
When working with images on NN's trained on a specific dataset (for example ImageNet), an image is first normalized to the mean and standard deviation of that dataset. When we want to save such an image later in the process we can use the function *torchvision.utils.save_image()*. However the image is still normalized and will have a different mean and standard deviation compared to the original image. There is no option to *denormalize* such an image such that the initial normalization is undone and the saved image has the same mean and std.

## Pitch

<!-- A clear and concise description of what you want to happen. -->
A extra parameter to the *torchvision.utils.save_image()* function to denormalize an image based on a mean and standardization array.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->
One way to tackle the problem currently is to use the *transforms.Normalize()* function. My current implementation is shown below. One flaw of this implementation is that the image has to be clipped to keep the values between 0 and 1. Thus some information is lost. I am not sure how to do this operation lossless.



"
4,7714,1,"Very slow for gradient penalty!. Gradient penalty (GP) here means we minimize L2 norm of the gradient w.r.t input images.

I work on Ubuntu14.04, Python2.7, CUDA8.0 and CUDNN.
The version of Pytorch is 0.4.0.
I find it is very slow when I apply gradient penalty (GP) for training CIFAR10 with Resnet18.
I test the average running time of each step with and without GP:

4 times slower compared with the standard training!
Here is my code:

Is there better implementation?
Or just as slow as it is? "
45,3863,1,"Considerable slowdown in Adam.step after a number of epochs with multiple losses. I have a model with multiple outputs and, therefore, multiple losses. When training I accumulate the losses using retain_graph. Something along the lines of:



where input, output and target are dictionaries with the respective data for the different inputs and losses.

I am using Adam for the optimization. 
I've noticed that after a number of epochs, the running time of an epoch goes suddenly up from 7sec to 34sec.
I also noticed a slowdown of CPU usage in my computer (I haven't test this yet on the GPU). Memory usage doesn't seem to increase.

I profiled the code and I saw this (output from cProfile):

Normal epoch:

Slow epoch:

I've tested with other adaptive losses like Adagrad, and there I can't see the issue.
It seems to be related to this line of code in Adam.step():

Any ideas about why this is happening? It seems like suddenly the size of the accumulated gradient explodes, but  I can't see why.

cc @vincentqb @VitalyFedyunin @ngimel"
121,6064,0,"[jit]][script] handling of 'void' returns. Methods returning no values generate weird error messages:

For instance:



produces 


What is expected?

* omitting a return statement should be valid and just return 0 values (someone might just be print debugging at this point and hasn't written the return yet)
*  with no value should work."
494,16277,0,"Inconsistency in inception model. ## 🐛 Bug

Hi
I was going through the torchvision code to implement c++ API and I noticed an Inconsistency.

In line 298 of inception.py stddev of a class of type BasicConv2d is set to 0.01:



But in line 60 it checks if it's conv2d or linear which a BasicConv2d is none:



I put this code inside the previous if statement:



and it only printed:



so convs inside conv1 BasicConv2d of InceptionAux get initialized with stddev=0.1

is this an error or is it intentional?"
63,28503,1,"Inconsistent behaviour in einsum with fp16. ## 🐛 Bug

torch.einsum seems to have inconsistent behavior with half-precision tensors, as illustrated in the snippet below (minimal non-working example):


As we can see in the example above, the results we get in fp16 can be significantly different from the fp32 (all the 0.000) but are also inconsistent for the same computation as highlighted in the comments.

## To Reproduce

**See snipped above.**

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Running the same computation multiple times should give the same result everytime for the same input.

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): 1.3.0
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (, , source): conda
 - Build command you used (if compiling from source): N/A
 - Python version: 3.7.4
 - CUDA/cuDNN version: 10.2/7.5
 - GPU models and configuration: Code was run on a single 2080 Ti
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
30,26797,1,"Inconsistent gradient from different backend of CTCLoss. ## 🐛 Bug

I'm switching to the cudnn backend of CTCLoss since the other is not fully reproducible,
however, it turns out that the exact same model that used to work with pytorch's cuda backend ctc loss now failed.
With some simple example, I found that there's a huge difference in gradient (both direction and magnitude) between two backends.
I'm not sure if the bug is on pytorch or cudnn, but as far as I know, TensorFlow also used CTC from cudnn and there is no similar issue.
Thanks in advance.

## To Reproduce

## Behavior



## Environment




cc @ezyang @gchanan @zou3519"
498,24537,0,"Migrate `asin` and `asin_` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
483,31483,0,"undefined symbol: _Py_ZeroStruct. Although I have installed ""pytorch"" and ""text"" with python and python3 using


When I want to text_classification tutorial, I get this error


I don't know what is missing here. Any way to fix that?

cc @ezyang"
632,17108,0,"[Running on windows 10] cuda runtime error (30) : unknown error at ..\aten\src\THC\THCGeneral.cpp:87. ## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)

While trying to run my test.py file on my anaconda prompt I got these messages below:

CUDA™ is AVAILABLE
Please assign a gpu core (int, <1): 0
THCudaCheck FAIL file=..\aten\src\THC\THCGeneral.cpp line=87 error=30 : unknown error
Traceback (most recent call last):
  File ""VSLcore.py"", line 202, in <module>
    DQNAgent()
  File ""VSLcore.py"", line 87, in DQNAgent
    torch.set_default_tensor_type('torch.cuda.FloatTensor')
  File ""D:\Softwares\Anaconda3\lib\site-packages\torch\__init__.py"", line 158, in set_default_tensor_type
    _C._set_default_tensor_type(t)
  File ""D:\Softwares\Anaconda3\lib\site-packages\torch\cuda\__init__.py"", line 162, in _lazy_init
    torch._C._cuda_init()
RuntimeError: cuda runtime error (30) : unknown error at ..\aten\src\THC\THCGeneral.cpp:87

What should I do?"
658,28347,0,"arange sometimes changes dimensionality of output tensor. ## 🐛 Bug

Using  with an  target Tensor object, it will sometimes change the dimensionality of the target Tensor.  I'm creating a (1,N) tensor, and sometimes calling  changes it to a (N+1) vector.  Same on cuda or cpu.


## To Reproduce



This will output:



## Expected behavior

 should not change the shape of the target Tensor.

## Environment

PyTorch 1.3, Ubuntu 18.04LTS, py 3.6.9 under Anaconda.  In detail (from ):



## Additional context

I started doing it this way (out= Tensor) because I wanted to make sure the whole thing to happen on GPU, and that was the first thing I figured out.  I see another way to do this now --  which works.  But this seems like a bug.


cc @ezyang @gchanan @zou3519 @jerryzh168"
536,1301,0,Test failure when running pytorch without cuDNN
520,8522,0,"[JIT] Unsupported op descriptor scatter. Don't support scatter in dispatch. Example failure:



Affected tests:
- 
- 
- 
- 
- 
- 
"
353,12609,0,"Request for stripped down / inference only pytorch wheels. ## 🚀 Feature
Creating a precompiled pytorch wheel file that is trimmed down, inference only version.

## Motivation

Right now pytorch wheels are on average ~400MB zipped -> 1.+ GB unzipped, which is not a big deal for training & prototyping as generally the wheels are only installed once - but that's not the case for productionizing using service providers like sagemaker / algorithmia / etc.

## Pitch

If we can create a trimmed down, potentially inference only capable wheel file - we can directly improve the load time performance of these algorithms in serverless algorithm delivery environments, which could directly pytorch's ability to compete in the HPC serverless marketplace.

## Alternatives

We could also provide a clear way for users to create their own wheels, by simplifying and documenting the build process somewhat to enable optional features during the compilation process. 

## Additional context

Full disclosure, I'm an employee at Algorithmia and this change would make my life much easier :smile: 


cc @malfet @seemethere @walterddr"
650,17029,0,"Floating Point Exception on PyTorch nightlies. ## 🐛 Bug
We are seeing a floating point exception in the nightly containers but not in v1.0.1 container. I believe there is a mkl-dnn patch that needs to be on master?

@soumith "
67,19827,1,"Random GPU Operations Hang. ## 🐛 Bug

I'm having an issue where certain GPU operations hang. It doesn't appear to be a multi-GPU problem, I can reproduce it on a single GPU. It doesn't seem to reliably effect the same operations but if I change around their order or cache certain results on the GPU to minimize copies it could cause the hang to move to another command. 

## To Reproduce

For my current code I can reproduce the problem fairly reliably with the following command:



e.g. indexing the tensor  which is stored on GPU. I should point out that I was seeing this on other commands, usually copying something onto the GPU, but as I mentioned after changing how i was using the GPU by caching certain results to limit copies and reordering things, it is happening on this line. 

Here is the time taken by this command for several different images:



Here's what the above numbers mean. The first  is telling me which line is printing, so it can be ignored. The next number is how long it took to execute the offending line in seconds. The images are all of comparable size. Newline separates each image.

So you can see the first image executes both lines without problem. The next one, only the first line is slow and it takes over 30s to execute. This trend continues for the rest of the images.


## Environment



## Additional context

My first thought was that this is something to do with garbage collection on the GPU, or something to do with the multi GPU IOMMU problem other people have had, but I'm not so sure anymore, especially since I can reproduce on a single GPU.
"
628,20697,0,"Indexing by 1-dimension `int64` tensor returns incorrect shape. ## 🐛 Bug

Sometimes indexing by a 1-dimension  tensor with a single element returns a tensor with reduced rank.

## To Reproduce

Steps to reproduce the behavior:

<img width=""498"" alt=""Screenshot 2019-05-19 21 41 52"" src=""https://user-images.githubusercontent.com/630490/57996908-ec462a00-7a7e-11e9-8ba2-1a7bf5f71bca.png"">

This bug gets even better:

<img width=""228"" alt=""Screenshot 2019-05-19 21 44 09"" src=""https://user-images.githubusercontent.com/630490/57996986-4515c280-7a7f-11e9-962d-75f60589a4a2.png"">




## Environment

Collecting environment information...
PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 410.79
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.2
[pip] torch==1.1.0
[pip] torchvision==0.2.2
[conda] mkl                       2019.3                      199
[conda] pytorch                   1.1.0           py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch
[conda] pytorch-nightly           1.1.0.dev20190411 py3.7_cuda10.0.130_cudnn7.4.2_0    pytorch
[conda] torchvision               0.2.2                      py_3    pytorch"
490,7428,0,"libtorch test_jit hangs if an error occurs. Steps to reproduce:
1. Build this commit https://github.com/pytorch/pytorch/pull/7275/commits/8d10a9245e364920e7fdba3af03b3a14ff41f126 with CUDA in pytorch-linux-xenial-cuda9-cudnn7-py3-test (alternately, ezyang/test_jit_hangtest_jitcpp-build/libtorch/bin/test_jit`

Expected result: it terminates

Actual result: it hang:



when I ctrl-c it finally comes back:



CC @lantiga @goldsborough "
37,29429,1,"NLLLoss reduce=True returning nan in float16. ## 🐛 Bug
**Version**
torch 1.3.1
torchvision 0.4.1

**Notes**
NLLLoss reduce=True doesn't seem to work in float16.
Also, training a model with loss1 in float16 doesn't seem to decrease the loss. (The model trains fine with loss1 in float32)
Possible related to #14878

**Code to reproduce**




"
590,10617,0,"Command ""python setup.py egg_info"" failed with error code 1. Trying to install pytorch on my pyenv python 2.7



Installed other libraries like numpy and pandas which worked fine.
pip list gives:

I am on macOS 10.13.5."
167,16646,0,"loss.backward() cause malloc: *** error for object 0x7fb849370210: incorrect checksum for freed object. I build a net: resnet18 + ctc loss, when run loss.backward(), there will be a error:python(17267,0x700007df0000) malloc: *** error for object 0x7fb849370210: incorrect checksum for freed object
i not find any method to resolve it?

note: my training date size is not equal, this mean :the widht of image is variable, and the height of imgage is 32,    the label length is variable

========= my net.py is ===================
# coding=utf-8
import torch
import torch.nn as nn
import torch.nn.functional as F


# 直接定义双层LSTM
class BiLSTM(nn.Module):
    def __init__(self, num_in, num_hidden, num_out):
        super(BiLSTM, self).__init__()
        self.LSTM = nn.LSTM(input_size=num_in, hidden_size=num_hidden, num_layers=2,
                            bidirectional=True, batch_first=False)
        self.embeding = nn.Linear(num_hidden * 2, num_out)

    def forward(self, input):
        n, c, h, w = input.size()
        input = input.permute(0, 3, 2, 1)
        input = input.reshape(n, w, h*c)
        input = input.permute(1, 0, 2)
        recurrent, _ = self.LSTM(input)

        out_put = self.embeding(recurrent)
        return out_put


class ResidualBlock(nn.Module):
    def __init__(self, inchannel, outchannel, stride=(1, 1)):
        super(ResidualBlock, self).__init__()
        self.left = nn.Sequential(
            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(outchannel),
            nn.ReLU(inplace=True),
            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(outchannel)
        )
        self.shortcut = nn.Sequential()
        if stride != (1, 1) or inchannel != outchannel:
            self.shortcut = nn.Sequential(
                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(outchannel)
            )

    def forward(self, x):
        out = self.left(x)
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    def __init__(self, ResidualBlock, num_classes=10, test=False):
        super(ResNet, self).__init__()
        self.inchannel = 64
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU()
        )
        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=(1, 1))
        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=(2, 2))
        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=(2, 2))
        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=(2, 1))

        # self.rnn = nn.Sequential(
        #     BidirectionalLSTM(512, 512, 512),
        #     BidirectionalLSTM(512, 512, num_classes),)
        self.rnn = BiLSTM(512, 512, num_classes)
        # self.fc = nn.Linear(512, num_classes)
        self.test = test

    def make_layer(self, block, channels, num_blocks, stride):
        strides = [stride] + [(1, 1)]*(num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.inchannel, channels, stride))
            self.inchannel = channels
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, (4, 1))

        if self.test:
            return out

        # out = out.view(out.size(0), -1)
        # out = self.fc(out)
        out = self.rnn(out)
        return out


def resnet18():
    return ResNet(ResidualBlock)



========== my train code is ==========
    def train_batch(self, epoch):
        self.model.train()
        for batch_idx, (img, label, width, lens) in enumerate(self.train_loader):
            self.optimizer.zero_grad()
            preds = self.model(img)
            log_probs = preds.log_softmax(2).detach().requires_grad_()
            loss = self.criterion(log_probs, label, width, lens)
            print(loss)
            loss.backward()
            self.optimizer.step()  # 更新参数
            if (batch_idx+1) % self.log_interval == 0:
                lr = self.scheduler.get_lr()[0]
                src_str = self.label_convert.get_str_from_label(lables=label)
                preds_size = torch.IntTensor([preds.size(0)] * preds.size(1))
                _, preds = preds.max(2)
                preds = preds.squeeze(2)
                preds = preds.transpose(1, 0)
                sim_preds = self.label_convert.decode(preds, preds_size, raw=False)
                n_correct = 0
                for pred, target in zip(sim_preds, src_str):
                    if pred == target:
                        n_correct += 1
                distance_value = util.edit_distance(src_str, sim_preds)
                localtime = datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
                print('[%s]\tTrain Epoch: {%d/%d}\tLoss:%.6f, acc: %.2f, error: %.2f, lr = %.4f'
                      % (str(localtime), epoch, batch_idx, loss, n_correct, distance_value, lr))
            if (batch_idx+1) % self.save_interval == 0:
                new_save_name = 'resnet18_%d_%d.pth' % (epoch, batch_idx+1)
                torch.save(self.state, new_save_name)
                util.update_file(self.save_path, new_save_name)
        new_save_name = 'resnet18_%d.pth' % epoch
        torch.save(self.state, new_save_name)
        util.update_file(self.save_path, new_save_name)
"
18,29809,1,"Memory leak with Conv1d on CPU. ## 🐛 Bug

When training (on CPU) a simple one-layer CNN (with variable-width batches, as it is standard in text classification), the memory usage increases significantly at each  call, and quickly causes an out of memory.

## To Reproduce



When running , I get this OOM error after 30 iterations:


The [memory profiler output](https://github.com/pytorch/pytorch/files/3846948/memory_profiler_output.txt) clearly shows a steady memory increase (and never decrease) at each  call, of **190Mb per call** on average.


## Expected behavior

I would expect the memory usage to increase slightly at the first step, but then stabilize - not increase linearly.

## Environment

PyTorch version: 1.3.1+cpu
Is debug build: No
CUDA used to build PyTorch: None

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.17.4
[pip3] torch==1.3.1+cpu
[conda] Could not collect


## Additional context

Here all the things I tested, following advice in the multiples related issues I found (https://github.com/pytorch/vision/issues/984, https://github.com/pytorch/pytorch/issues/5285 and others):

1. Check that there are no references to a model output kept accross iterations (e.g. loss)

2. Remove anything possible to narrow down the issue (data loader, tqdm, backward, optimization, ...). This is why I only kept the forward pass with dummy data.

3. Use  :
 
Same exact issue happens.

4. With random but fixed-size tensors (replacing  with ), the issue disappears.
However, I am doing CNN on text, so unfortunately, I need batches to have variable width...

5. It I remove the  line, the issue disappears as well, so it is not a problem with the data generation itelf.

6. I tried the following Pytorch versions and had the exact same issue with all of them:
*  from pip ()
*  from pip ()
*  from conda ()

cc @ezyang @gchanan @zou3519 @jerryzh168"
657,17489,0,"Windows CPU debug build fails at linking stage. ## 🐛 Bug
Hi, 
I can't build Windows Debug CPU version, the build fails during linking. 
I am using the latest source code. At the end of post there are exact commands I execute.
I am really after Libtorch build, I do not care about Python, but because I do not understand the whole thing, I feel obliged to build all.

Thank you,
Slawek

1. First of all an observation, CMakeLists.txt contains this:
set (CMAKE_CXX_FLAGS_DEBUG ""${CMAKE_CXX_FLAGS_DEBUG} -fno-omit-frame-pointer -O0"")
set (CMAKE_LINKER_FLAGS_DEBUG ""${CMAKE_STATIC_LINKER_FLAGS_DEBUG} -fno-omit-frame-pointer -O0"")
which causes endless warnings of unknown options.

Compilation appears successful  (2000 files or so) but then failures happen during linking
2.  

3. And then, another failure

Now, I downloaded and copied python37_d.lib into C:\local\Anaconda3\envs\PytorchDebug\libs and python37_d.dll into C:\local\Anaconda3\envs\PytorchDebug, but it does not help.


## Environment
 - PyTorch Version: 1, latest code
 - OS: Windows 10
 - Build commands:
call c:\local\Anaconda3\scripts\activate PytorchDebug
rem Done before: conda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing
rem Done before, in f:\progs2\PytorchDebug: git clone --recursive https://github.com/pytorch/pytorch
F:
cd progs2\PytorchDebug\pytorch
set ""VS150COMNTOOLS=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Auxiliary\Build""
set CMAKE_GENERATOR=Visual Studio 15 2017 Win64
set DISTUTILS_USE_SDK=1
set NO_CUDA=1
set DEBUG=1
call ""%VS150COMNTOOLS%\vcvarsall.bat"" x64 -vcvars_ver=14.11
python setup.py install

 - Python version: 3.7
"
326,28515,0,"Get rid of libc10.so. I finally have a good reason to merge libc10.so into libtorch.so (and corresponding libc10_cuda.so into libtorch_cuda.so, etc.): I am trying to devirtualize access to AutogradMeta, but because TensorImpl lives in c10 and AutogradMeta lives in torch, I cannot do this as the destructor would have to cross a dynamic library boundary. By absorbing c10 into torch I will be able to do this.

Some dangers: putting c10 into libtorch might push Windows build over max library size. See https://github.com/pytorch/pytorch/issues/27215

Other alternatives I thought of:
* Move TensorImpl back to ATen/core
* Keep the virtual interface to AutogradMeta

Some alternatives that don't work
* Move AutogradMeta into c10. AutogradMeta must be declared after Tensor as it contains a field Tensor, and we cannot replace that field with an  as the public API of tensor  returns a mutable  reference. (Though, it might be possible to fix this up by just moving TensorImpl out of c10)

cc @ezyang @gchanan @zou3519 @jerryzh168 @dzhulgakov @smessmer "
491,923,0,"Build broken on Mac OS X. Building form source for Mac OS X fails. Turns out there is a missing dependency between THPP and THCS, adding this dependency to CMakeLists.txt was sufficient for a successful build."
534,12562,0,"Declarations.yaml cleanup. Based on notes from @ezyang, @zdevito and @gchanan.

We'd like Declarations.yaml to be the single, externally visible API that all consumers, both internal and external, go through. Up until now, it has been designed under the assumption that only internal consumers make use of it, and that we can refactor these consumers when we wish to change Declarations.yaml. However, we have quickly gotten into a state where even our internal consumers are unmaintainable. This issue tracks a list of changes we'd like to apply to Declarations.yaml.

Meta-principle: the metadata for a type primarily consists of the type string (e.g., ). However, there may be some other metadata.

- [x] Stop hard-coding the list of arguments that  expands into. Instead, the list of kwargs a TensorOptions expands into should be specified only once and used everywhere.
- [ ] De-C++-ify our type syntax. This means no more putting  in an input type (so basically, drop  and use  only). Our type system should be C++/Python-agnostic. To determine if something has a reference or not, should be determined if the parameter in question is an input or output parameter, which should be part of the type string.
- [x] Add optional to the type syntax. @wanchaol is working on this
- [ ] Replace  with a simple list of  and/or .
- [x] Eliminate  entirely; once optional is supported, any arguments which have ""complex"" default values can simply specify the argument as optional, and then let the kernel compute the default argument itself
- [x] Unify  and . We think we have two only because TH and native had different paths, accidentally.
- [x] Augment type strings with alias sets, perhaps with exclamation marks. This could be used to remove  annotations. This will be handled by the JIT team.
- [x] Eliminate . @gchanan is working on this
- [ ] Eliminate . These would be replaced as outputs of ""inner"" native functions, which are the actual differentiable functions. Blocked on adding named outputs.
- [x] Add named outputs, ala . This helps the clarity of defining derivatives for multi-return functions.
- [ ] Eliminate , since we have eliminated most broadcasting from Type
- [ ] Eliminate 
- [ ] Eliminate , instead inferring it from the name of a function. Put this inference function in one place and make everyone use it.
- [ ] Eliminate , no one actually needs it anymore"
259,14726,0,"[caffe2] Corresponding C++ API for prepare_prediction_net. ## 🚀 Feature
Corresponding C++ API for prepare_prediction_net

## Motivation

We have a python API that is able to load predictor models in MetaNetDef format. https://github.com/pytorch/pytorch/blob/edb88b5f3af03718b443d015f195faa1832ce95b/caffe2/python/predictor/predictor_exporter.py#L127 However, the corresponding C++ API is missing.

## Pitch

In the Python world, we are able to export and load models in MetaNetDef format, however, when we want to productionize the model and load it in C++, the API to do so is missing.

## Alternatives

Right now, the only alternative is to export the init and predict nets seperately as protobufs and load them in C++."
392,21119,0,"One libtorch FindTorch.cmake module to support multi-configs(both debug and release). ## 🐛 Bug

Libtorch is excellent to provide cmake for multiple configs on windows. 

  * Download here (Release version): 
    https://download.pytorch.org/libtorch/cu90/libtorch-win-shared-with-deps-latest.zip
  * Download here (Debug version): 
    https://download.pytorch.org/libtorch/cu90/libtorch-win-shared-with-deps-debug-latest.zip

However, it is quite inconvenient. Usually, I need to copy different versions to switch between different configurations, which is annoying during development. Can we support both configs inside **one FindTorch.cmake**?

"
156,29246,0,"Replacing convolutional blocks causes exporting to Onnx to fail. ## 🐛 Bug

We are trying to export a model created in PyTorch to Onnx. The model is created with function:

and then, one convolutional block of that model is swapped with a different kind of block, like so:

When we try to export this updated model, like so:

an error occurs -  for some reason, PyTorch fails to export the model, that otherwise is fully functional.

If the blocks are not swapped, the network is exported successfully.

## To Reproduce

You can get the code from a temporary repository I've created for this issue:


Run  file, you will get the following error:




## Expected behavior

We expect to have our model exported, even if it has its convolutional blocks swapped with another type of block. Ideally, we would want to swap all the blocks of the model, not just the first one, as in the example given.

## Environment

 - PyTorch Version: 1.2.0
 - OS: Ubuntu 19.04
 - How you installed PyTorch: pip3
 - Python version: 3.7.3
 - CUDA/cuDNN version: No Cuda
 - GPU models and configuration: No


cc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"
404,17163,0,"Support torch.zeros with HalfTensor (half). 

This might be nontrivial to do because zeros is implemented in terms of vectorized fill on CPU, so you'd have to work out how to make that code work for half."
660,3491,0,"RuntimeError: CUDA error (3): initialization error . 
I run these codes using pytorch 0.2.0, python 3.5 with error


But using cpu( ) woks fine"
541,24583,0,"Migrate `leaky_relu` and `leaky_relu_` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
691,8554,0,"[pytorch][build] ‘vmsLog2’ was not declared in this scope. I tried to rebase and rebuild torch from source today, but got an error:


See [full gist](https://gist.github.com/elanmart/8fca075c596378bd785abd8079567684).

For future reference, getting rid of old  install in  and updating  via  seems to have solved the issue.

cc @soumith @cpuhrsch "
232,30798,0,"How can I implement ""nn.unFold"" on 5D tensor?. ❓ How can I implement ""nn.unFold"" on 5D tensor?

I am implementing an operation on 3D image. I found I need ""nn.unFold"" function in my process. But until now, pytorch does not have official implementation in latest release version. 

I want to implement it in official release code form by myself. But I am a little confused by the relationship  between implementation part of c++ and cuda code. 

Could some one give me some suggestions that which files I should refer to corresponding to implementation of ""nn.unFold"" in 4D version..."
499,2870,0,"torch.max has extra dimensions on a slice with size 1. 
The size reported at the end should be (32, 1), but is (32, 1, 1). Using torch==0.2.0.post3 both on CPU and GPU."
589,2062,0,"Installation error . I am installing pytorch from source using python 2.7. I am getting the following error:

 File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/gen.py"", line 6, in <module>
    import preprocess_declarations
  File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/preprocess_declarations.py"", line 3, in <module>
    from function_wrapper import TYPE_FORMAL_GENERIC
  File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/function_wrapper.py"", line 2, in <module>
    from code_template import CodeTemplate
  File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/code_template.py"", line 13, in <module>
    class CodeTemplate(object):
  File ""/scratch0/Softwares/pytorch_new/torch/lib/ATen/code_template.py"", line 15, in CodeTemplate
    '(^[^\n\S]*)?\$([^\d\W]\w*|\{,?[^\d\W]\w*\,?})', re.MULTILINE)
  File ""/scratch0/Softwares/virtualpython/env_new/lib64/python2.7/re.py"", line 190, in compile
    return _compile(pattern, flags)
  File ""/scratch0/Softwares/virtualpython/env_new/lib64/python2.7/re.py"", line 242, in _compile
    raise error, v # invalid expression
sre_constants.error: nothing to repeat

CMake Error at CMakeLists.txt:126 (message):
  Failed to get generated_cpp list


-- Configuring incomplete, errors occurred!



"
224,5938,0,"RuntimeError: value cannot be converted to type uint8_t without overflow: 10000. I am trying to run the [this code](https://github.com/yunjey/pytorch-tutorial/blob/4b67434961a64ba5e19e63d44f0b9979d2a9aa11/tutorials/01-basics/feedforward_neural_network/main-gpu.py).

The training progress is perfect，but the accuracy did not print out，and some error came out like this：

And the code is：


I have tried it before with python3.5 on win10，it could ran perfectly，but somehow couldn't use the cuda，so i turn to the ubuntu，is it the python version problem？

- OS:ubuntu16.04
- PyTorch version:0.4.0a0+7cbbc0b
- How you installed PyTorch (conda, pip, source):source
- Python version:2.7
- CUDA/cuDNN version:9.0
- GCC version (if compiling from source):gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609

I know that this question shouldn't be asked here cause it is not the pytorch's issus, but i can't access the stackoverflow, it is slow on Chinese internet .So it will be very appericated if someone could help me with this.Thx



"
611,1930,0,Different behavior of sum() in-place on CPU vs GPU
750,9499,0,"Move ConstantPadNd into ATen.  is currently implemented in Python at [torch/nn/_functions/padding.py](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/padding.py). We'd like it to be implemented as a native function in ATen. Note that you will need to update  in  after porting.

Reading material:

1. How to add a native function: https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/
2. Derivatives are added in 

Tips:
  There is no context object in native functions; instead, if something needs to be reused in , 
  + if it is an input argument or output of , you can use it directly in .
  + otherwise, make it an output of the 
"
593,25801,0,"[jit] Missing source highlight error. 



cc @suo"
341,27034,0,"RuntimeError:[enforce fail at context.h:48] option.device_type() ==PROTO_CPU. 1vs0. 
Traceback (most recent call last):
File ""/usr/VMZ-master-1/tools/train_net.py"", line 586, in
main()
File ""/usr/VMZ-master-1/tools/train_net.py"", line 581, in main
Train(args)
File ""/usr/VMZ-master-1/tools/train_net.py"", line 404, in Train
workspace.CreateNet(test_model.net)
File ""/root/pytorch/build/caffe2/python/workspace.py"", line 181, in CreateNet
StringifyProto(net), overwrite,
File ""/root/pytorch/build/caffe2/python/workspace.py"", line 215, in CallWithExceptionIntercept
return func(args, kwargs)
RuntimeError: [enforce fail at context.h:48] option.device_type() == PROTO_CPU. 1 vs 0
frame #0: c10::ThrowEnforceNotMet(char const, int, char const, std::__cxx11::basic_string<char, std::char_traits, std::allocator > const&, void const) + 0x78 (0x7fed19c32178 in /usr/local/lib/libc10.so)
frame #1: + 0x2686d70 (0x7fecdcf03d70 in /usr/local/lib/libtorch.so)
frame #2: + 0x2723fec (0x7fecdcfa0fec in /usr/local/lib/libtorch.so)
frame #3: + 0x3aff5ee (0x7fecde37c5ee in /usr/local/lib/libtorch.so)
frame #4: std::_Function_handler<std::unique_ptr<caffe2::OperatorBase, std::default_deletecaffe2::OperatorBase > (caffe2::OperatorDef const&, caffe2::Workspace*), std::unique_ptr<caffe2::OperatorBase, std::default_deletecaffe2::OperatorBase > ()(caffe2::OperatorDef const&, caffe2::Workspace)>::_M_invoke(std::_Any_data const&, caffe2::OperatorDef const&, caffe2::Workspace*&&) + 0x23 (0x7fed1a4b5433 in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
frame #5: + 0x236c25c (0x7fecdcbe925c in /usr/local/lib/libtorch.so)
frame #6: caffe2::CreateOperator(caffe2::OperatorDef const&, caffe2::Workspace*, int) + 0x328 (0x7fecdcbea528 in /usr/local/lib/libtorch.so)
frame #7: caffe2::dag_utils::prepareOperatorNodes(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x2ad (0x7fecdcbda06d in /usr/local/lib/libtorch.so)
frame #8: caffe2::AsyncNetBase::AsyncNetBase(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x24d (0x7fecdcbb670d in /usr/local/lib/libtorch.so)
frame #9: caffe2::AsyncSchedulingNet::AsyncSchedulingNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x9 (0x7fecdcbbb5b9 in /usr/local/lib/libtorch.so)
frame #10: + 0x23410ae (0x7fecdcbbe0ae in /usr/local/lib/libtorch.so)
frame #11: std::_Function_handler<std::unique_ptr<caffe2::NetBase, std::default_deletecaffe2::NetBase > (std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*), std::unique_ptr<caffe2::NetBase, std::default_deletecaffe2::NetBase > ()(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace)>::_M_invoke(std::_Any_data const&, std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*&&) + 0x23 (0x7fecdcbbdf83 in /usr/local/lib/libtorch.so)
frame #12: caffe2::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x4a5 (0x7fecdcbb0495 in /usr/local/lib/libtorch.so)
frame #13: caffe2::Workspace::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, bool) + 0x103 (0x7fecdcc2fe23 in /usr/local/lib/libtorch.so)
frame #14: caffe2::Workspace::CreateNet(caffe2::NetDef const&, bool) + 0x91 (0x7fecdcc30d61 in /usr/local/lib/libtorch.so)
frame #15: + 0x57906 (0x7fed1a4ad906 in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
frame #16: + 0x57bd2 (0x7fed1a4adbd2 in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
frame #17: + 0x99e3d (0x7fed1a4efe3d in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)

frame #33: __libc_start_main + 0xe7 (0x7fed1e897b97 in /lib/x86_64-linux-gnu/libc.so.6)

"
473,20717,0,"Problem with DataLoader. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce
I have training , validation and test dataset. I have created dataloaders for three of them with training shuffle = True and for valdiation and test shuffle = False . So in one of the code epoch loop contains iterator over train and validation and in another one , epoch loop contains iterator over train , validation and test.

Check the output of the code 1 and 2
1. https://www.kaggle.com/suchith0312/pytorch-dataloader-testing?scriptVersionId=14425205
2. https://www.kaggle.com/suchith0312/pytorch-dataloader-testing?scriptVersionId=14425182

## Expected behavior

The train id's from output of files must be same but they are different. 

## Environment
 - PyTorch Version : 1.0.1.post2
 - Python version: 3.6.6
 - CUDA version : 10.0
- Cudnn version 7.4.2 "
270,28882,0,"Support RRef[T].__call__(*args) which invokes T.__call__(*args) on owner. This is a necessary syntax sugar to for the following use case:



We can implement  by sending a message to the owner,
triggering the owner to run  locally, and returning immediately another RRef of the output.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"
248,13850,0,"num_workers > 0 leading to OOM in non-IPython environment. My code works perfectly in an IPython Jupyter Notebook environment. I'm training a GAN while asynchronous loading data with ImageFolder and DataLoader.  behaves as expected.

When I export my code to .py and execute it from VSCode inside my conda environment (where I started the notebook from) my memory quickly goes up until it reaches . (RAM goes to 100% aswell).

I am using this syntax because I'm on windows: 


Using  and  works but then my epoch time increases about 22x. Everything over 1 leads to the aforementioned OOMs.

Theres shouldn't be any issue because I'm just copying my Ipython code to VSCode and then run the .py file from there, why is this happening?


**System information**

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.2

OS: Microsoft Windows 10 Home
GCC version: Could not collect
CMake version: Could not collect

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 9.2.148
GPU models and configuration: GPU 0: GeForce RTX 2070
Nvidia driver version: 416.81
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] cuda92                    1.0                           0    pytorch
[conda] pytorch                   0.4.1           py37_cuda92_cudnn7he774522_1  [cuda92]  pytorch
[conda] torchvision               0.2.1                     <pip>"
83,17350,1,"torch.nn.CrossEntropyLoss with ""reduction"" sum/mean is not deterministic on segmentation outputs / labels. ## 🐛 Bug
torch.nn.CrossEntropyLoss doesn't output deterministic results on segmentation outputs / labels, when using reduction other than 'none'.
Happens only on GPU. CPU does give a consistent behavior. 

## To Reproduce

## Output


## Expected behavior
I believe the expected behavior of the reduction='sum' and 'mean' should be as consistent as the 'none' option (where I use numpy for reduction).

## Environment

## Additional context
Ran on amazon K80 instance (p2.xlarge).
I know it does seem like a very tiny error but the result of this is that two training sessions of my segmentation network (with identical parameters, initialization, order of image-batches, random seed, etc') doesn't produce identical results. That is problematic when I want to investigate a specific training session.
"
258,28306,0,"Torch ONNX export broken for RandomUniform and RandomUniformLike. ## 🐛 Bug

 reports issues in exporting models with  and .

## To Reproduce

Steps to reproduce the behavior:

Enable Case 2 or Case 4 and run the example below:



Errors reported by the torch.onnx exporter:

Case 2:


Cases 3 & 4:


## Expected behavior

ONNX RandomUniform export must by supported by torch and onnx model must be generated for all cases in the example script.

## Environment


 - PyTorch Version: 3.6
 - OS (e.g., Linux): Ubuntu 18.04.3 LTS
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source): NA
 - Python version: 3.6
 - CUDA/cuDNN version: cuda 10.1 / cudnn 7.6.3
 - GPU models and configuration: 
GPU 0: GeForce GTX 1070 Ti
GPU 1: TITAN V
 - Any other relevant information:
[pip] torch==1.3.0
[pip] torchvision==0.4.1

## Additional context

NA


cc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof"
356,7773,0,"[feature request] Add Local Contrast Normalization . ## Issue description
As mentioned in this paper  :- http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf
I noticed that Local Response Norm is present. This is will be a good addition too.
I have an implementation ready and can create a PR soon, if approved.


cc @albanD @mruberry"
466,4573,0,"Documentation for ""char_rnn_classification_tutorial"" has wrong output cell calculation. @soumith Looks like [this tutorial](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) has a bug in this code:      

    

Looks like the output of the forward pass is done on the input and NOT on the processed data as computed by the RNN cell (ie: self.i2h(combined))   

So, this:    
   
Should become:    

  
If you guys agree, I'm happy to make that minor (but frustrating if you don't know what's going on) change to the docs.   "
349,11389,0,"[distributions] Torch distribution samplers slow on expanded parameters. ## Issue description
We use expanded tensors as distribution parameters in many cases where we dynamically broadcast the parameters at runtime. While working on a related (https://github.com/pytorch/pytorch/pull/11341) PR, I noticed that sampling can be slow when using expanded tensors as distribution parameters. I have narrowed this slowdown to the native torch samplers.

While this is likely expected behavior, it raises the question of:
 (a) whether we should be doing anything inside of distributions to ensure that parameter tensors are contiguous, and if so,
 (b) under what conditions should we ensure contiguity - always by default, or have it be controllable by the user via an optional keyword argument. If we use the same instance to draw multiple samples, it is worth the one time cost of calling  on the distribution parameters (I think, given the relatively low overhead of , we can probably make it the default). 

## Profiling code


cc. @fritzo, @vishwakftw 


cc @fritzo @neerajprad @alicanb @vishwakftw @nikitaved"
583,947,0,"thpp Tensor templatized over Device (and maybe Density, e.g sparse vs dense). Right now, THPP exposes four templatized classes:



I propose to consolidate these into a single class, like so:

We can then do the following:

and so on.

In this fashion, nothing about the current API would change (and so all current code that uses THPP would all work the same), but users of THPP would be able to choose what level of templatization they want. If there are some current methods that are only in some subset of the classes, we can either do template class specialization, or simply make those methods part of all the classes.

I am interested in this because I am using THPP in a separate project, in which it would be cleaner to have the Tensor class to be templatized over at least the Device (in addition to the Type), as it is in most other frameworks (e.g MXNet).

I am happy to do this and submit a PR if the developers are interested, otherwise I will just make those changes locally."
309,13711,0,"In eval(),  BatchNorm running_mean is different between different GPUs. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
After train , when test the model, the BatchNorm is different between different GPUs, it does not do broadcast in eval()?

## To Reproduce

Steps to reproduce the behavior:

1.
1.
1.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
346,6265,0,"[Caffe2] mobile_exporter init_net has code calling information. **The init_net file generated from the mobile_exporter function has code calling information,**
There is fragment information in my generated init_net file:

■146 ""GivenTensorFill*	
shape0*
values-6}■?-
{╓>-$└?R┐  File ""Style_torch.py"", line 186, in <module>
    init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input)
  File ""/home/wguo/lib/temp1/caffe2/build/caffe2/python/predictor/mobile_exporter.py"", line 86, in Export
    add_tensor(init_net, blob_name, blob)
  File ""/home/wguo/lib/temp1/caffe2/build/caffe2/python/predictor/mobile_exporter.py"", line 53, in add_tensor
    utils.MakeArgument(""values"", values),
*  "
387,17397,0,"How does pytorch count batch size in SGD with multiGPU when it does Batch Norm?. ## ❓ Questions and Help
When pytorch does the batch normalization in SGD, what batch size it use? Per GPU batch size, or total batch size over all GPUs?
For example, suppose I set the per-GPU batch size as 32, and I use 8 GPUs. When Pytorch does batch normalization, what batch size it use, 32 or 32 x 8?
My concern comes from a facebook's paper ""Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"". If Pytorch's implementation is the same with what this paper describes, I can modify the parameters of my model accordingly; if not, I need to know. Thanks!
"
627,5917,0,"type object 'Embedding' has no attribute 'from_pretrained'. - OS: Ubuntu 14.04 LTS
- PyTorch version: 0.4.0
- How you installed PyTorch (conda, pip, source): pip
- Python version: 2.7.6
- CUDA/cuDNN version: 8
- GPU models and configuration: Tesla K20
- GCC version (if compiling from source): 4.8.4

"
531,27680,0,"1>E:\software\libtorch\include\torch/csrc/utils/variadic.h(195): error C2951: 模板 声明只能在全局、命名空间或类范围内使用. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.
1.
1.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
128,15620,0,cuda.  cuda runtime error(11):invalid argumen at /pytorch/torch/lib/THC/generic/THCTensorCopy.c:70
685,2670,0,"Adding an attribute as a buffer should throw in error. We had a bug in OpenNMT where we first created an attribute  and then tried to add  as a buffer. This behavior will cause  to exist, but not be cast with . We think this behavior is counter-intuitive, and it should throw an error. 

@jianyuzhan 

"
179,22050,0,"RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR in 1.1.0 . I am currently using Windows 7, Pytorch version 1.1.0, 
It seems from what I have read this error should have been fixed
in 1.1.0?

Here is the environment:

Collecting environment information...
PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 9.0

OS: Microsoft Windows 7 Professional
GCC version: (tdm64-1) 5.1.0

CMake version: version 3.7.0-rc1


Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX 1070
Nvidia driver version: 411.31
cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin\cudnn64_7.dll

Versions of relevant libraries:
[pip3] numpy==1.16.4

[pip3] torch==1.1.0

[pip3] torchsummary==1.5.1

[pip3] torchvision==0.3.0
[conda] Could not collect


Here is the traceback:





I have multiple versions of CUDA installed for compatibility
with Tensorflow, Keras and Pytorch.


Thank you. 



The script I am running is the  script using 


https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html




"
540,30331,0,"[v1.4.0] Release Tracker. The branch v1.4.0 has been cut.

If you need any particular patches onto this branch, please comment below and send a PR to v1.4.0 branch (instead of master).

---

[Current PRs open against the v1.4.0 branch](https://github.com/pytorch/pytorch/pulls?utf8=%E2%9C%93&q=is%3Apr+is%3Aopen+base%3Av1.4.0)
"
355,28472,0,"at::Tensor::data() is deprecated but no other way is suggested for cpp extensions. ## 🐛 Bug

Even the official [docs for cpp extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html) use  but I get deprecation warning.

## To Reproduce

>  warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated [-Wdeprecated-declarations]                                                                                                                               /home/ehazar/miniconda3/envs/py3_night/lib/python3.6/site-packages/torch/include/ATen/core/TensorBody.h:312:1: note: declared here                                                                                                                                                 T * data() const {


## Expected behavior

Provide a non-deprecated way for cpp extensions. Similarly, no alternative to / nor  is provided for cpp extensions.

## Environment

Collecting environment information...
PyTorch version: 1.4.0.dev20191018
Is debug build: No
CUDA used to build PyTorch: 10.0

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 1070
Nvidia driver version: 418.40.04
cuDNN version: /usr/local/lib/libcudnn.so.5.1.10

Versions of relevant libraries:
[pip] numpy==1.17.2
[pip] torch==1.4.0.dev20191018
[pip] torchvision==0.5.0a0+155c504
[conda] Could not collect


cc @yf225"
444,25859,0,"TORCH_WARN must not take out GIL (should buffer warnings). At the moment, we have an attractively named  macro which you can use to report warnings from C++. What if I told you... that using this function could cause your code to take out a lock and all sorts of performance nastiness like that? Well, that's exactly what this macro does, because it calls a custom warning handler that, in the case of our Python extension, calls into Python code to file a Python warning. This is bad bad bad, for both performance reasons, and also because it could cause a deadlock (as to file a Python warning, we must acquire the GIL).

A better strategy is our Python handler must buffer warnings, and then when we (normally) return back to Python and acquire the GIL, actually report the warnings. The correct place to add this logic is probably where we also handle catching C++ exceptions and converting them into Python exceptions."
556,4858,0,"CUDA multinomial with replacement can select zero-probability events. I'm running Ubuntu 16.04.3 on an AWS P3.2xlarge, with the NVIDIA libcuda1-384 package, version 384.111-0ubuntu0.16.04.1, and pytorch version 0.3.0.post4 and cuda90, installed via conda.

In some circumstances, the CUDA multinomial sampler with  can sample an event with zero probability.

The script below reproduces this. I haven't tried to reduce the distribution to a minimal case.  The output is below the script.

I've attached [the RNG state](https://github.com/pytorch/pytorch/files/1665704/failed_state.pt.gz) just prior to the failure. Reproduction from this state is demonstrated in the script.

Bests regards,
Alex

{SAVE_PATH}failed_state.pt"
453,31554,0,"RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory. RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory

Environment: 
1. Pytorch 1.3.1
2. Linux
3. Pycharm

"
717,19807,0,"Create Extension Error: Pytorch 0.4.0. Hi everyone,

I am trying to user FFI extensions in order to add some CUDA compiled functions. The error I encountered is as follows;

cffi.CDefError: cannot parse ""#include <THC/THC.h>""
<cdef source string>:29:1: Directives not supported yet

My environmental configuration is as follows;

Cuda Compilation Tool Version: 9.0, v9.0.176
pycparser version 2.18
PyTorch Version: 0.4.0

Could you please provide assitance about this issue?
Thanks."
538,9167,0,"Redundant code in distributed. It seems like the  is not used at all. Could someone tell me why don't directly  to ?

https://github.com/pytorch/pytorch/blob/4b2b6907929093634c1e452aa2a5a85e4dd9a793/torch/csrc/distributed/Module.cpp#L786"
495,18354,0,Port SpatialReflectionPadding and TemporalReflectionPadding to ATen. Large
304,13636,0,"Conv3d with cudnn error. When I use **torch.backends.cudnn.benchmark = True** in Conv3d, it returned error 
**RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR**

I tired **re -rf ~/.nv** but it doesn't work. The error only disappears when **torch.backends.cudnn.benchmark = False**.

Ubuntu 16.04.4 LTS Tesla P100 Cuda 9.0 cudnn 7.1 
pytorch 1.0.0a0+d03c6ba from source and 0.4.1 from pip both tried."
169,1114,0,"Sequential to support better slicing. 

Would be nice in absence of [remove_module](https://github.com/pytorch/pytorch/issues/358), but probably low-priority."
367,28549,0,"Expose DifferentiableGraphBackward to python. ## 🚀 Feature

Currently, there is no easy way of visualizing the graph generated by autodiff in jit. We want something that could do



## Additional context

In autograd, the  for  of a JIT graph is defined as

 
in 

This class does not have python correspondence, therefore python only views it as a general . We need to create a Python class for it so that we could access additional information.

**Please confirm if this feature sounds useful and if yes, assign me to this issue and I will work on it.**

cc @suo @ezyang @SsnL @albanD @zou3519 @gqchen"
85,3021,1,"CPU scaling issue. Platform: c4.4xlarge EC2 instance (16 vCPUs)
OS: Fedora cloud 26-1.5
Version: PyTorch v0.2.0

Description:
Running 4 workloads on all cores is x8 slower than pinning each workload to 4 different cpus.

How-to reproduce:


The following yields 220 ms/sequence (while not utilizing the CPUs fully):


The following yields 1850 ms/sequence:

And moreover, all the CPUs are 100% utilized...

Motivation:
I wanted to scale training on multi-core systems, a single workload not utilized the cpus fully, so I thought about utilizing the rest of the resources by running multiple workloads."
582,8393,0,"[jit][script] Can't allocate zero gradients for a value without a type. See also https://github.com/pytorch/pytorch/issues/8410

Example:


output:



Operator level tests that fail with this:

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
"
550,3914,0,"Broken ByteTensor indexing of Variables (still works for Tensors). In PyTorch master 0.4 709fcfd, variables cannot be indexed.



This used to work in PyTorch 0.2 f964105:
"
233,1128,0,"Pad a list of tensors. Taking in a list of tensors, can we have a function to pad them? This is a useful feature to process sequences of different lengths and process mini batches. 

I think the documentation of pad_packed_sequence and torch.nn.utils.rnn.pack_padded_sequence are a little confusing http://pytorch.org/docs/nn.html?highlight=pad_packed#torch.nn.utils.rnn.pad_packed_sequence maybe adding some examples could help the explanation. "
330,24600,0,"Migrate `multi_margin_loss` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
288,21015,0,"How to use Infiniband for cpu-cluster with backend gloo?. Now I'm trying to build pytorch from source for my cpu-cluster with backend gloo.
After installing pytorch, I got this information from install summay:

In my cluster, the network interface ""eno1"" represents Ethernet, and ""ib0"" represents Infiniband.
I set the environment variable , and distributed pytorch works fine. But when I set , it will cause some error.

What should I do?
Thanks."
271,30413,0,"nn.functional should maintain API parity with nn where possible. ## 🚀 Feature
 should provide a functional alternative to every stateful component in  that supports it.

Some, like  were deprecated in  because they're now the same as those in .

## Motivation

People usually import modules and submodules that are used often under short intuitive names.

One such name I've often encountered is , in the same vein as importing .

The current plan to move some math functions permanently away from  somewhat awkwardly splits the API.

1. Every operation that has a  has its name in  (to my knowledge).
2. Almost every operation (s) in  that can be exposed cleanly in a functional manner has a name in . But some don't.

## Pitch

Move all 'mathy' functions with class counterparts from  to  and expose aliases to the basic/fundamental ones in .

Here we get into ugly territory; I don't know how exactly I'd argue for some methods currently under , but ,  and co definitely belong in .

## Alternatives

Flip the implementation/re-exporting tale with the impls in  and the re-exports in .

## Additional context

E.g.
https://github.com/pytorch/pytorch/issues/6245 : Deprecate torch.nn.functional.tanh?
"
686,17615,0,"Long tensor to Float tensor slicing assignment fails. ## 🐛 Bug
Assigning a Long tensor to a Float tensor silently fails.

## To Reproduce

## Expected behavior
The tensor should be correctly copied or an error message should be raised.

## Environment

"
105,22120,0,"convert pth to onnx. ## 🐛 Bug


print(torch.__version__)
1.1.0

print(torch.version.cuda)
9.0.176

OS: linux 18.04

 - How you installed PyTorch (, , source): i try both of them, and try to compile from source.
 - Python version: 3.6
 - CUDA/cuDNN version: 7.6.0

Hi, im try to convert .pth weights from yolact project to onnx, im running the following code:



and get the following error:
"
33,12501,1,"C++: Calling Workspace::RunNet for a prediction on a different thread each time causes a GPU memory leak. ## 🐛 Bug
Hi, 
I am trying to obtain a model prediction (on GPU) while running another piece of code in parallel (on CPU). Since I am streaming data, I instantiate a separate std::thread (or std::async) every time to call Workspace::RunNet, this causes a GPU memory leak which is not noticeable unless you are streaming data. However, if the thread used is maintained (a worker thread with the same thread_id), the leak does not occur.
Thank you,
<!-- A clear and concise description of what the bug is. -->

## To Reproduce
Steps to reproduce the behavior:
1. Given a pre-trained model, please use the following sample code to reproduce the issue:
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->


## Expected behavior
Not expecting a memory leak when executing on threads with different thread ids.
<!-- A clear and concise description of what you expected to happen. -->

## Environment

## Additional context

<!-- Add any other context about the problem here. -->
"
731,20794,0,"pytorch1.1.0 windows than one operator "" "" matches these operands. When I compile the c++ extension on windows with pytorch1.1.0. I get the following error.
How to resolve the question? Thanks!
E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(190 than one operator ""<"" matches these operands:
            built-in operator ""arithmetic < arithmetic""
            function ""operator<(const __half &, const __half &)""
            operand types are: c10::Half < c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(191 than one operator ""<="" matches these operands:
            built-in operator ""arithmetic <= arithmetic""
            function ""operator<=(const __half &, const __half &)""
            operand types are: c10::Half <= c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(192 than one operator "">"" matches these operands:
            built-in operator ""arithmetic > arithmetic""
            function ""operator>(const __half &, const __half &)""
            operand types are: c10::Half > c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(193 than one operator "">="" matches these operands:
            built-in operator ""arithmetic >= arithmetic""
            function ""operator>=(const __half &, const __half &)""
            operand types are: c10::Half >= c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(194 than one operator ""=="" matches these operands:
            built-in operator ""arithmetic == arithmetic""
            function ""operator==(const __half &, const __half &)""
            operand types are: c10::Half == c10::Half

E:/Program Files/Python35/lib/site-packages/torch/include\THC/THCNumerics.cuh(196 than one operator ""!="" matches these operands:
            built-in operator ""arithmetic != arithmetic""
            function ""operator!=(const __half &, const __half &)""
            operand types are: c10::Half != c10::Half"
519,371,0,"nn.Embedding CUDA ""too many resources requested for launch"". Looks like nn.Embedding backwards doesn't work when the batch size is >1024. I think it switches sorting algorithms to something that runs out of registers.

Here's a repro:


Running it on K40:


cc @wickedfoo"
429,4872,0,"Memory leak CUDNN 7003. I updated pytorch to 0.3 using conda. I realized that there is a memory leak in RAM in my experiments. Later, I tried to compile from source python with CUDNN 6... and 7005, other versions of CUDNN solved the problem. Is it possible to update CUDNN (from 7003 to 7005) in conda/pip packages? 

"
338,27406,0,"[jit] String frontend doesn't support default arg values. 

cc @suo"
522,23222,0,"Maybe there is a problem when using random.shuffle(). ## 🐛 Bug

When I applied the random.shuffle to a 1-D tensor, the num in the tensor was changed.

## To Reproduce

Steps to reproduce the behavior:
1.Do as follows:
->a = torch.arange(10)
->random.shuffle(a)

## Expected behavior
Expected: 
  tensor([1, 8, 5, 3, 9, 0, 2, 6, 4, 7])
but I got:
  tensor([0, 1, 2, 1, 4, 5, 1, 5, 7, 4])

I am not sure whether it's a bug.

## Environment

Python 3.7.0
torch 1.1.0
"
425,4048,0,"Feature request: ModuleDict, like ModuleList. Self-explanatory. Currently there is no easy way to maintain a dictionary as an attribute of a Module whose values are themselves Modules that need to be registered. Introducing a ModuleDict class that functions the same as a ModuleList, except it exposes a dictionary interface, would resolve this. This has the following practical benefits for users:

1) Checkpointing is much better with a ModuleDict that has strings as keys and Modules as values than a ModuleList. I have personally run into problems when I change the order of modules in my ModuleList and try to load old checkpoints, because the checkpoint key names are order-dependent with ModuleLists, so it breaks my code.

2) It combines the advantages of a ModuleList (dynamic assignment of Modules to a class) with the advantages of using named attributes (documentation / readability / sane module names in checkpoints).

Another pro is that this is very simple to implement. Here is a first pass, if there is support for this feature I will clean it up and submit this as a PR:

"
638,671,0,"Conv3d needs a no-bias option. Right now, VolumetricConvolution in THNN doesn't mark or handle bias=NULL cases"
27,20125,1,"NCCL hang in PyTorch Distributed Data Parallel for Mixed Precision Training. ## 🐛 Bug

On an AWS p3.16xlarge machine, I'm using the mixed precision training from https://github.com/NVIDIA/Megatron-LM checked in at https://github.com/cybertronai/transformer-xl
When I run with [adaptive softmax](https://github.com/cybertronai/transformer-xl/blob/master/mem_transformer.py#L477) with 8 GPUs and [apex mixed precision](https://github.com/cybertronai/transformer-xl/blob/master/fp16_opt.py), NCCL hangs. When I remove , it works as expected (after reducing batch size to avoid OOM). If I remove  it also works fine, so it's definitely a mixed precision issue.

Sorry if this is a dup of #11672 but I haven't been able to figure it out.

## To Reproduce

Steps to reproduce the behavior:
On a p3.16xlarge


In the [logs](https://github.com/NVIDIA/nccl/files/3144052/combined.txt) I found this:

How do I figure out where this is coming from?  Is this something my code should account for or is it a bug in DDP or apex?

## Expected behavior
Doesn't hang silently.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).


## Additional context

"
109,7480,0,"[caffe2] How to use multiple CPUs?. I want to run a caffe2 model on multiple CPUs (evaluation).

Is there a way to do that directly? Maybe it requires OpenBLAS?

I don't think is it possible to use multiprocessing and call RunNet on different workspaces."
195,11907,0,"Please rename package pytorch to stay consistent with entire python world. Hi,
 Like many, I installed pytorch, which says it is installing the package 'pytorch' and yet it is called 'torch' when you attempt to import it from python.

This cuts against the standard practices of the entire python world. Please remedy.

thanks!"
707,10041,0,"[JIT] Support torch.distributions.utils.broadcast_all(). ## Problem

The main blocker to using torch.distributions in the JIT is the  function that is used in every distribution's  method. Currently  calls  under the hood, which results in a JIT error

An expensive workaround is to replace the non-jittable  invocation with a  which is jittable, roughly


## Proposed solution

@soumith and @neerajprad suggested implementing a C++ version of , which would have the added benefit of speeding up non-jitted code."
223,1357,0,"ConvNd C implementation doesn't check tensor types. the error only happens in very specific conditions. (change the view of input tensor and use weight converted from numpy array.)
 
The output of functional conv2d is far off in scale (e12 or so.) "
391,6518,0,"[PyTorch] torch.Tensor and torch.Tensor.dtype have confusing repr. 
I expect to at least see  print  by default.

cc @gchanan "
144,30015,0,"JIT fails for multihead attention. ## 🐛 Bug

I can run the model before torchscript while the torchscripted model fails in running time for the multihead attention @driazati

## To Reproduce

The following code sample:


## Expected behavior



## Environment

bento classyvision env


cc @suo"
374,24645,0,"Migrate `thnn_conv2d_forward` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
59,17738,1,"Inconsistent results of torch.argmax with tensors that have duplicated values. ## 🐛 Bug
Inconsistent results of torch.argmax with tensors that have duplicated values
<!-- A clear and concise description of what the bug is. -->

## To Reproduce



When I did the same on another machine(pytorch 0.4.1, CUDA 9.0)

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior
torch.argmax returns the same value for tensors that have same data in it, regardless of devices(or other factors).
<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: 0.4.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 410.79
cuDNN version: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1

Versions of relevant libraries:
[pip3] msgpack-numpy==0.4.4.1
[pip3] numpy==1.15.0
[pip3] torch==0.4.1.post2
[pip3] torchtext==0.3.1
[pip3] torchvision==0.2.1
[conda] blas                      1.0                         mkl
[conda] mkl                       2018.0.3                      1
[conda] mkl_fft                   1.0.4            py36h4414c95_1
[conda] mkl_random                1.0.1            py36h4414c95_1
[conda] mxnet-mkl                 1.3.0.post0               <pip>
[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch
[conda] torchtext                 0.3.1                     <pip>
[conda] torchvision               0.2.1                    py36_0


## Additional context
It would be helpful if  the documentation describes if torch.argmax returns the last or the first index if a tensor holds duplicated values.
<!-- Add any other context about the problem here. -->
"
445,10004,0,"Detaching gradients instead of using no_grad in spectral_norm. Looking at [this](https://github.com/pytorch/pytorch/blob/3609977d7f4629f75dd1f8d904ddd5b52388124f/torch/nn/utils/spectral_norm.py#L53) code I've noticed that there is an easy way to make ,  differentiable wrt  (it is useful as normalizing constant  depends on ,  and ,  depend on ). 
If we  starting point of power iteration  we avoid cumulating gradients problem while allowing gradients pass through power iteration. As it adds some computational overhead it can be optional.
"
587,15589,0,"unable to load python-trained model with libtorch c++ api in Windows. version: stable 1.0.0
get from the url
https://download.pytorch.org/libtorch/cu90/libtorch-win-shared-with-deps-latest.zip

test code

#include <torch/torch.h>
#include <torch/script.h>

#include<fstream>
#include <iostream>
using namespace std;

int main(int argc, const char* argv[]) {

  at::Tensor tensor = torch::rand({2, 3}).to(at::kCUDA);
  std::cout << tensor << std::endl;


  /**/
  string model_name = ""model.pt"";
  try {
	  
	  std::ifstream in(model_name, std::ios_base::binary);

	  if (in.fail()) {
		  cout << ""failed to open model"" << endl;
	  }
	  else {
		  cout << ""successed to open model"" << endl;
	  }

	  AT_CHECK(!in.fail(), ""load: could not open file "", model_name);

	  cout << ""parsed checking"" << endl;
	   

	  std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(model_name);
	  module->to(at::kCUDA);

	  assert(module != nullptr);
	  std::cout << ""ok\n"";

	  std::vector<torch::jit::IValue> inputs;
	  inputs.push_back(torch::ones({ 1, 3, 224, 224 }).to(at::kCUDA));

	  // Execute the model and turn its output into a tensor.
	  at::Tensor output = module->forward(inputs).toTensor();

	  std::cout << output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) << '\n';
	 
  }
  catch (exception & err) {
	  cout << err.what() << endl;
  }
   
  

  cout << ""finished"" << endl;



}


output:

D:\learn\cv\lab\example-app\build6\Debug>example-app.exe  model.pt
 0.6020  0.1421  0.9155
 0.6821  0.7416  0.7934
[ Variable[CUDAFloatType]{2,3} ]
load: could not open file  (load at ..\torch\csrc\jit\import.cpp:250)
(no backtrace available)
finished

the model is the python trained,with the following code
import torch
import torchvision

# An instance of your model.
model = torchvision.models.resnet18()

# An example input you would normally provide to your model's forward() method.
example = torch.rand(1, 3, 224, 224)

# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.
traced_script_module = torch.jit.trace(model, example)

# save
traced_script_module.save(""model.pt"")


it should be in the right position, as the check code shows."
539,7072,0,"JIT fails on GPU !=0. ## Issue description

Run the test case code below on a system with 2 or more GPUs and you will get this stack trace:

Traceback (most recent call last):
  File ""testnet.py"", line 51, in <module>
    r = net(V)
RuntimeError: arguments are located on different GPUs at /home/tester/pytorch/aten/src/THC/generic/THCTensorMathPairwise.cu:250

## Code example

"
210,6968,0,"[feature request] Implement torch.stack() / _cat() for also for torch.HalfTensor. ## Issue Description
It seems that stacking is currently (PyTorch 0.4) is not supported for tensors of type ; see code example. 

I don't know if this is intentional for some reason (hence I didn't label this issue as a bug), but I couldn't find any issue or documentation pointing to that. Since some methods, like , depend on  (in this case implicitly through the  method), it would be great if either stacking for HalfTensors could get implemented, or the documentation could be updated to contain some kind of hint.

## Code Example


On PyTorch 0.3.1, one gets basically the same error when one tries to do the same thing:
"
603,23589,0,"ValueError: only one element tensors can be converted to Python scalars. So I have tried the script in the issue **ValueError: only one element tensors can be converted to Python scalars #22674** with the newest Pytorch Version, but it seems doesn't work. Do you have any idea about this

The configuration im using:
Pytorch version: 1.1.0
Python version:3.7.3
torchvision: 0.3.0"
626,2656,0,"AttributeError: 'module' object has no attribute 'data' . import os                                                                                                                                                                                                   
import numpy as np
import cPickle
import torch
import torchvision
import gzip
class YT8M(torch.utils.data.Dataset):
  def __init__(self, data_dir):
    self.data_dir = data_dir
    self.file_list = os.listdir(data_dir)

AttributeError: 'module' object has no attribute 'data' 

Why is there no attribute 'data'?
"
640,2404,0,"Advanced Indexing issues. There seem to be issues with corner cases where  or  fails if  is 2 or 1 dimensional resp. In other words, an unnecessary trailing  causes failures, while it doesn't in numpy.

MWE is here: https://gist.github.com/arunmallya/23457765e37faf8932da9276e2b449a8"
562,566,0,"rebuild pip wheels with manylinux. For install instructions please go to http://pytorch.org

This is needed to work across many different linux distros, new and old.

manylinux will build the wheel on a CentOS5 (yes!) Docker machine."
138,31095,0,"pin_memory stuck in DDP/Reducer constructor. ### Problem

@baobablyh  @pietern and I are working on PR #28883. We are trying to create pinned memory tensors in Reducer constructor:


https://github.com/pytorch/pytorch/blob/4902a08e44e1e8dbcced99b9c18321cf9bb644d5/torch/csrc/distributed/c10d/reducer.cpp#L113-L116

However, we observed that, one of the process created the pinned memory tensor successfully, but another process stuck at the line above. More specifically, it stuck when creating the pinned memory storage (stack traces are attached at the end)

https://github.com/pytorch/pytorch/blob/4902a08e44e1e8dbcced99b9c18321cf9bb644d5/aten/src/ATen/native/Memory.cpp#L24

@baobablyh also discovered that, when adding 3 seconds sleep time after the Reducer constructioin, this problem disappear. So, the symptom likes like something is not ready. It might be caused by invoking async copy in one process while another process has an unready the pinned memory tensor (not sure).

@ngimel @mruberry Do you know what could cause this problem? Or is there any way to synchronize until the pinned memory is ready?

### Reproduce

1. fetch #28883
2. comment out the  in 
 https://github.com/pytorch/pytorch/blob/4902a08e44e1e8dbcced99b9c18321cf9bb644d5/torch/nn/parallel/distributed.py#L308-L310
3. run 

### Trace

Process 1



Process 2



### Environment



cc @ezyang @gchanan @zou3519 @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"
408,15544,0,"python api ok but c++ api bad. ## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)

i build pytorch v1 source code in a docker container without gpu, but has cuda.
commit and save to a docker image.
run this docker image on another machine with GPU, occur below:
 pytorch python api is ok:
![image](https://user-images.githubusercontent.com/17922949/50450451-20526e80-0969-11e9-8158-029ae2700ff5.png)
but pytorch c++ api is bad:
![image](https://user-images.githubusercontent.com/17922949/50450462-32cca800-0969-11e9-9701-fea278df8f1f.png)


gcc version : 4.8.4
cuda : 9.0.176
nvidia driver : 384.81
cudnn : 7.0.3

why ? hope to get help!

  


"
122,17484,0,"Parameter not registering if .to(device) is used. ## 🐛 Bug

During instantiation of a custom module, parameters do not register if they are initialized with a .to('cuda') function call on the parameter level. 

## To Reproduce



On the other hand, if the .to('cuda') is called on the wrapped tensor, everything works as expected.




OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.5
Is CUDA available: N/A
CUDA runtime version: 7.5.17
GPU models and configuration: GPU 0: GeForce GTX 1070
Nvidia driver version: 384.130
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.1
[pip] numpy==1.13.3
[pip] torch==1.0.0
[pip] torchvision==0.2.1
[conda] Could not collect

"
65,18786,1,"tensor.cuda() is slow after invoking model(input) and loss.backward(). ## ❓ Questions and Help

### I'm trying to test the costing of time in the main procedures in train-step, in order to accelerate my training. Then I found that  become slow after executing  and 
Here are the codes and the results with only load batch from dataloader and convert it to cuda:

    for i, (b, target) in enumerate(train_loader):
        start = time.time()
        target_cuda = target.cuda()
        input = b.cuda().half()
        now = time.time()
        cost = now - start
        print(cost)
        # output = model(input)
        # loss = criterion(output[0], target_cuda)
        # loss.backward()model(input)loss.backward()torch.cuda.synchronize().cuda()backward().`
- [Discussion Forum](https://discuss.pytorch.org/)
"
81,22961,1,"performance much worse on 2080ti than 1080ti. ## 🐛 Bug

I have a model that I have historically trained on 1080ti, and recently I discovered that the training speed is much worse (almost 2x slower) on 2080ti. The rest of the setup (nvidia driver  + cpu + networking) is the same between the two.

I profiled my script using , and discovered that on the 2080ti, way too much time (~70%) is spent in this function:


Any ideas what the problem could be?
I have attached the two profiles in case they are helpful.
[1080ti.log](https://github.com/pytorch/pytorch/files/3400744/1080ti.log)
[2080ti.log](https://github.com/pytorch/pytorch/files/3400745/2080ti.log)

 - PyTorch Version (e.g., 1.0): 1.1.0
 - OS (e.g., Linux): Ubuntu 16.04
 - How you installed PyTorch (, , source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.6
 - CUDA/cuDNN version: 10.0 / 7.4
 - GPU models and configuration: 1080ti + 2080ti, nvidia driver 410.78
 - Any other relevant information:
"
605,28795,0,"Getting Build Error. I am building PyTorch and I am getting the following error. Would be great to know how to fix it.

Thank you

"
314,10170,0,"build error. ## Issue description
Failed to build the caffe2

## Code example

CMake 3.11.3

## System Info
- Caffe2:

******** Summary ********
General:
  CMake version         : 3.11.3
  CMake command         : C:/Program Files/CMake/bin/cmake.exe
  Git version           : v0.1.11-9628-gf908b2b91-dirty
  System                : Windows
  C++ compiler          : C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe
  C++ compiler version  : 19.0.24215.1
  BLAS                  : Eigen
  CXX flags             : /DWIN32 /D_WINDOWS /W3 /GR /EHsc -DONNX_NAMESPACE=onnx_c2 /MP /bigobj
  Build type            : Release
  Compile definitions   : 
  CMAKE_PREFIX_PATH     : 
  CMAKE_INSTALL_PREFIX  : C:/dev/pytorch/build/install

  BUILD_CAFFE2          : ON
  BUILD_ATEN            : OFF
  BUILD_BINARY          : ON
  BUILD_CUSTOM_PROTOBUF : ON
    Link local protobuf : ON
  BUILD_DOCS            : OFF
  BUILD_PYTHON          : OFF
  BUILD_SHARED_LIBS     : ON
  BUILD_TEST            : OFF
  USE_ASAN              : OFF
  USE_ATEN              : OFF
  USE_CUDA              : ON
    CUDA static link    : OFF
    USE_CUDNN           : ON
    CUDA version        : 8.0
    cuDNN version       : 7.0.5
    CUDA root directory : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0
    CUDA library        : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cuda.lib
    cudart library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cudart_static.lib
    cublas library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cublas.lib;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cublas_device.lib
    cufft library       : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cufft.lib
    curand library      : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/curand.lib
    cuDNN library       : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/cudnn.lib
    nvrtc               : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/lib/x64/nvrtc.lib
    CUDA include path   : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/include
    NVCC executable     : C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/bin/nvcc.exe
    CUDA host compiler  : $(VCInstallDir)bin
    USE_TENSORRT        : OFF
  USE_ROCM              : OFF
  USE_EIGEN_FOR_BLAS    : ON
  USE_FFMPEG            : OFF
  USE_GFLAGS            : ON
  USE_GLOG              : ON
  USE_GLOO              : OFF
  USE_LEVELDB           : OFF
  USE_LITE_PROTO        : OFF
  USE_LMDB              : OFF
  USE_METAL             : OFF
  USE_MKL               : 
  USE_MOBILE_OPENGL     : OFF
  USE_MPI               : OFF
  USE_NCCL              : OFF
  USE_NERVANA_GPU       : OFF
  USE_NNPACK            : OFF
  USE_OBSERVERS         : OFF
  USE_OPENCL            : OFF
  USE_OPENCV            : OFF
  USE_OPENMP            : OFF
  USE_PROF              : OFF
  USE_REDIS             : OFF
  USE_ROCKSDB           : OFF
  USE_ZMQ               : OFF
  Public Dependencies  : Threads::Threads;gflags;glog::glog
  Private Dependencies : cpuinfo;onnxifi_loader
Configuring done
Generating done
"
728,10690,0,non-blocking cuda2cpu copy shouldn't require contiguity
373,14996,0,"as_tensor does not use the device of the default tensor type. ## 🐛 Bug

According to the [doc](https://pytorch.org/docs/stable/torch.html?highlight=torch%20as_tensor#torch.as_tensor) for , the input should be copied to a cuda device if the default tensor type is a cuda tensor. 

> Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.

It does copy to cuda device if the input is a numpy array, but not if the input is a cpu tensor. 

## To Reproduce

Steps to reproduce the behavior:



## Expected behavior

The device should be the 'cuda' device in both cases.

## Environment

PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce GTX 1070
GPU 1: GeForce GTX 1080

Nvidia driver version: 390.77
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] Could not collect
[conda] blas                      1.0                         mkl
[conda] mkl                       2018.0.3                      1
[conda] mkl_fft                   1.0.6            py37h7dd41cf_0
[conda] mkl_random                1.0.1            py37h4414c95_1
[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch
[conda] torchvision               0.2.1                      py_2    pytorch



cc @ngimel @jlin27 @mruberry"
158,18671,0,"[jit] crash in unpickler code. See this test result: https://fburl.com/testinfra/6yuycydh

"
300,5157,0,"BCELoss - weight parameter shape incorrect. The  parameter of  seems to be incorrectly defined when using a multi-dimensional input and target. Related [forum thread](https://discuss.pytorch.org/t/binary-cross-entropy-weights/13299).

The documentation defines  as:
> If given, has to be a Tensor of size “nbatch”.

However, this example throws an error:


A workaround is to  the  tensor to match the number of dimensions:


Internally,  is called and fails in the first code snippet.
The second code snippet applied a weighting for each batch element, which is fine.

Should we automatically unsqueeze the weight tensor, if input and target are multi-dimensional?

Also, how should we handle class weighting?
If we just pass 2 weights as the  tensor, the code successfully runs, but does not apply class weighting, which might mislead some users:


A workaround would be:


What is wanted behavior of the  parameter in : class or batch weighting?
Both cases have some issues at the moment in my opinion.
I would like to fix this issue, but I would like to hear some opinions on the right behavior.
Class weighting would be consistent with other loss functions like , but maybe batch weighting is a more common use case for .

PyTorch version:  (installed from source)

"
369,20323,0,"Support size to `torch.normal`. This would be more consistent with numpy.



cc @mruberry @rgommers @heitorschueroff"
680,15166,0,"BUG:torch.nn.functional.multi_margin_loss. pytorch1.0
BUG:torch.nn.functional.multi_margin_loss
example:
import torch 
import torch.nn.functional as F
a=[[1.,2.,3.,4.]]
b=[2]
input=torch.tensor(a,requires_grad=True)
target = torch.tensor(b)
k = torch.tensor(c)
output = F.multi_margin_loss(input,target,p=1.,margin=1.,reduce=False,size_average=False)
print(output)
--------------------------
tensor([0.5000], grad_fn=<MultiMarginLossBackward>)
"
694,3238,0,"Dimension issue with `softmax` and related functions for 1D tensors. There seems to be an erroneous dimension calculation for any function that uses the  private function. If the input is a 1D tensor, the implicit dimension computed is 1, which is a problem since  is invalid for a 1D tensor.

Minimal reproducible example:

This produces the error:

I'm running pytorch installed from source, from  (off of commit dc6510f7)"
29,5801,1,"KLDivLoss behaves differently on CPU/GPU. Pytorch Version: '0.3.0'

Code:


Without cuda, both the gradients and the loss are the same:


With cuda the gradients returned by KLDivLoss are scaled down by the number of class, while the loss is the same. 
"
501,7525,0,"tqdm install is failing with ValueError: bad marshal data (unknown type code). Sample failing CI build: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-trusty-pynightly-test/6167//console

I did some investigation, and the error only happens if you use  directly (e.g., if you use , and it goes away if you use a newer Python 3.7 nightly (e.g., from deadsnakes). So it might be a bug in Python 3.7 (slash setuptools) which was subsequently fixed.

Unfortunately, preinstall tqdm isn't sufficient to avoid the problem. But  on torchvision might be enough. I am giving it a try."
663,26834,0,"FakeQuantize params not rendering correctly. the parameters for the FakeQuantize function aren't showing up right. 
<img width=""897"" alt=""Screen Shot 2019-09-25 at 1 12 06 PM"" src=""https://user-images.githubusercontent.com/45861273/65636145-99353400-df96-11e9-9e0f-4c6b79b7ac91.png"">
"
41,18175,1,"[Caffe2] Does Caffe2 support distributed training with only CPU?. I found this sentence on https://caffe2.ai/docs/distributed-training.html:
""We’re assuming that you’ve successfully built Caffe2 and that you have a system with at least one GPU, but preferably more to test out the distributed features.""

Since I have to do some experiments about CPU performance, I wonder if I can use distributed training with CPU only cluster?"
368,6171,0,"Compute csr representation on torch.sparse.Tensor.coalesce for faster sparse matrix multiplication . On the code on sparse matrix-dense matrix multiplication
https://github.com/pytorch/pytorch/blob/master/aten/src/THS/generic/THSTensorMath.c#L305
and 
https://github.com/pytorch/pytorch/blob/master/aten/src/THCS/generic/THCSTensorMath.cu#L58 , 
it first computes csr representation, then perform sparse matrix-dense matrix multiplication based on that representation, then destroy the csr representation. 
If we repeatedly perform matrix multiplication on the same sparse matrix, computing csr representation is redundant. This could be resolved by computing csr representation only once when coalescing  and save it as a member of  or  ( https://github.com/pytorch/pytorch/blob/master/aten/src/THS/generic/THSTensor.cpp#L428 and
https://github.com/pytorch/pytorch/blob/master/aten/src/THCS/generic/THCSTensor.cu#L38 ). This would enhance speed of sparse matrix-dense matrix multiplication.

Will this be a good modification? Shall I try to make a pull request on this one?



cc @vincentqb @aocsa @nikitaved @pearu @mruberry"
649,7989,0,"[documentation request] 'clone' needs better documentation. [The current documentation for  is](https://pytorch.org/docs/stable/tensors.html?highlight=clone#torch.Tensor.clone):

> Returns a copy of the  tensor. The copy has the same size and data type as .

This documentation fails to elucidate the fact that any gradient propagating through the cloned tensor will propagate to the original tensor. This is critical to the functionality of clone (and is why the method isn't called ""copy""), and is just begging for newcomers to make hard-to-find mistakes. "
665,22549,0,"How to programmatically check PyTorch version. ## 📚 Documentation

[Minor minor detail]
In the reproducibility section of the docs or in the FAQ, I would add a simple subsection/snippet of code to show how to programmatically check the running version of PyTorch. 
This can also encourage users to take into account heterogeneity of PyTorch versions in their code.

By the way, a simple regex on  is enough (this assuming version numbering will not change).

"
442,2490,0,"libquadmath is repeated in TH/CMakeLists.txt. in file 'torch/lib/TH/CMakeLists.txt', line 473 ~ 477,



libquadmath is repeated.
"
295,12646,0,"Caffe2 Installation inside Pytorch. i am getting this error message (please see the attached) after run the command ""python setup.py install"":
![pytorch_question](https://user-images.githubusercontent.com/8035201/46946663-b2551580-d0a2-11e8-9f9e-afd34fd2454d.png). 
I followed this office guide of pytorch: https://caffe2.ai/docs/getting-started.html?platform=ubuntu&configuration=compile.
But the weird thing is I can print the coffee2 sucessfully
![coffe2](https://user-images.githubusercontent.com/8035201/46946767-03fda000-d0a3-11e8-8de5-4ccbba7c3d36.png)

i would like to ask if does anybody knows how to fix this?

"
261,14685,0,"[Caffe2] Exception when creating gradient for [Cast] SquaredL2Distance as output layer of CNN network. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

I am getting the following error when I use the SquaredL2Distance operator as the output layer of my CNN network **(1st attempt)**:



28train_net

Then I tried to fix the error converting  to float (even though it is already float32, see below) using the Cast operator **(2nd attempt)**

However, I got the following error:



Furthermore, I created the LMDB training dataset which stores the image data as uint8 and the label as a multivalue of float64:
**key: 00000001
image_data: shape: (210, 280, 3) type: uint8
indicators:    shape: (14,)              type: float64**

**How can I fix the error?**

## To Reproduce

Steps to reproduce the behavior:

1. **Project consists in two files: a trainer and a creator. Run the trainer which calls the train function of the creator.**

Trainer.py:


CNNCreator_dpnet_dpnet.py:



<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Execute SquaredL2Distance as the output layer of the CNN network

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): **Caffer2 tag v0.4.0**
 - OS (e.g., Linux): **Ubuntu 16.04**
 - How you installed PyTorch (conda, pip, source): **Build from source (tag v0.4.0)**
 - Build command you used (if compiling from source):
 - Python version: **Python 2.7**
 - CUDA/cuDNN version: **8.0/7.0.5**
 - GPU models and configuration: **GTX 1050**
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
159,15567,0,"Improve error messages for pack_padded_sequence(..., enforced_sorted=True). Right now it errors out saying that sequences must be sorted. Users may not be aware that they can pass in  to the function to avoid the sorting requirement if they do not intend to do ONNX export; it would be great if the error message suggested that instead."
266,11532,0,"[JIT][tracer] Slicing shape is specialized to tensor rank. Example:






these  calls we emit (e.g. ) are specialized to the rank of the tensor we called  on"
565,4119,0,"CI with DEBUG=1. Turning on DEBUG=1 turns off inlining. However, if you declare a function as , it won't get object code generated for it, which means there will be undefined symbol errors. You might not notice this with a release build, since everything gets inlined away.

We can catch errors like this by doing CI with DEBUG=1. Probably just pick one platform and run that only."
116,7764,0,"Incorrect compilation instructions for Mac. The instructions at https://github.com/pytorch/pytorch#from-source for compiling from source say to use the following command on macOS:



Why I try to compile with that command, it fails with the following error:



I was able to get it to compile by using the following command instead:



## System Info

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): Source
- Build command you used (if compiling from source): See above
- OS: macOS 10.13.4
- PyTorch version: Latest code from repository
- Python version: 3.6.1
"
457,8039,0,"RuntimeError: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70. ## Issue description



## Code example



## System Info



 does not detect  even though it's installed:



"
201,1626,0," Modules/gcmodule.c:380: visit_decref: Assertion `((gc)->gc.gc_refs >> (1)) != 0' failed.. Continued from: https://github.com/pytorch/pytorch/issues/1624 (running tests on a debug build of python3.6.1.

The following tests in TestNN failed with gc problems:

test_variable_sequence_cuda
test_cuda_rnn_fused
test_rnn_initial_hidden_state
test_RNN_cpu_vs_cudnn_no_dropout
test_RNN_cpu_vs_cudnn_with_dropout
test_RNN_dropout_state
test_RNN_change_dropout

Error is:
((gc)->gc.gc_refs >> (1)) != 0' failed.

#0  0x00007ffff712c1d7 in raise () at /lib64/libc.so.6
#1  0x00007ffff712d8c8 in abort () at /lib64/libc.so.6
#2  0x00007ffff7125146 in __assert_fail_base () at /lib64/libc.so.6
#3  0x00007ffff71251f2 in  () at /lib64/libc.so.6
#4  0x0000000000435f63 in visit_decref (op=0x7fffc006e4f8, data=<optimized out>) at Modules/gcmodule.c:380
#5  0x00007fffecd7c20d in THPVariable_traverse(THPVariable*, visitproc, void*) (self=self@entry=0x7fffc00781f0, visit=visit@entry=0x435efd <visit_decref>, arg=arg@entry=0x0)
    at torch/csrc/autograd/python_variable.cpp:92
#6  0x00000000004a86ce in subtype_traverse (self=0x7fffc00781f0, visit=0x435efd <visit_decref>, arg=0x0) at Objects/typeobject.c:1021
#7  0x00000000004356ee in subtract_refs (containers=containers@entry=0x885d20 <generations>) at Modules/gcmodule.c:399
#8  0x0000000000436809 in collect (generation=generation@entry=0, n_collected=n_collected@entry=0x7fffffff96b8, n_uncollectable=n_uncollectable@entry=0x7fffffff96b0, nofail=nofail@entry=0)
    at Modules/gcmodule.c:956
#9  0x0000000000436c47 in collect_with_callback (generation=0) at Modules/gcmodule.c:1128
#10 0x0000000000436d3f in collect_generations () at Modules/gcmodule.c:1151
#11 0x0000000000436eac in _PyObject_GC_Alloc (use_calloc=use_calloc@entry=0, basicsize=basicsize@entry=72) at Modules/gcmodule.c:1726
#12 0x0000000000437382 in _PyObject_GC_Malloc (basicsize=basicsize@entry=72) at Modules/gcmodule.c:1736
#13 0x00000000004a8443 in PyType_GenericAlloc (type=0x7fffed9eb400 <THPSizeType>, nitems=2) at Objects/typeobject.c:936
#14 0x00007fffecd3083f in THPSize_New(int, long*) (dim=2, sizes=0x4547f5c0) at torch/csrc/Size.cpp:16
#15 0x00007fffed1c2102 in THCPDoubleTensor_size(_object*, _object*, _object*) (self=<optimized out>, args=0x7ffff7fa0058, kwargs=0x0)
    at /data/users/gchanan/pytorch6/torch/csrc/generic/TensorMethods.cpp:650
#16 0x00000000004950a9 in _PyCFunction_FastCallDict (func_obj=func_obj@entry=0x7fffc28df328, args=args@entry=0x4547e598, nargs=nargs@entry=0, kwargs=kwargs@entry=0x0) at Objects/methodobject.c:231
#17 0x000000000049545a in _PyCFunction_FastCallKeywords (func=func@entry=0x7fffc28df328, stack=stack@entry=0x4547e598, nargs=nargs@entry=0, kwnames=kwnames@entry=0x0) at Objects/methodobject.c:295
#18 0x0000000000530f41 in call_function (pp_stack=pp_stack@entry=0x7fffffff9908, oparg=oparg@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4798
#19 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#20 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x4547e3e8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#21 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=0x7fffc2406e80, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x7fffc008c480, argcount=4, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=defs@entry=0x0, defcount=defcount@entry=0, kwdefs=kwdefs@entry=0x0, closure=closure@entry=0x0, name=name@entry=0x0, qualname=qualname@entry=0x0) at Python/ceval.c:4128
#22 0x0000000000531168 in PyEval_EvalCodeEx (_co=<optimized out>, globals=<optimized out>, locals=locals@entry=0x0, args=args@entry=0x7fffc008c480, argcount=<optimized out>, kws=kws@entry=0x0, kwcount=kwcount@entry=0, defs=defs@entry=0x0, defcount=defcount@entry=0, kwdefs=0x0, closure=0x0) at Python/ceval.c:4149
#23 0x000000000047386f in function_call (func=0x7fffc23a6840, arg=0x7fffc008c458, kw=0x0) at Objects/funcobject.c:604
#24 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7fffc23a6840, args=args@entry=0x7fffc008c458, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#25 0x00000000005316b0 in PyEval_CallObjectWithKeywords (func=0x7fffc23a6840, args=0x7fffc008c458, kwargs=kwargs@entry=0x0) at Python/ceval.c:4718
#26 0x0000000000444c69 in PyObject_CallObject (o=<optimized out>, a=<optimized out>) at Objects/abstract.c:2172
#27 0x00007fffecd74eb3 in THPFunction_apply(_object*, _object*) (cls=0x11c9178, _inputs=0x7fffc1da3418) at torch/csrc/autograd/python_function.cpp:722
#28 0x00000000004950b8 in _PyCFunction_FastCallDict (func_obj=func_obj@entry=0x7fffc28d61c0, args=args@entry=0x4547bf70, nargs=nargs@entry=3, kwargs=kwargs@entry=0x0) at Objects/methodobject.c:234
#29 0x000000000049545a in _PyCFunction_FastCallKeywords (func=func@entry=0x7fffc28d61c0, stack=stack@entry=0x4547bf70, nargs=nargs@entry=3, kwnames=kwnames@entry=0x0) at Objects/methodobject.c:295
#30 0x0000000000530f41 in call_function (pp_stack=pp_stack@entry=0x7fffffff9e58, oparg=oparg@entry=3, kwnames=kwnames@entry=0x0) at Python/ceval.c:4798
#31 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#32 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x4547bdc8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#33 0x00000000005301ec in _PyFunction_FastCall (co=co@entry=0x7fffc31e2640, args=0x7fffffffa038, args@entry=0x7fffffffa020, nargs=nargs@entry=3, globals=globals@entry=0x7fffc31e1e68)
    at Python/ceval.c:4880
---Type <return> to continue, or q <return> to quit---
#34 0x000000000053c110 in _PyFunction_FastCallDict (func=func@entry=0x7fffc23f61c8, args=args@entry=0x7fffffffa020, nargs=nargs@entry=3, kwargs=kwargs@entry=0x0) at Python/ceval.c:4982
#35 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffc23f61c8, args=args@entry=0x7fffffffa020, nargs=nargs@entry=3, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2295
#36 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffc23f61c8, obj=0x7fffc0085e28, args=0x7fffc22eb988, kwargs=0x0) at Objects/abstract.c:2358
#37 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#38 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7fffc23963d8, args=args@entry=0x7fffc22eb988, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#39 0x00000000004af68e in call_method (o=<optimized out>, nameid=nameid@entry=0x8a5740 <PyId___setitem__>, format=format@entry=0x635a54 ""(OO)"") at Objects/typeobject.c:1453
#40 0x00000000004af814 in slot_mp_ass_subscript (self=<optimized out>, key=<optimized out>, value=<optimized out>) at Objects/typeobject.c:5944
#41 0x00000000004444f0 in PyObject_SetItem (o=o@entry=0x7fffc0085e28, key=key@entry=0x7fffc22eb838, value=value@entry=0x7fffc0089b80) at Objects/abstract.c:180
#42 0x00000000005345cc in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:1727
#43 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x4547ed58, throwflag=throwflag@entry=0) at Python/ceval.c:718
#44 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=0x7fffc1f6b700, globals=<optimized out>, locals=locals@entry=0x0, args=<optimized out>, argcount=1, kwnames=0x0, kwargs=0x307b1470, kwcount=0, kwstep=kwstep@entry=1, defs=0x7fffc1f69d18, defcount=defcount@entry=1, kwdefs=kwdefs@entry=0x0, closure=closure@entry=0x0, name=name@entry=0x7fffefef55e0, qualname=qualname@entry=0x7fffefef55e0)
    at Python/ceval.c:4128
#45 0x0000000000530da7 in fast_function (func=func@entry=0x7fffc1f8a110, stack=<optimized out>, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4939
#46 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffa4e8, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#47 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#48 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307b1208, throwflag=throwflag@entry=0) at Python/ceval.c:718
#49 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0x7fffc231ff18, nargs=nargs@entry=2, globals=<optimized out>) at Python/ceval.c:4880
#50 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffc1dc39b0, stack=<optimized out>, nargs=nargs@entry=2, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#51 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffa6e8, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#52 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#53 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x7fffc231fd78, throwflag=throwflag@entry=0) at Python/ceval.c:718
#54 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0x307b0898, nargs=nargs@entry=1, globals=<optimized out>) at Python/ceval.c:4880
#55 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffc1dc3b20, stack=<optimized out>, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#56 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffa8e8, oparg=oparg@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#57 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#58 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307b0698, throwflag=throwflag@entry=0) at Python/ceval.c:718
#59 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede08340, globals=globals@entry=0x7fffede77670, locals=locals@entry=0x0, args=args@entry=0x7fffffffab20, argcount=argcount@entry=2, kwnames=kwnames@entry=0x7ffff7fa0080, kwargs=kwargs@entry=0x7ffff7fa0088, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x7fffede0ff88, defcount=1, kwdefs=0x0, closure=0x0, name=0x7fffede06860, qualname=0x7fffede054a0) at Python/ceval.c:4128
#60 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd4f280, args=args@entry=0x7fffffffab20, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e09b8) at Python/ceval.c:5031
#61 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd4f280, args=args@entry=0x7fffffffab20, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e09b8) at Objects/abstract.c:2295
#62 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd4f280, obj=0x7fffc00d0e28, args=0x7fffc00e1ae8, kwargs=0x7fffc00e09b8) at Objects/abstract.c:2358
#63 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#64 0x0000000000444e8c in PyObject_Call (func=0x7fffeff04838, args=0x7fffc00e1ae8, kwargs=0x7fffc00e09b8) at Objects/abstract.c:2246
#65 0x000000000052f6b9 in do_call_core (func=func@entry=0x7fffeff04838, callargs=callargs@entry=0x7fffc00e1ae8, kwdict=kwdict@entry=0x7fffc00e09b8) at Python/ceval.c:5067
#66 0x000000000053aad1 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3366
#67 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x7fffc0064250, throwflag=throwflag@entry=0) at Python/ceval.c:718
#68 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede084c0, globals=globals@entry=0x7fffede77670, locals=locals@entry=0x0, args=args@entry=0x7fffffffaec0, argcount=argcount@entry=2, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x7ffff7fa3270, qualname=0x7fffede04ce8)
    at Python/ceval.c:4128
---Type <return> to continue, or q <return> to quit---
#69 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd4f3f0, args=args@entry=0x7fffffffaec0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Python/ceval.c:5031
#70 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd4f3f0, args=args@entry=0x7fffffffaec0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2295
#71 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd4f3f0, obj=0x7fffc00d0e28, args=0x7fffc00e1b50, kwargs=0x0) at Objects/abstract.c:2358
#72 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#73 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7fffede3c6e8, args=args@entry=0x7fffc00e1b50, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#74 0x00000000004b1e1d in slot_tp_call (self=<optimized out>, args=0x7fffc00e1b50, kwds=0x0) at Objects/typeobject.c:6167
#75 0x00000000004450b1 in _PyObject_FastCallDict (func=func@entry=0x7fffc00d0e28, args=args@entry=0x307af060, nargs=nargs@entry=1, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2316
#76 0x00000000004456e5 in _PyObject_FastCallKeywords (func=func@entry=0x7fffc00d0e28, stack=0x307af060, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Objects/abstract.c:2480
#77 0x0000000000531011 in call_function (pp_stack=pp_stack@entry=0x7fffffffb0c8, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4822
#78 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#79 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307aee98, throwflag=throwflag@entry=0) at Python/ceval.c:718
#80 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede1e880, globals=globals@entry=0x7fffedeaad78, locals=locals@entry=0x0, args=args@entry=0x7fffffffb300, argcount=argcount@entry=2, kwnames=kwnames@entry=0x7ffff7fa0080, kwargs=kwargs@entry=0x7ffff7fa0088, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x7fffedd4d5c8, defcount=1, kwdefs=0x0, closure=0x0, name=0x7fffede06860, qualname=0x7fffedd50970) at Python/ceval.c:4128
#81 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd54eb8, args=args@entry=0x7fffffffb300, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e0a30) at Python/ceval.c:5031
#82 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd54eb8, args=args@entry=0x7fffffffb300, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e0a30) at Objects/abstract.c:2295
#83 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd54eb8, obj=0x7fffc00e1400, args=0x7fffc00e1cf0, kwargs=0x7fffc00e0a30) at Objects/abstract.c:2358
#84 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#85 0x0000000000444e8c in PyObject_Call (func=0x7ffff064cf38, args=0x7fffc00e1cf0, kwargs=0x7fffc00e0a30) at Objects/abstract.c:2246
#86 0x000000000052f6b9 in do_call_core (func=func@entry=0x7ffff064cf38, callargs=callargs@entry=0x7fffc00e1cf0, kwdict=kwdict@entry=0x7fffc00e0a30) at Python/ceval.c:5067
#87 0x000000000053aad1 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3366
#88 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x7fffc00d9e20, throwflag=throwflag@entry=0) at Python/ceval.c:718
#89 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede14a00, globals=globals@entry=0x7fffedeaad78, locals=locals@entry=0x0, args=args@entry=0x7fffffffb6a0, argcount=argcount@entry=2, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x7ffff7fa3270, qualname=0x7fffedd55838)
    at Python/ceval.c:4128
#90 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd54d48, args=args@entry=0x7fffffffb6a0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Python/ceval.c:5031
#91 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd54d48, args=args@entry=0x7fffffffb6a0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2295
#92 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd54d48, obj=0x7fffc00e1400, args=0x7fffc00d0e90, kwargs=0x0) at Objects/abstract.c:2358
#93 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#94 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7fffedee0288, args=args@entry=0x7fffc00d0e90, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#95 0x00000000004b1e1d in slot_tp_call (self=<optimized out>, args=0x7fffc00d0e90, kwds=0x0) at Objects/typeobject.c:6167
#96 0x00000000004450b1 in _PyObject_FastCallDict (func=func@entry=0x7fffc00e1400, args=args@entry=0x307ae830, nargs=nargs@entry=1, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2316
#97 0x00000000004456e5 in _PyObject_FastCallKeywords (func=func@entry=0x7fffc00e1400, stack=0x307ae830, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Objects/abstract.c:2480
#98 0x0000000000531011 in call_function (pp_stack=pp_stack@entry=0x7fffffffb8a8, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4822
#99 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#100 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307ae668, throwflag=throwflag@entry=0) at Python/ceval.c:718
#101 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede1e880, globals=globals@entry=0x7fffedeaad78, locals=locals@entry=0x0, args=args@entry=0x7fffffffbae0, argcount=argcount@entry=2, kwnames=kwnames@entry=0x7ffff7fa0080, kwargs=kwargs@entry=0x7ffff7fa0088, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x7fffedd4d5c8, defcount=1, kwdefs=0x0, closure=0x0, name=0x7fffede06860, qualname=0x7fffedd50970) at Python/ceval.c:4128
#102 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd54eb8, args=args@entry=0x7fffffffbae0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e0058) at Python/ceval.c:5031
#103 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd54eb8, args=args@entry=0x7fffffffbae0, nargs=nargs@entry=2, kwargs=kwargs@entry=0x7fffc00e0058) at Objects/abstract.c:2295
---Type <return> to continue, or q <return> to quit---
#104 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd54eb8, obj=0x7fffc00d0dc0, args=0x7fffc00d0ae8, kwargs=0x7fffc00e0058) at Objects/abstract.c:2358
#105 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#106 0x0000000000444e8c in PyObject_Call (func=0x7fffede3c7c8, args=0x7fffc00d0ae8, kwargs=0x7fffc00e0058) at Objects/abstract.c:2246
#107 0x000000000052f6b9 in do_call_core (func=func@entry=0x7fffede3c7c8, callargs=callargs@entry=0x7fffc00d0ae8, kwdict=kwdict@entry=0x7fffc00e0058) at Python/ceval.c:5067
#108 0x000000000053aad1 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3366
#109 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x7fffc00d9c28, throwflag=throwflag@entry=0) at Python/ceval.c:718
#110 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffede14a00, globals=globals@entry=0x7fffedeaad78, locals=locals@entry=0x0, args=args@entry=0x7fffffffbe80, argcount=argcount@entry=2, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x7ffff7fa3270, qualname=0x7fffedd55838)
    at Python/ceval.c:4128
#111 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedd54d48, args=args@entry=0x7fffffffbe80, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Python/ceval.c:5031
#112 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedd54d48, args=args@entry=0x7fffffffbe80, nargs=nargs@entry=2, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2295
#113 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedd54d48, obj=0x7fffc00d0dc0, args=0x7fffc00d0bb8, kwargs=0x0) at Objects/abstract.c:2358
#114 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#115 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7ffff0655e58, args=args@entry=0x7fffc00d0bb8, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2246
#116 0x00000000004b1e1d in slot_tp_call (self=<optimized out>, args=0x7fffc00d0bb8, kwds=0x0) at Objects/typeobject.c:6167
#117 0x00000000004450b1 in _PyObject_FastCallDict (func=func@entry=0x7fffc00d0dc0, args=args@entry=0x307ad4b0, nargs=nargs@entry=1, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2316
#118 0x00000000004456e5 in _PyObject_FastCallKeywords (func=func@entry=0x7fffc00d0dc0, stack=0x307ad4b0, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Objects/abstract.c:2480
#119 0x0000000000531011 in call_function (pp_stack=pp_stack@entry=0x7fffffffc088, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4822
#120 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#121 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307ad298, throwflag=throwflag@entry=0) at Python/ceval.c:718
#122 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0x307acf50, nargs=nargs@entry=2, globals=<optimized out>) at Python/ceval.c:4880
#123 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffedca1110, stack=<optimized out>, nargs=nargs@entry=2, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#124 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffc288, oparg=oparg@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#125 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#126 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307acda8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#127 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0x307a77b8, nargs=nargs@entry=1, globals=<optimized out>) at Python/ceval.c:4880
#128 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffedca1a68, stack=<optimized out>, nargs=nargs@entry=1, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#129 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffc488, oparg=oparg@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#130 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#131 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x307a75b8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#132 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7fffedceb400, globals=globals@entry=0x7fffedd4c9b8, locals=locals@entry=0x0, args=args@entry=0x7fffffffc6c0, argcount=argcount@entry=1, kwnames=kwnames@entry=0x7fffc22e5e10, kwargs=kwargs@entry=0x7fffc22e5e18, kwcount=2,
    kwcount@entry=1, kwstep=kwstep@entry=2, defs=0x7fffedca12a8, defcount=11, kwdefs=0x7fffedcefd78, closure=0x0, name=0x7ffff7fa3430, qualname=0x7fffedcef478) at Python/ceval.c:4128
#133 0x000000000053c29d in _PyFunction_FastCallDict (func=func@entry=0x7fffedca1338, args=args@entry=0x7fffffffc6c0, nargs=nargs@entry=1, kwargs=kwargs@entry=0x7fffc00d5238) at Python/ceval.c:5031
#134 0x0000000000445042 in _PyObject_FastCallDict (func=func@entry=0x7fffedca1338, args=args@entry=0x7fffffffc6c0, nargs=nargs@entry=1, kwargs=kwargs@entry=0x7fffc00d5238) at Objects/abstract.c:2295
#135 0x0000000000445345 in _PyObject_Call_Prepend (func=0x7fffedca1338, obj=0x7fffc00d09b0, args=0x7ffff7fa0058, kwargs=0x7fffc00d5238) at Objects/abstract.c:2358
#136 0x000000000045ee6b in method_call (method=<optimized out>, args=<optimized out>, kwargs=<optimized out>) at Objects/classobject.c:317
#137 0x0000000000444e8c in PyObject_Call (func=func@entry=0x7ffff064ce58, args=args@entry=0x7ffff7fa0058, kwargs=kwargs@entry=0x7fffc00d5238) at Objects/abstract.c:2246
#138 0x00000000004b1a5c in slot_tp_init (self=<optimized out>, args=0x7ffff7fa0058, kwds=0x7fffc00d5238) at Objects/typeobject.c:6380
#139 0x00000000004ada4d in type_call (type=0xb5e1c8, args=0x7ffff7fa0058, kwds=0x7fffc00d5238) at Objects/typeobject.c:915
#140 0x00000000004450b1 in _PyObject_FastCallDict (func=func@entry=0xb5e1c8, args=args@entry=0x3079d738, nargs=nargs@entry=0, kwargs=kwargs@entry=0x7fffc00d5238) at Objects/abstract.c:2316
---Type <return> to continue, or q <return> to quit---
#141 0x00000000004456e5 in _PyObject_FastCallKeywords (func=func@entry=0xb5e1c8, stack=0x3079d738, nargs=nargs@entry=0, kwnames=kwnames@entry=0x7fffc1f43260) at Objects/abstract.c:2480
#142 0x0000000000531011 in call_function (pp_stack=pp_stack@entry=0x7fffffffc900, oparg=<optimized out>, kwnames=kwnames@entry=0x7fffc1f43260) at Python/ceval.c:4822
#143 0x000000000053a6d4 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3300
#144 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0x3079d588, throwflag=throwflag@entry=0) at Python/ceval.c:718
#145 0x00000000005301ec in _PyFunction_FastCall (co=<optimized out>, args=0xa1a588, nargs=nargs@entry=0, globals=<optimized out>) at Python/ceval.c:4880
#146 0x0000000000530cf7 in fast_function (func=func@entry=0x7fffc1d9e3f0, stack=<optimized out>, nargs=nargs@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4915
#147 0x0000000000530ffe in call_function (pp_stack=pp_stack@entry=0x7fffffffcaf8, oparg=oparg@entry=0, kwnames=kwnames@entry=0x0) at Python/ceval.c:4819
#148 0x000000000053a5b6 in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:3284
#149 0x0000000000530124 in PyEval_EvalFrameEx (f=f@entry=0xa1a3f8, throwflag=throwflag@entry=0) at Python/ceval.c:718
#150 0x0000000000530b4b in _PyEval_EvalCodeWithName (_co=_co@entry=0x7ffff0a6c4c0, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88, args=args@entry=0x0, argcount=argcount@entry=0, kwnames=kwnames@entry=0x0, kwargs=kwargs@entry=0x8, kwcount=kwcount@entry=0, kwstep=kwstep@entry=2, defs=defs@entry=0x0, defcount=defcount@entry=0, kwdefs=kwdefs@entry=0x0, closure=closure@entry=0x0, name=name@entry=0x0, qualname=qualname@entry=0x0) at Python/ceval.c:4128
#151 0x0000000000531168 in PyEval_EvalCodeEx (_co=_co@entry=0x7ffff0a6c4c0, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88, args=args@entry=0x0, argcount=argcount@entry=0, kws=kws@entry=0x0, kwcount=kwcount@entry=0, defs=defs@entry=0x0, defcount=defcount@entry=0, kwdefs=kwdefs@entry=0x0, closure=closure@entry=0x0) at Python/ceval.c:4149
#152 0x00000000005311b2 in PyEval_EvalCode (co=co@entry=0x7ffff0a6c4c0, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88) at Python/ceval.c:695
#153 0x00000000004231c4 in run_mod (mod=mod@entry=0xab1fe0, filename=filename@entry=0x7ffff0b5f660, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88, flags=flags@entry=0x7fffffffcea0, arena=arena@entry=0x7ffff0b94520) at Python/pythonrun.c:980
#154 0x0000000000425a44 in PyRun_FileExFlags (fp=fp@entry=0x9501a0, filename_str=filename_str@entry=0x7ffff7efaeb0 ""test/test_nn.py"", start=start@entry=257, globals=globals@entry=0x7ffff7eaac88, locals=locals@entry=0x7ffff7eaac88, closeit=closeit@entry=1, flags=flags@entry=0x7fffffffcea0) at Python/pythonrun.c:933
#155 0x0000000000425ddb in PyRun_SimpleFileExFlags (fp=fp@entry=0x9501a0, filename=<optimized out>,
    filename@entry=0x7ffff7efaeb0 ""test/test_nn.py"", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffcea0) at Python/pythonrun.c:396
#156 0x0000000000425f37 in PyRun_AnyFileExFlags (fp=fp@entry=0x9501a0, filename=0x7ffff7efaeb0 ""test/test_nn.py"", closeit=closeit@entry=1, flags=flags@entry=0x7fffffffcea0) at Python/pythonrun.c:80
#157 0x00000000004349d9 in run_file (fp=fp@entry=0x9501a0, filename=filename@entry=0x917300 L""test/test_nn.py"", p_cf=p_cf@entry=0x7fffffffcea0) at Modules/main.c:338
#158 0x0000000000435583 in Py_Main (argc=argc@entry=3, argv=argv@entry=0x916010) at Modules/main.c:809
#159 0x000000000041d21a in main (argc=3, argv=0x7fffffffd018) at ./Programs/python.c:69
`

Other tests from test/run_test.sh pass, although I didn't run the MAGMA tests because I haven't installed it outside of a conda environment."
102,7342,0,"[feature request] [PyTorch] More flexible optimizer API. Currently, there is no easy way to change the decay / momentum / lr for a parameter after constructing the optimizer. For example, it is hard to stop a parameter from being decayed in an iteration.

Maybe we should support a getter/setter API like .

cc @vincentqb"
177,27037,0,"`padding_mode` support for `functional.unfold`. ## 🚀 Feature
Add  support for  operation.

## Motivation
 is a very useful function for window operations other than convolution. In its current implementation, it only supports the ""constant"" padding mode, and automatically fills the margins with zeros. I have a use case where I need to use the ""edge"" padding mode. 

## Pitch

I want to  to have the  argument exactly like the convolution operation. This would also make the API more coherent.

## Alternatives

I guess the alternative is to manually copy cell values to the padded area.
"
114,11982,0,"[feature request] Publish wheels with debug symbols. It would be great if wheels of pytorch with debug symbols were published (or at least made available via https://ci.pytorch.org), so that users can investigate native crashes without going through the whole (error prone) process of building pytorch themselves.

For reference, I asked [the question on the forums first, but got no answer](https://discuss.pytorch.org/t/pytorch-wheel-with-debug-symbols/25169).

cc @ezyang @seemethere @malfet @walterddr"
480,15016,0,"torch.tril does not support 0-sized dims. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:



<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior



## Environment

PyTorch version: 1.0.0a0+54b5dd9
Is debug build: No
CUDA used to build PyTorch: 9.2.88

OS: CentOS Linux 7 (Core)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
CMake version: version 3.12.2

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: Tesla M40
GPU 1: Tesla M40

Nvidia driver version: 396.26
cuDNN version: Probably one of the following:
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.0.5
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.3
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.20
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.1.2
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.4.1
/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn_static.a
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.6
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.6.0.20
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.6.0.21
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.7.1.2
/usr/local/fbcode/gcc-5-glibc-2.23/lib/libcudnn.so.7.1.4
/usr/local/fbcode/platform007/lib/libcudnn.so.7.1.4

Versions of relevant libraries:
[pip] Could not collect
[conda] magma-cuda92              2.4.0                         1    pytorch
[conda] torch                     1.0.0a0+54b5dd9           <pip>

"
73,20355,1,"Backwards hangs. Training is hanging during a call to backwards - I might be wrong but it looks like it hangs while one of the threads tries to acquire a lock.

The setup is the following:
 - pytorch version is 1.0.0post2
 - start 4 python processes on same machine (using multiprocessing spawn, but same happens with Popen)
 - each process uses a different gpu and different data (all data is loaded into memory prior to start training)
 - use torch.distributed (nccl) to synchronise training; all communication between different processes happens via nccl

What I observe is that at random times, one of the processes hangs - and the others then wait for it in the call to all_reduce.

Unfortunately, it is hard to reproduce, and it can take an arbitrary amount of time to hang - sometimes it hangs after 10min, sometimes it hangs after 10h+.

Any help is appreciated - thanks.

At the moment where it hangs, the threads of the frozen process are in this state:
> (gdb) info threads
  Id   Target Id         Frame
  16   Thread 0x7f22711b9700 (LWP 23578) ""python"" 0x00007f22d64fff0d in poll () from /lib64/libc.so.6
  15   Thread 0x7f2270943700 (LWP 23614) ""python"" 0x00007f22d650c03f in accept4 () from /lib64/libc.so.6
  14   Thread 0x7f2265fff700 (LWP 23644) ""python"" 0x00007f22d64fff0d in poll () from /lib64/libc.so.6
  13   Thread 0x7f22617fe700 (LWP 30293) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  12   Thread 0x7f22609ed800 (LWP 30294) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  11   Thread 0x7f22605eb880 (LWP 30295) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  10   Thread 0x7f224dffe900 (LWP 30296) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  9    Thread 0x7f224dbfc980 (LWP 30297) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  8    Thread 0x7f224d7faa00 (LWP 30298) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  7    Thread 0x7f224d3f8a80 (LWP 30299) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  6    Thread 0x7f221aee5700 (LWP 68788) ""python"" 0x00007f22d70f851d in __lll_lock_wait () from /lib64/libpthread.so.0
  5    Thread 0x7f221a6e4700 (LWP 68789) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  4    Thread 0x7f2219ee3700 (LWP 68790) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  3    Thread 0x7f22196e2700 (LWP 68791) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  2    Thread 0x7f2218ee1700 (LWP 68792) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
  1    Thread 0x7f22d750f740 (LWP 23478) ""python"" 0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0

The stack trace per thread is below.

Thread 1:
>#0  0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f22bfb9656f in __gthread_cond_wait (__mutex=<optimized out>, __cond=<optimized out>)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/x86_64-conda_cos6-linux-gnu/bits/gthr-default.h:877
#2  std::condition_variable::wait (this=<optimized out>, __lock=...) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:53
#3  0x00007f2278eb08e3 in torch::autograd::Engine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#4  0x00007f22bc696a0c in torch::autograd::python::PythonEngine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
#5  0x00007f22bc69722c in THPEngine_run_backward(THPEngine*, _object*, _object*) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
...

Threads 2-5:
>#0  0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f22bfb9656f in __gthread_cond_wait (__mutex=<optimized out>, __cond=<optimized out>)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/x86_64-conda_cos6-linux-gnu/bits/gthr-default.h:877
#2  std::condition_variable::wait (this=<optimized out>, __lock=...) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:53
#3  0x00007f2278eac92b in torch::autograd::ReadyQueue::pop() () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#4  0x00007f2278eaf083 in torch::autograd::Engine::thread_main(torch::autograd::GraphTask*) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#5  0x00007f2278eaba77 in torch::autograd::Engine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#6  0x00007f22bc6968aa in torch::autograd::python::PythonEngine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
#7  0x00007f22bfb9a678 in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#8  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#9  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 6:
>#0  0x00007f22d70f851d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x00007f22d70f61a0 in pthread_cond_broadcast@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#2  0x00007f22bfb96594 in __gthread_cond_broadcast (__cond=<optimized out>)
    at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/build/build-cc-gcc-final/x86_64-conda_cos6-linux-gnu/libstdc++-v3/include/x86_64-conda_cos6-linux-gnu/bits/gthr-default.h:865
#3  std::condition_variable::notify_all (this=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/condition_variable.cc:73
#4  0x00007f2278eaf167 in torch::autograd::Engine::thread_main(torch::autograd::GraphTask*) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#5  0x00007f2278eaba77 in torch::autograd::Engine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1
#6  0x00007f22bc6968aa in torch::autograd::python::PythonEngine::thread_init(int) () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
#7  0x00007f22bfb9a678 in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#8  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#9  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 7-13:
>#0  0x00007f22d70f5995 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f22cb364725 in __kmp_suspend_64 () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#2  0x00007f22cb2d5e4e in bool _INTERNAL_25_______src_kmp_barrier_cpp_3dc39ea5::__kmp_wait_template<kmp_flag_64, 1, false, true>(kmp_info*, kmp_flag_64*, void*) ()
   from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#3  0x00007f22cb2d9398 in _INTERNAL_25_______src_kmp_barrier_cpp_3dc39ea5::__kmp_hyper_barrier_release(barrier_type, kmp_info*, int, int, int, void*) () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#4  0x00007f22cb2df4d2 in __kmp_fork_barrier(int, int) () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#5  0x00007f22cb320d8e in __kmp_launch_thread () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#6  0x00007f22cb360571 in _INTERNAL_26_______src_z_Linux_util_cpp_51eec780::__kmp_launch_worker(void*) () from /opt/anaconda3/lib/python3.7/site-packages/numpy/../../../libiomp5.so
#7  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#8  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 14:
>#0  0x00007f22d64fff0d in poll () from /lib64/libc.so.6
#1  0x00007f22beb8a323 in ?? () from /lib64/libcuda.so.1
#2  0x00007f22bebecacd in ?? () from /lib64/libcuda.so.1
#3  0x00007f22beb8c988 in ?? () from /lib64/libcuda.so.1
#4  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#5  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 15:
>#0  0x00007f22d650c03f in accept4 () from /lib64/libc.so.6
#1  0x00007f22beb8b2ca in ?? () from /lib64/libcuda.so.1
#2  0x00007f22beb7d8dd in ?? () from /lib64/libcuda.so.1
#3  0x00007f22beb8c988 in ?? () from /lib64/libcuda.so.1
#4  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#5  0x00007f22d650abad in clone () from /lib64/libc.so.6

Thread 16:
>#0  0x00007f22d64fff0d in poll () from /lib64/libc.so.6
#1  0x00007f22bca13879 in c10d::TCPStoreDaemon::run() () from /opt/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so
#2  0x00007f22bfb9a678 in std::execute_native_thread_routine_compat (__p=<optimized out>) at /opt/conda/conda-bld/compilers_linux-64_1534514838838/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94
#3  0x00007f22d70f1e25 in start_thread () from /lib64/libpthread.so.0
#4  0x00007f22d650abad in clone () from /lib64/libc.so.6"
284,28206,0,"torch.utils.tensorboard.SummaryWriter.add_graph do not support non-tensor inputs. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.Run my script below:


and you can see the trace(take bug 3 as an example):

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
writer.add_graph should run normally.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 Collecting environment information...
PyTorch version: 1.3.0
Is debug build: No
CUDA used to build PyTorch: None
OS: Mac OSX 10.14.6
GCC version: Could not collect
CMake version: Could not collect
Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
Versions of relevant libraries:
[pip] numpy==1.17.2
[pip] torch==1.3.0
[pip] torchvision==0.4.1
[conda] torch                     1.3.0                    pypi_0    pypi
[conda] torchvision               0.4.1                    pypi_0    pypi

## Additional context

<!-- Add any other context about the problem here. -->
1.TensorboardX.SummaryWriter.add_graph has the same bug as torch.utils.tensorboard
2.Besides this bug, I hope add_graph could accept not only a tuple as positional arguments, but also a dict as keyword arguments for the model.forward()'s input"
730,16181,0,"[Feature Request] Add `__getitem__()` to `nn.Module`. ## 🚀 Feature

Enable to call each layer in nn.Module like python dict.

 or 
-> 


## How

Add bellow function to  .
- reference: [chainer.Chain](https://github.com/chainer/chainer/blob/v5.1.0/chainer/link.py#L871-L873)



## Motivation

Access each layer in network easily (Especially for beginners).

#### old



#### new




Maybe this is different from the concept of PyTorch?"
310,30987,0,"Remove `.data`. Even though it is not documented, many users still use it. And it leads to many bugs in user code.
So we should remove it completely to prevent this.
The expected steps are:
- [ ] Add a new api to make a shallow copy that does not share version? Or a cat + unbind function that does not share version counter if that is enough.
- [ ] Remove the use of .data in all our internal code
  - [ ] 
  - [ ]  and 
  - [ ] 
  - [ ]  Blocky by https://github.com/pytorch/pytorch/pull/30258#issuecomment-558344600 (not blocked ones were done in #31479)
  - [ ]  Use that requires the new api mentioned above.
  - [x]  #31480 
  - [ ]  Simple ones done in #31481 and #31482
  - [ ] 
  - [ ] 
  - [ ] 
  - [x] *ignored* 
- [ ] Add warning to every call to 
- [ ] Actually remove  ?


cc @ezyang @SsnL @albanD @zou3519 @gqchen"
384,215,0,"missing tensor constructors. Missing while writing docs

- (size, stride)
- (storage, storageOffset, size, stride)"
13,28761,1,"torch.tensor() is very slow when it is passed an h5py Dataset.. ## 🐛 Bug

 is very slow when it is passed an h5py Dataset.

## To Reproduce

Create a new HDF5 file with a 1000x1000 float32 dataset:



Then load it back into a Tensor:



It's very slow. However, if the dataset is converted into a NumPy array first, it performs much faster:



The resulting tensors are equal. Perhaps the manual NumPy array conversion avoids an expensive conversion to a Python nested list of floats.


cc @VitalyFedyunin @ngimel"
15,25690,1,"Dropout behaves differently on different devices. I have two machines. One has a P100 and another has 4 K80s.
I found the following code output differently on the two machines.
It means seeding doesn't make the output of dropout reproducible on different devices. 
Is it by design?
If we can not make the output reproducible on different devices, it may be a trouble of comparing different models.

The output of P100 machine is always

The output of K80 machine is always


"
60,847,1,"the loss is not decreasing. Hi,
I have created a simple model consisting of two 1-layer nn competing each other. So, I have my own loss function based on those nn outputs.  It is very similar to GAN. The problem is that for a very simple test sample case, the loss function is not decreasing. For now I am using non-stochastic optimizer to eliminate randomness.  Here is the pseudo code with explanation

n1_model = Net1(Dimension_in_n1, Dimension_out) # 1-layer nn with sigmoid
n2_model =Net2(Dimension_in_n2, Dimension_out) # 1-layer nn with sigmoid

n1_optimizer = torch.optim.LBFGS(n1_model.parameters(), lr=0.01,max_iter = 50)
n2_optimizer = torch.optim.LBFGS(n2_model.parameters(), lr=0.01, max_iter = 50)

for t in range(iter):
    x_n1 = Variable(torch.from_numpy(...)) #load input of nn1 in batch size
    x_n2 = Variable(torch.from_numpy(...)) #load input of nn2 in batch size

    def closure():
       reset_grad(n1_model.parameters())
       reset_grad(n2.parameters())
        y_n1 = n1_model(x_n1)
        y_n2 = n2_model(x_n2)
        n1_params =getParams(n1_model)
        n2_param =getParams(n2_model)
        loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer
        loss.backward(retain_variables=True)
        return loss

    n1_optimizer.step(closure)


    def clos():
       reset_grad(n1_model.parameters())
       reset_grad(n2_model.parameters())
        y_n1 = n1_model(x_n1)
        y_n2 = n2_model(x_n2)
        n1_params =getParams(n1_model)
        n2_param =getParams(n2_model)
        loss = my_loss_function(y_n1, y_n2, n1_params, n2_param) # my loss function, mean square error + regularizer
        loss.backward()
        return loss

    n2_optimizer.step(clos)


and here is the definition of my loss function:

def my_loss_function(n1_output, n2_output, n1_parm, n2_param):
    sm = torch.pow(n1_output - n2_output, 2)
    reg = torch.norm(n1_parm,2) + torch.norm(n2_param,2)
    y = torch.sum(sm) + 1 * reg
    return y

when I plot loss function, it has oscillation; I expect it to decrease during training.
"
93,4081,1,"20% more gpu memory usage on resnet. Been testing with resnet 50 when I noticed that a batch of 128 / gpu no longer fits in multi-gpu. I'm seeing around 20% more gpu memory being used now (see below).

'old version' commit hash https://github.com/pytorch/pytorch/commit/50009144c02155be6afd4570e93f453e73904a8e
'new version' commit hash https://github.com/pytorch/pytorch/commit/7ddcb91c7f84c3da8cc9f7fba28a3ae9ecd6cc45

"
421,24006,0,"result random. abosolutely same pytorch code:
when I use 1 gpu to train the model, every time results are same
But when i use mutli gpus to train the model, every time result are random, 
pazzled!"
709,12540,0,"[RFC] Removing Nervana GPU. NervanaGPU is currently included as a [third-party module](https://github.com/pytorch/pytorch/tree/c2a57d082d36f30f32d5c4cc7147ab57ce2c3097/third_party). However the repository is not under active development anymore and has not been modified since 3 years ago.

Checking the commit that added it, it was [this one](https://github.com/pytorch/pytorch/commit/9201cdd029aa820ebc92c71bd4f6d4105163ca80) for the optimized GEMM kernel that was developed at Nervana.

There are 2 kernels of interest in Nervana's work:

### Nervana GEMM

While Nervana GEMM is extremely tuned for Maxwell Architecture, but with Volta and Turing, GPUs are now offering Tensor Cores that are much faster. I expect that Nvidia tuned their GEMM implementation since then.

Alternatives:
- CuBLAS
- [Nervana Maxas](https://github.com/NervanaSystems/maxas) which isolated the GEMM kernel and is a much smaller library than the full NervanaGPU, so it's probably possible to clone and maintain it.
- Honorary mention: [Nvidia Cutlass](https://github.com/NVIDIA/cutlass), which achieves 90% speed of CuBLAS and supports Tensor cores without assembly.
- Nvidia's TensorRT for inference.

### Nervana convolution kernel

(If used, but i don't think it is)

The main appeal is the winograd kernel [discussed extensively here](https://github.com/soumith/convnet-benchmarks/issues/93) but it's [not even in NervanaGPU repo but in Neon repo.](https://github.com/NervanaSystems/nervanagpu/issues/25#issuecomment-203168418).
Furthermore it has been integrated into CuDNN with torch NCHW layout instead of Neon's CHWN layout and convolution was probably one of Nvidia focus in the past 2 years.

### Conclusion

Should the benchmarks show that alternatives to Nervana achieves the same speed on current GPU for a wide range of input size (as tensor cores are currently very restrictive), I think NervanaGPU code should be dropped to save on code, maintenance, dependencies and build options.


"
567,25971,0,"Outdated exception message, ""Call init_rpc(name) first."". ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Call


got


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera"
655,3214,0,"Cannot directly use lr_scheduler in current pip-install package. When I install pytorch using pip, I found lr_scheduler.py is in , but this line is missing:
https://github.com/pytorch/pytorch/blob/d89d9d74bdd0b64c119980003aa29195a61f93a9/torch/optim/__init__.py#L18
Thus user should manually add  to use torch.optim.lr_scheduler.
I hope it will be updated in the next version."
148,22296,0,"Support data parallel for heterogeneous GPU. ## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
Hopefully,  can support data parallel on multiple heterogeneous GPU.


## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
I'm currently working on a big model and would like to use  to speed up training. However, I use two different GPU on my computer, i.e, GTX 1060 and GTX 1080, when I train the model, 1060 becomes a bottleneck and slows down the whole training process.


## Pitch
It will be great that  can automatically assign tasks according to computation power or support manually specifying mini-batch size on each GPU.
<!-- A clear and concise description of what you want to happen. -->

"
668,3388,0, recipe for target 'CMakeFiles/THD.dir/base/TensorDescriptor.cpp.o' failed
24,9873,1,"Pytorch is slow when only using CPU, and cannot utilize multicore of CPU. When I testing the acceleration effect on CPU by decomposing convolution layers, I found pytorch is slow  and cannot utilize multicores of CPU. 




Output is that while cpu usage is 100%-200%
> 147.48149728775024 16.699654817581177

![pytorch](https://user-images.githubusercontent.com/10665923/43258302-eca30248-9104-11e8-8045-49d8d63819a4.PNG)

However, Keras (TF  back-end)  is faster and multi-threaded.

Output is that while cpu usage is 800%-1600%
> 26.393059253692627 13.783706188201904

![keras](https://user-images.githubusercontent.com/10665923/43258314-f57cf11c-9104-11e8-8cfd-c094a798df48.PNG)

## System Info
Pytorch 0.4
Ubuntu 16.04



cc @VitalyFedyunin @ngimel"
174,27954,0,"[quantization] Dynamic LSTM doesn't serialize properly when traced. Repro



Output



cc @suo @jerryzh168 @jianyuh @dzhulgakov @raghuramank100"
643,5553,0,"NCCL Error 1 when using torch.nn.DataParallel. I have ran into this error on the parameter broadcast step of .
It sounds like this, to be precise: .
#2332 is possibly related.
"
679,24304,0,"MinMax Observer should return tensor of scale and tensor of zero_point?. ## 🐛 Bug
Right now it returns a tensor of [scale, zero_point], I think maybe we should just return the values directly, or two Tensors?
https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py#L65"
390,8981,0,"Intermittent failure of TestCollectEnv.test_expect. The failing diff usually looks something like:



I thought this was some weird buffering problem, but then I looked more carefully at the test, and one of the regexes looks VERY suspicious:



If the size of the short version hash varies, this regex will gobble more or less. It really shouldn't be hardcoded as wildcard periods.

CC @zou3519 "
403,26081,0,"RNN fails to compile due to USE_FBGEMM=OFF. ## 🐛 Bug

Compiling libtorch without FBGEMM causes a compile error in RNN.

> /d/dev/pytorch/aten/src/ATen/native/RNN.cpp:286:57: error: ‘PackedLinearWeight’ was not declared in this scope

The struct exists in aten/src/ATen/native/quantized/cpu/fbgemm_utils.h

Workaround was to #ifdef certain sections of code. See diff:


## To Reproduce

Steps to reproduce the behavior:

1. cmake -DUSE_FBGEMM=OFF ..
2. Compile error

## Expected behavior

Compiles

## Environment

 - PyTorch Version (e.g., 1.0): master
 - OS (e.g., Linux): Linux x64
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): cmake -DUSE_FBGEMM=OFF ..
 - Python version: 3.6
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information: N/A

cc @jerryzh168 @jianyuh @dzhulgakov @zou3519"
38,13045,1,"dataparallel not working on nvidia gpus and amd cpus. ## 🐛 Bug

We have a number of machines with Threadripper CPUs, and 2 NVIDIA GPUs, some have 1070ti cards some 1080 some 1080ti and one with titanXp, they all displayed this behavior, when switching to using data parallel, training would fail, i.e. accuracy would not go up. We first saw this in our code base, but it also happens on the imagnet example from the pytorch examples repo

## To Reproduce

Steps to reproduce the behavior:

1. run the imagnet example for the examples repo in pytorch with dataparallel

these error messages were found in the dmesg log:


[1118468.873266] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000ea13a000 flags=0x0020]
[1118468.942145] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000ea139068 flags=0x0020]
[1118468.942189] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0000040 flags=0x0020]
[1118468.942227] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d00007c0 flags=0x0020]
[1118468.942265] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0001040 flags=0x0020]
[1118468.942303] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0000f40 flags=0x0020]
[1118468.942340] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d00016c0 flags=0x0020]
[1118468.942377] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0002040 flags=0x0020]
[1118468.942414] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d0001e40 flags=0x0020]
[1118468.942452] nvidia 0000:0a:00.0: AMD-Vi: Event logged [IO_PAGE_FAULT domain=0x000f address=0x00000000d00025c0 flags=0x0020]
[1118468.942489] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0003040 flags=0x0020]
[1118468.942525] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0002d40 flags=0x0020]
[1118468.942560] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d00034c0 flags=0x0020]
[1118468.942596] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0004040 flags=0x0020]
[1118468.942632] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0003c40 flags=0x0020]
[1118468.942667] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d00043c0 flags=0x0020]
[1118468.942703] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0005040 flags=0x0020]
[1118468.942739] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d0004b40 flags=0x0020]
[1118468.942774] AMD-Vi: Event logged [IO_PAGE_FAULT device=0a:00.0 domain=0x000f address=0x00000000d00052c0 flags=0x0020]

## Expected behavior

accuracy would not pick up in most cases, it never picked up for the validation set. We managed to work around this problem by turning off IOMMU in the bios.

## Environment

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 2.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a

"
99,25150,0,"Problems with install python from source. ## 🐛 Bug

For Pytorch 1.3.0 a from source installation is necessary, if one only have cuda 9.0. I cant update the version, because I do not have root access.

## To Reproduce
I have done everything what they say in the tutorial. The result is the following output of CMakerError:
main':
CheckSymbolExists.c:(.text+0x1b): undefined reference to 

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

PyTorch version: From Source
Is debug build: N/A
CUDA used to build PyTorch: 9.0

OS: Debian GNU/Linux 9.9 (stretch)
GCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
CMake version: version 3.14.0

Python version: 3.7
Is CUDA available: yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX 1080
GPU 1: NVS 310

Nvidia driver version: 384.111
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.17.0
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0
[conda] blas                      1.0                         mkl  
[conda] magma-cuda90              2.5.0                         1    pytorch
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] mkl-service               2.0.2            py37h7b6447c_0  
[conda] mkl_fft                   1.0.14           py37ha843d7b_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0

## Additional context

<!-- Add any other context about the problem here. -->
"
20,4211,1,"Initial .cuda() slow occur again at the latest cuda9 version PyTorch . Similar to #537. the first call of .cuda() take a very long time. It take even more on a model.cuda().





PyTorch install with conda: . 
GPU: GTX 1080 and CUDA9.0."
298,19160,0,"Suggest model.eval() in torch.no_grad (and vice versa). ## 📚 Documentation

  and  are both commonly used in evaluating a model.

[Confusion exists about whether setting  also sets ](https://discuss.pytorch.org/t/does-model-eval-with-torch-set-grad-enabled-is-train-have-the-same-effect-for-grad-history/17183/5?u=ataraxy)

Would you accept a PR which suggests usage of the other, with words like:

* If evaluating a model's performance, using Module.eval() may also be useful.
* If evaluating a model's performance, using autograd.no_grad may also be useful."
19,8818,1,"MPI causing job to hang --- unresponsive to external (termination) signals. ## Issue description
When running  with the MPI backend in a cluster, the job fails to exit and hangs unresponsively --- essentially any node in the cluster that the runs the script becomes unresponsive when you try to terminate the job. This varies from run to run, but it happens often enough that it is problematic.

Here is a minimal reproduction script. High level steps:

1.  initialize multiple DataLoader workers
2. communicate model parameters
3. update model parameters
4. run forward-backward pass

## Code example

For a single machine with  GPUs, run



issues.py:



## System Info

PyTorch (built from source) version: 0.5.0a0+f8c18e0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010
CMake version: version 3.11.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration:
GPU[0-8]: V100 (NVIDIA voltas)

Nvidia driver version: 384.81
cuDNN version: cudnn/v7.0-cuda.9.0

Versions of relevant libraries:

openmpi/3.0.0/gcc.5.4.0
NCCL/2.2.12-1-cuda.9.0

[pip] numpy (1.14.3)
[pip] torch (0.5.0a0+f8c18e0)
[pip] torchvision (0.2.1)
[conda] magma-cuda90   2.3.0       1         pytorch
[conda] torch                     0.5.0   <pip>
[conda] torchvision            0.2.1   <pip>

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd"
397,12365,0,"[JIT] mytuple[0], mytuple[i] does not work in JIT. Current, the following code gives an error:

With error message:
"
464,3822,0,"Building from source on master is broken. Related to https://github.com/pytorch/pytorch/pull/3817.

When building from source on Ubuntu 16.04, I get the following.


Reverting the offending PR solves the problem."
146,31560,0,"What should I do if my model has multiple inputs and multiple outputs?. Here are the functions to export the model using onnx:
torch.onnx.export(model, dummy_input, ""alexnet.onnx"", verbose=True, input_names=input_names,
output_names=output_names)
What should I do if my model has multiple inputs and multiple outputs? How can I export a model?
My model interface is roughly as follows:
 model = YOLO_SEGONE()
 model_initial(model, model_name)
 if is_cuda == True:
        model.cuda()
 model.eval()
 mask_out1，mask_out2 = model.seg_sub_net3D(conv9_roiA, conv7_roiA, conv5_roiA, conv3_roiA)
        "
10,11333,1,"MultivariateNormal and potrf is slow on gpu and seems to have some memory leak. ## Issue description

When I initialize a [MultivariateNormal](https://github.com/pytorch/pytorch/blob/bb7d1837bc164b06e1d0826a20a8f7e8338a44e3/torch/distributions/multivariate_normal.py#L76) object with a covariance matrix provided with batch dimension too, it is very slow on GPU, and it is always slower on GPU then on CPU. 
It even seems to leak the memory with or without the batch dimension and both on CPU and GPU, but with different intensity (check n parameter in code example).
One more strange thing: It uses more CPU RAM when running on GPU.

Digging a bit deeper, I experienced the same with the Tensor's potrf function, so probably this is the root cause.

## Code example


## Outputs

Testing MultivariateNormal









Testing potrf









## System Info
Collecting environment information...
PyTorch version: 0.5.0a0+e9ad743
Is debug build: No
CUDA used to build PyTorch: 9.2.88

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0
CMake version: version 3.12.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.2.148
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti

Nvidia driver version: 396.54
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[pip] Could not collect
[conda] magma-cuda91              2.3.0                         1    pytorch
[conda] torch                     0.5.0a0+e9ad743           <pip>
[conda] torchfile                 0.1.0                     <pip>
[conda] torchnet                  0.0.4                     <pip>
[conda] torchvision               0.2.1                     <pip>

Pytorch is used in docker container
pip version:
pip 18.0 from /opt/conda/lib/python3.6/site-packages/pip (python 3.6)
"
546,27123,0,"RuntimeError: CUDA error: device-side assert triggered. Hello, I implemented DNC codes with PyTorch Version 1.2.
but I got runtime errors when I trained my DNC model. 
I attach a file which contains error messages, because it is too long to write here. 
----------------------
I solve this problem.

"
577,4053,0,"grad_fn() crashed in pytorch 0.3.0. @soumith 
Environment:
unbuntu 16.04+torch-0.3.0.post4-cp35-cp35m-linux_x86_64.whl (from pytorch.org)
windows 10 + pytorch(master) compiled with msvc


According to internal logic，it should be a failure but not a crash. when I tested the code  in pytorch (0.2.0), it did not crash."
178,7721,0,"Error compiling from source (OS X 10.12, CUDA 9.0, CUDNN 7.0.4). Installed clean OS 10.12.6 on MacBook Pro late 2013 with GT 750M video card. Been trying to get this to work for weeks, tried every tip I can find. Following error message:

running install
running build_deps
+ WITH_CUDA=0
+ [[ --with-cuda == \-\-\w\i\t\h\-\c\u\d\a ]]
+ WITH_CUDA=1
+ shift
+ WITH_ROCM=0
+ [[ --with-nnpack == \-\-\w\i\t\h\-\r\o\c\m ]]
+ WITH_NNPACK=0
+ [[ --with-nnpack == \-\-\w\i\t\h\-\n\n\p\a\c\k ]]
+ WITH_NNPACK=1
+ shift
+ WITH_MKLDNN=0
+ [[ ATen == \-\-\w\i\t\h\-\m\k\l\d\n\n ]]
+ WITH_GLOO_IBVERBS=0
+ [[ ATen == \-\-\w\i\t\h\-\g\l\o\o\-\i\b\v\e\r\b\s ]]
+ WITH_DISTRIBUTED_MW=0
+ [[ ATen == \-\-\w\i\t\h\-\d\i\s\t\r\i\b\u\t\e\d\-\m\w ]]
+ CMAKE_INSTALL='make install'
+ USER_CFLAGS=
+ USER_LDFLAGS=
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
++ dirname tools/build_pytorch_libs.sh
+ cd tools/..
+++ pwd
++ printf '%q\n' /Users/hsr/pytorch
+ PWD=/Users/hsr/pytorch
+ BASE_DIR=/Users/hsr/pytorch
+ TORCH_LIB_DIR=/Users/hsr/pytorch/torch/lib
+ INSTALL_DIR=/Users/hsr/pytorch/torch/lib/tmp_install
+ THIRD_PARTY_DIR=/Users/hsr/pytorch/third_party
+ CMAKE_VERSION=cmake
+ C_FLAGS=' -DTH_INDEX_BASE=0 -I""/Users/hsr/pytorch/torch/lib/tmp_install/include""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/TH"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THC""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THS"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCS""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THNN"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCUNN""'
+ C_FLAGS=' -DTH_INDEX_BASE=0 -I""/Users/hsr/pytorch/torch/lib/tmp_install/include""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/TH"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THC""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THS"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCS""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THNN"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1'
+ LDFLAGS='-L""/Users/hsr/pytorch/torch/lib/tmp_install/lib"" '
+ LD_POSTFIX=.so.1
+ LD_POSTFIX_UNVERSIONED=.so
++ uname
+ [[ Darwin == \D\a\r\w\i\n ]]
+ LDFLAGS='-L""/Users/hsr/pytorch/torch/lib/tmp_install/lib""  -Wl,-rpath,@loader_path'
+ LD_POSTFIX=.1.dylib
+ LD_POSTFIX_UNVERSIONED=.dylib
+ CPP_FLAGS=' -std=c++11 '
+ GLOO_FLAGS=
+ THD_FLAGS=
+ NCCL_ROOT_DIR=/Users/hsr/pytorch/torch/lib/tmp_install
+ [[ 1 -eq 1 ]]
+ GLOO_FLAGS='-DUSE_CUDA=1 -DNCCL_ROOT_DIR=/Users/hsr/pytorch/torch/lib/tmp_install'
+ [[ 0 -eq 1 ]]
+ [[ 0 -eq 1 ]]
+ CWRAP_FILES='/Users/hsr/pytorch/torch/lib/ATen/Declarations.cwrap;/Users/hsr/pytorch/torch/lib/THNN/generic/THNN.h;/Users/hsr/pytorch/torch/lib/THCUNN/generic/THCUNN.h;/Users/hsr/pytorch/torch/lib/ATen/nn.yaml'
+ CUDA_NVCC_FLAGS=' -DTH_INDEX_BASE=0 -I""/Users/hsr/pytorch/torch/lib/tmp_install/include""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/TH"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THC""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THS"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCS""   -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THNN"" -I""/Users/hsr/pytorch/torch/lib/tmp_install/include/THCUNN"" -DOMPI_SKIP_MPICXX=1'
+ [[ '' -eq 1 ]]
+ '[' -z 8 ']'
+ BUILD_TYPE=Release
+ [[ -n '' ]]
+ [[ -n '' ]]
+ echo 'Building in Release mode'
Building in Release mode
+ mkdir -p torch/lib/tmp_install
+ for arg in '""$@""'
+ [[ ATen == \n\c\c\l ]]
+ [[ ATen == \g\l\o\o ]]
+ [[ ATen == \A\T\e\n ]]
+ pushd /Users/hsr/pytorch/aten
~/pytorch/aten ~/pytorch
+ build_aten
+ mkdir -p build
+ pushd build
~/pytorch/aten/build ~/pytorch/aten ~/pytorch
+ cmake .. -DCMAKE_BUILD_TYPE=Release -DNO_CUDA=0 -DNO_NNPACK=0 -DCUDNN_INCLUDE_DIR=/usr/local/cuda/include -DCUDNN_LIB_DIR=/usr/local/cuda/lib -DCUDNN_LIBRARY=/usr/local/cuda/lib/libcudnn.7.dylib -DNO_MKLDNN=1 -DMKLDNN_INCLUDE_DIR= -DMKLDNN_LIB_DIR= -DMKLDNN_LIBRARY= -DATEN_NO_CONTRIB=1 -DCMAKE_INSTALL_PREFIX=/Users/hsr/pytorch/torch/lib/tmp_install -DCMAKE_EXPORT_COMPILE_COMMANDS=1 -DCMAKE_C_FLAGS= -DCMAKE_CXX_FLAGS= -DCMAKE_EXE_LINKER_FLAGS= -DCMAKE_SHARED_LINKER_FLAGS= -DWITH_ROCM=0
-- Autodetected CUDA architecture(s): 3.0 
-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor
-- Removing -DNDEBUG from compile flags
-- MAGMA not found. Compiling without MAGMA support
-- Could not find hardware support for NEON on this machine.
-- No OMAP3 processor on this machine.
-- No OMAP4 processor on this machine.
-- SSE2 Found
-- SSE3 Found
-- AVX Found
-- AVX2 Found
-- Atomics: using GCC intrinsics
-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - iomp5 - pthread - m]
--   Library mkl_intel_lp64: /Users/hsr/anaconda3/envs/kate/lib/libmkl_intel_lp64.dylib
--   Library mkl_intel_thread: /Users/hsr/anaconda3/envs/kate/lib/libmkl_intel_thread.dylib
--   Library mkl_core: /Users/hsr/anaconda3/envs/kate/lib/libmkl_core.dylib
--   Library iomp5: /Users/hsr/anaconda3/envs/kate/lib/libiomp5.dylib
--   Library pthread: /usr/lib/libpthread.dylib
--   Library m: /usr/lib/libm.dylib
-- MKL library found
-- Found a library with BLAS API (mkl).
-- Found a library with LAPACK API. (mkl)
-- Found cuDNN: v7.0.4  (include: /usr/local/cuda/include, library: /usr/local/cuda/lib/libcudnn.7.dylib)
disabling MKLDNN because NO_MKLDNN is set
CMake Deprecation Warning at src/ATen/CMakeLists.txt:25 (CMAKE_POLICY):
  The OLD behavior for policy CMP0026 will be removed from a future version
  of CMake.

  The cmake-policies(7) manual explains that the OLD behaviors of all
  policies are deprecated and that a policy should be set to OLD only under
  specific short-term circumstances.  Projects should be ported to the NEW
  behavior and not rely on setting a policy to OLD.


-- Using python found in /Users/hsr/anaconda3/envs/kate/bin/python
-- TBB: using libc++.
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
-- Configuring build for SLEEF-v3.2
   Target system: Darwin-16.7.0
   Target processor: x86_64
   Host system: Darwin-16.7.0
   Host processor: x86_64
   Detected C compiler: AppleClang @ /Library/Developer/CommandLineTools/usr/bin/clang
-- Using option  to compile libsleef
-- Building shared libs : OFF
-- MPFR : LIB_MPFR-NOTFOUND
-- GMP : LIBGMP-NOTFOUND
-- RUNNING_ON_TRAVIS : 0
-- COMPILER_SUPPORTS_OPENMP : 
disable contrib because ATEN_NO_CONTRIB is set
-- Configuring done
-- Generating done
-- Build files have been written to: /Users/hsr/pytorch/aten/build
+ make install -j8
[  0%] Built target mkdisp
[  0%] Built target common
[  2%] Built target mkalias
[  2%] Built target mkrename
[  6%] Built target cpuinfo
[  6%] Built target cuda_aten_files_are_generated
[  6%] Built target aten_files_are_generated
Scanning dependencies of target tbb_static
[  6%] Built target mkmasked_gnuabi
[  6%] Built target arraymap
[  6%] Built target mkrename_gnuabi
[  6%] Built target renamedsp256.h_generated
[  7%] Built target dispavx.c_generated
[  8%] Built target headers
[  9%] Built target renameSSE2.h_generated
[  9%] Built target renameAVX.h_generated
[  9%] Built target renameFMA4.h_generated
[  9%] Built target renameSSE4.h_generated
[  9%] Built target renameAVX2.h_generated
[ 10%] Built target dispsse.c_generated
[ 10%] Built target renameAVX2128.h_generated
[ 10%] Built target renamedsp128.h_generated
Scanning dependencies of target sleefavx2
Scanning dependencies of target sleefavx2128
[ 11%] Built target sleefsse2
[ 11%] Built target dispavx_obj
[ 11%] Built target sleefavx
[ 11%] Built target sleeffma4
[ 11%] Built target sleefsse4
[ 11%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimdsp.c.o
[ 11%] Building C object sleef/src/libm/CMakeFiles/sleefavx2.dir/sleefsimddp.c.o
Scanning dependencies of target dispsse_obj
[ 11%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimdsp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/sleefavx2128.dir/sleefsimddp.c.o
[ 12%] Building C object sleef/src/libm/CMakeFiles/dispsse_obj.dir/dispsse.c.o
[ 12%] Building CXX object src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp.o
[ 12%] Building CXX object src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp.o
[ 13%] Building CXX object src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/condition_variable.cpp.o
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp:23:
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:26:
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:102:52: error: unknown type
      name 'isolation_tag'
    task_proxy* internal_pop( __TBB_ISOLATION_EXPR(isolation_tag isolation) ) {
                                                   ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:108:27: error: use of
      undeclared identifier 'no_isolation'; did you mean 'isolation'?
        if ( isolation != no_isolation ) {
                          ^~~~~~~~~~~~
                          isolation
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:102:66: note: 'isolation'
      declared here
    task_proxy* internal_pop( __TBB_ISOLATION_EXPR(isolation_tag isolation) ) {
                                                                 ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:109:36: error: no member
      named 'isolation' in 'tbb::internal::task_prefix'
            while ( curr->prefix().isolation != isolation ) {
                    ~~~~~~~~~~~~~~ ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/mailbox.h:206:44: error: unknown type
      name 'isolation_tag'
    task_proxy* pop( __TBB_ISOLATION_EXPR( isolation_tag isolation ) ) {
                                           ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:123:9: error: 
      use of undeclared identifier 'enforce_segment_allocated'
        enforce_segment_allocated(s.load<relaxed>()); //it's hard to rec...
        ^
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp:23:
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:237:43: error: unknown
      type name 'isolation_tag'
    task* get_task( __TBB_ISOLATION_EXPR( isolation_tag isolation ) );
                                          ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:188:13: error: 
      use of undeclared identifier 'enforce_segment_allocated'
            enforce_segment_allocated(s.load<relaxed>());
            ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:246:31: error: unknown
      type name 'isolation_tag'
    task* get_task( size_t T, isolation_tag isolation, bool& tasks_omitted );
                              ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:257:51: error: unknown
      type name 'isolation_tag'
    task* get_mailbox_task( __TBB_ISOLATION_EXPR( isolation_tag isolation ) );
                                                  ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:265:75: error: unknown
      type name 'isolation_tag'
  ...steal_task( __TBB_ISOLATION_ARG( arena_slot& victim_arena_slot, isolatio...
                                                                     ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:273:9: error: 
      use of undeclared identifier 'enforce_segment_allocated'
        enforce_segment_allocated(array0); // initial segment should be ...
        ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:366:115: error: unknown
      type name 'isolation_tag'
  ...__TBB_atomic reference_count& completion_ref_count, isolation_tag isolat...
                                                         ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:424:47: error: unknown
      type name 'isolation_tag'
    task* reload_tasks( __TBB_ISOLATION_EXPR( isolation_tag isolation ) );
                                              ^
[ 13%] Building CXX object src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/critical_section.cpp.o
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:426:127: error: unknown
      type name 'isolation_tag'
  ...__TBB_ISOLATION_ARG( intptr_t top_priority, isolation_tag isolation ) );
                                                 ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:391:9: error: 
      use of undeclared identifier 'enforce_segment_allocated'
        enforce_segment_allocated(my_segment[k].load<relaxed>()); //if v...
        ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:430:52: error: unknown
      type name 'isolation_tag'
    task* winnow_task_pool ( __TBB_ISOLATION_EXPR( isolation_tag isolation ) );
                                                   ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/scheduler.h:434:88: error: unknown
      type name 'isolation_tag'
  ...size_t H0 , __TBB_ISOLATION_ARG( size_t T0, isolation_tag isolation ) );
                                                 ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:409:13: error: 
      use of undeclared identifier 'enforce_segment_allocated'
            enforce_segment_allocated(my_segment[k].load<relaxed>());
            ^
/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp:466:9: error: 
      use of undeclared identifier 'enforce_segment_allocated'
        enforce_segment_allocated(my_segment[i].load<relaxed>());
        ^
6 errors generated.
make[2]: *** [src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/concurrent_vector.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp:28:
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:25:2: error: 
      Do not #include this internal file directly; use public TBB headers
      instead.
#error Do not #include this internal file directly; use public TBB heade...
 ^
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:102:31: error: 
      use of undeclared identifier 'continue_msg'
    class function_body_leaf< continue_msg, continue_msg, B> : public fu...
                              ^
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:125:38: error: 
      use of undeclared identifier 'continue_msg'
    class function_body_leaf< Input, continue_msg, B> : public function_...
                                     ^
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:148:31: error: 
      use of undeclared identifier 'continue_msg'
    class function_body_leaf< continue_msg, Output, B > : public functio...
                              ^
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:208:40: error: 
      unknown class name 'task'; did you mean 'tbb::task'?
    class forward_task_bypass : public task {
                                       ^~~~
                                       tbb::task
/Users/hsr/anaconda3/envs/kate/include/tbb/task.h:542:7: note: 'tbb::task'
      declared here
class task: __TBB_TASK_BASE_ACCESS interface5::internal::task_base {
      ^
In file included from /Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp:28:
/Users/hsr/anaconda3/envs/kate/include/tbb/internal/_flow_graph_impl.h:218:29: error: 
      use of undeclared identifier 'SUCCESSFULLY_ENQUEUED'
            if (new_task == SUCCESSFULLY_ENQUEUED) new_task = NULL;
                            ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
[ 13%] Built target sleefavx2128
[ 14%] Built target sleefavx2
20 errors generated.
make[2]: *** [src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/Users/hsr/pytorch/third_party/tbb/src/tbb/arena.cpp.o] Error 1
[ 14%] Built target dispsse_obj
Scanning dependencies of target sleef
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefdp.c.o
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefsp.c.o
[ 15%] Building C object sleef/src/libm/CMakeFiles/sleef.dir/sleefld.c.o
make[1]: *** [src/ATen/cpu/tbb/CMakeFiles/tbb_static.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 15%] Linking C static library ../../lib/libsleef.a
[ 15%] Built target sleef
make: *** [all] Error 2

Any help appreciated, thanks for your time. "
269,12855,0,"Model with Caffe2 runs much slower than it with pytorch in GPU mode !!!!. Firstly, I run my model(CRNN model) on Pytorch GPU. With code:

**Result:  cost time: 0.002339872884750366**

Secondly, I convert my pytorch model to onnx and then convert onnx to pb in caffe2. With code:


and 


Thirdly,I run model in Caffe2 on GPU.  With code:

**Run time per RunNet: 0.005980247449874878**

**But, the speed is slower than that in Pytorch !  How weird!** 

Any one konws what happens!
"
297,8430,0,"batchnorm2d  track_running_stats. when I load a pretrained model, Runtime error occurred:  Missing key(s) in state_dict: bn.num_batches_tracked in every BatchNorm2d layer"
279,4689,0,"switch CUDA svd and qr to using cuSolver. Currently we use MAGMA for these solvers, which uses a mix of CPU and GPU. Wonder if cuSolver will be faster, and we can avoid the CPU path entirely.

cc: @ngimel just want to confirm, cuSolver (dense) wont use the CPU right?

cc @ngimel @vincentqb @vishwakftw @SsnL @jianyuh"
303,18434,0,"improve jit error message for legacy constructor. Reported by @jph00

> new_tensor is a legacy constructor and is not supported in the JIT

It'd be helpful to know in the error message what the non-legacy alternative is."
183,23466,0,"import failure (macOS). ## 🐛 Bug
I was using pytorch without any issue, but I just found out today that when importing torch, it threw me an error. Possibly regarding MKL library. I could not figure out why.

## To Reproduce

Steps to reproduce the behavior:

1. 
1.
1.
Below are the error message:


## Expected behavior

Clearly, it should not be happening.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:



 - PyTorch Version (e.g., 1.0): 1.1.0 (although not shown above, this is the version I installed from conda)
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

I have no idea what happened but probably because I was using julia and trying to install ubuntu on my Mac. Maybe somehow I messed up with my environment?
"
365,3025,0,"Function request: np.isin. I believe that should be useful to have a function similar to https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.in1d.html, that compares a tensor element-wise with a list of possible values. (I'm using it to filter labels/classes in some classifiers).

Expected behavior:



Now it's possible to implement it by iterating over the filter, and storing the results in a tensor with the OR operator. But a faster implementation is possible with TH/THC

My current implementation:

(N, S)(S)batch X samplessamples

cc @mruberry @rgommers @heitorschueroff"
168,21249,0,"The code is incomplete and the data set is missing.  Hello, have you ever encountered the pytorch official website fine-tuning Maskrcnn, less code, and the data set mask in pedestrian detection is all black，In references/detection/, we have a number of helper functions to simplify training and evaluating detection models. Here, we will use references/detection/engine.py, references/detection/utils.py and references/detection/transforms.py. Just copy them to your folder and use them here.there is less code"
465,19742,0,"Serialization of tensors with pickle.dumps seems to be inconsistent, leading to inconsistent redis cache hit/miss. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

Scenario: 

Redis cache set up 
call set pickle.dumps(tensor) some_value
call get pickle.dumps(tensor)
call get pickle.dumps(tensor) (different call)

hit on call 1,
potential miss on call 2 

These inconsistent hits/misses occur over O(1000) calls to the the same tensors in objects repeated ~9 times


## To Reproduce

Steps to reproduce the behavior: (skeleton)
0. import redis
0. import time
0. value = torch.tensor(...) 
0. start = time.time()
1. redis_cache = redis.StrictRedis(host=""localhost"", port=6379, db=0)  # redis_cache
2. redis_cache.set(pickle.dumps(value), pickle.dumps((hidden, cell)))
3. cache_return = redis_cache.get(pickle.dumps(value))
            if cache_return is not None:
                logger.info(""Cache hit for {} took {} ms"".format(value, time.time()-start))
                return pickle.loads(cache_return)
4. cache_return = redis_cache.get(pickle.dumps(value))
            if cache_return is not None:
                logger.info(""Cache hit for {} took {} ms"".format(value, time.time()-start))
                return pickle.loads(cache_return)

(Cache miss logging would happen after .set() is called)

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

Example Log

2019-04-25 14:04:00,344 ID_1 [INFO] cache_encoder: Cache hit for tensor([[ 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  3]]) took 0.00019598007202148438 ms
2019-04-25 14:04:00,347 ID_1 [INFO] cache_encoder: Cache miss for tensor([[ 2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  3]]) took 0.001859426498413086 ms

The weird thing is the hit happened before the miss, so it looks like something is unreliable with the serialisation

2019-04-25 14:04:00,303 ID_1 [INFO] cache_encoder: Cache hit for tensor([[ 2, 12,  3]]) took 0.0001957416534423828 ms
2019-04-25 14:04:00,312 ID_1 [INFO] cache_encoder: Cache miss for tensor([[ 2, 12,  3]]) took 0.0018868446350097656 ms

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

Consistent cache hits for the same tensor after: set pickle.dumps() cache_return

## Environment

 - PyTorch Version (e.g., 1.0): 1.01
 - OS (e.g., Linux): Ubuntu 16.04
 - How you installed PyTorch (, , source): pip install
 - Build command you used (if compiling from source):
 - Python version: 3.6
 - CUDA/cuDNN version: 8
 - GPU models and configuration: GeForce GTX 1050
 - Any other relevant information: 

## Additional context

<!-- Add any other context about the problem here. -->

When using str(value) instead of pickle.dumps(value), the cache hits are consistent (however, for large tensors ... get injected into the str() output, so this ultimately fails). This makes me think the problem is in how pickle.dumps() serializes the tensors. I saw a similar issue: https://github.com/pytorch/pytorch/issues/9168 for changes (improving speed) along these lines (moving to byteIO).

Because of the related issue I thought I would post the issue here.
"
39,30968,1,"Categorical.sample too slow. ## 🐛 Bug

Categorical.sample(shape) calls _shape_ times .
This is slow when we want to get many samples from a large number of classes.

## To Reproduce

Steps to reproduce the behavior:


Outputs : 

## Expected behavior
Shouldn't be slower than a single call to torch.multinomial

which outputs : 

## Environment
PyTorch version: 1.3.1
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 410.79
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.4
[pip] numpysane==0.17
[pip] torch==1.3.1
[conda] blas                      1.0                         mkl
[conda] faiss-gpu                 1.6.0            py37h1a5d453_0    pytorch
[conda] mkl                       2019.4                intel_243    intel
[conda] mkl_fft                   1.0.12           py37ha843d7b_0
[conda] mkl_random                1.0.2            py37hd81dba3_0
[conda] pytorch                   1.3.1           py3.7_cuda10.0.130_cudnn7.6.3_0    pytorch

cc @vincentqb @fritzo @neerajprad @alicanb @vishwakftw"
622,7528,0,"[caffe2] C2 ONNX build is flaky due to lack of ordering dep. When it fails it looks like this:



This indicates we're trying to run the proto compiler before the proto file is created. This probably means there should be some dep declared which we didn't. It's intermittent because sometimes we're lucky and the build gets ordered the right way.

Maybe this should get reported to onnx repo.

CC @houseroad @bddppq @anderspapitto 

Cases of this failing:
- https://github.com/pytorch/pytorch/pull/7465
- https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-mkl-ubuntu16.04-build/4286//console"
423,26189,0,"[ONNX] bool comparison with 0,1 should export False,True constants. ## 🐛 Bug

Should be able to compare boolean to 0,1 and exporter maybe should then use Fale,True as constants. But [comparing bool to 0 results in error](https://github.com/pytorch/pytorch/issues/25805)

## To Reproduce



## Expected behavior

export 0 as False constant to be valid ONNX

## Environment

PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: version 3.14.0

Python version: 2.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 1070
Nvidia driver version: 418.40.04
cuDNN version: /usr/local/lib/libcudnn.so.5.1.10

Versions of relevant libraries:
[pip] mictorch==0.0.1
[pip] numpy==1.16.4
[pip] torch==1.2.0
[pip] torchvision==0.4.0
[conda] Could not collect

## Additional context

Work around is:

"
696,7373,0,"help delete issue. If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

Provide a short description.

## Code example

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2:
- How you installed PyTorch (conda, pip, source):
- Build command you used (if compiling from source):
- OS:
- PyTorch version:
- Python version:
- CUDA/cuDNN version:
- GPU models and configuration:
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
"
548,15287,0,"Please make Slerp function available for generative model sampling. ## 🚀 Feature
I see that [](https://pytorch.org/docs/stable/torch.html?highlight=lerp#torch.lerp) function is available but  does not exist in PyTorch 1.0.

## Motivation
This feature request is motivated from the current research requirements and the way generative models are being developed (GANs specifically) i.e. the manipulation of output samples is made from latent input space. 
The paper [DCGAN](https://arxiv.org/abs/1511.06434) performed latent interpolation linearly ([](https://pytorch.org/docs/stable/torch.html?highlight=lerp#torch.lerp) would suffice) but [Sampling Generative Networks](https://arxiv.org/abs/1609.04468) paper suggests to use [Slerp function](https://en.wikipedia.org/wiki/Slerp) for interpolation. Moreover the following recent works go for Slerp too, for instance recently introduced [GAN 2.0](https://arxiv.org/abs/1812.04948), etc.

## Pitch
Please make [Slerp](https://en.wikipedia.org/wiki/Slerp) function available so that we can sample generative networks more effectively. 

## Alternatives
As an alternative please provide a PyTorch -based code snippet to compute [Slerp function](https://en.wikipedia.org/wiki/Slerp) effectively and easily.

## Additional context

Here is a Wikipedia's article showing example of [C++ and Python-NumPy code snippet](https://en.wikipedia.org/wiki/Slerp#Source_code) for [Slerp](https://en.wikipedia.org/wiki/Slerp). "
742,24690,0,"Migrate `digamma` and `digamma_` from the TH to Aten (CPU). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
574,19057,0,"building blocked when executing `python setup.py build` during COMPILER_SUPPORTS_LONG_DOUBLE. ## building blocked when executing python setup.py build

### Here is my environment info:

- OS: Ubuntu 16.04
- GCC: 5.3.0
- CUDA: 9.0
- GPU: GeForce GTX 1080 * 2
- PyTorch to be installed: 1.0.0

### I try to build PyTorch=1.0.0 from source but it seems to be blocked here (). I had waited for a long time but it didn't go on. Why did this happen?




"
595,16693,0,"[build error] Build caffe error. ## ❓Error while building latest pytorch



I did the installing steps presented in the official website. This is on macOS.

UI.:
Interesting that there is string.h in the anaconda directory:
"
460,7498,0,Production. How to deploy the pYtorch trained model to production ? 
362,25045,0,"[Distance functions] F.pdist backward CUDA invalid configuration. ## 🐛 Bug

After passing the input through a  function, the gradient *w.r.t.* to the input can not be correctly calculated on a cuda device.

## To Reproduce

Steps to reproduce the behavior:

3. When increasing the tensor size and running

the error message is:


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version: **1.2.0**
 - OS (e.g., Linux): **CentOS 7.6.1810 (Core)**
 - How you installed PyTorch (, , source): **conda**
 - Build command you used (if compiling from source): N/A
 - Python version: **3.6.7**
 - CUDA/cuDNN version: **V10.0.130**
 - GPU models and configuration: **GeForce GTX 1080**
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->

cc @ngimel"
56,5436,1,"Slower performance of eq and fill_ compared to numpy. 




"
561,24639,0,"Migrate `std` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
337,22687,0,"DispatchStub should report what operator it failed to find kernel for. In #22681 we get an unhelpful error message because it just says that CUDA kernels is missing, it doesn't say what is missing. It would be good of dispatch stub gave more information. Maybe we can pass in a string constant to the template parameter and update the macro to stringify the struct name to pass in.

cc @cpuhrsch @VitalyFedyunin @colesbury "
642,8109,0,"Sometimes the torch.save method does not work properly.. ## Issue description
When indexing a part of the tensor, the entire original tensor is saved.

## Code example



I expect that divided tensors will be stored. (It will be about 100KB)
However, it is actually stored in the original matrix size. (Approximately 12MB)

## System Info

PyTorch
conda
Windows 10
0.4.0
Python 3.6
CUDA 9.0
Nvidia Geforce 1070"
70,11647,1,"Performance (speed) regression from 0.4.0 to 0.4.1. ## Issue description

I have a fairly optimized cnn-blstm-crf tagger [here](https://github.com/dpressel/baseline/blob/master/python/baseline/pytorch/tagger/model.py) with the crf defined [here](https://github.com/dpressel/baseline/blob/f0204432076c166ce3d6672705826f33aec95dbe/python/baseline/pytorch/torchy.py#L669).

On pytorch  using cuda  and cudnn  I can run a single epoch of the conll 2003 NER task in 21.41 +/- 0.28

When the only thing I change is the version of pytorch to  (the current conda install) a single epoch now takes  27.99 +/-  0.26

These models are run on gpu.

From the pytorch forums I was told to post here https://discuss.pytorch.org/t/large-preformance-regression-on-0-4-1/25037

## Code example

Please try to provide a minimal example to repro the bug.

Assuming pytorch is installed



## System Info

### 0.4.0

Collecting environment information...
PyTorch version: 0.4.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: GPU 0: GeForce GTX 1070 with Max-Q Design
Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7.1.2
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so
/usr/local/cuda-9.0/lib64/libcudnn.so.7
/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0/lib64/libcudnn.so.7.1.2
/usr/local/cuda-9.0/lib64/libcudnn.so.7.2.1
/usr/local/cuda-9.0/lib64/libcudnn_static.a

Versions of relevant libraries:
[pip] numpy (1.14.3)
[pip] torch (0.4.0)
[pip] torchfile (0.1.0)
[pip] torchvision (0.2.1)
[conda] cuda90                    1.0                  h6433d27_0    pytorch
[conda] pytorch                   0.4.0            py36hdf912b8_0  
[conda] torchfile                 0.1.0                     <pip>
[conda] torchvision               0.2.1                    py36_1    pytorch


### 0.4.1
Collecting environment information...
PyTorch version: 0.4.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: GPU 0: GeForce GTX 1070 with Max-Q Design
Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn.so.7.1.2
/usr/local/cuda-9.0-cudnn-7/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so
/usr/local/cuda-9.0/lib64/libcudnn.so.7
/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0/lib64/libcudnn.so.7.1.2
/usr/local/cuda-9.0/lib64/libcudnn.so.7.2.1
/usr/local/cuda-9.0/lib64/libcudnn_static.a

Versions of relevant libraries:
[pip] numpy (1.14.3)
[pip] torch (0.4.1.post2)
[pip] torchfile (0.1.0)
[pip] torchvision (0.2.1)
[conda] cuda90                    1.0                  h6433d27_0    pytorch
[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch
[conda] torchfile                 0.1.0                     <pip>
[conda] torchvision               0.2.1                    py36_1    pytorch"
64,14687,1,"Advanced indexing slower than numpy. ## 🐛 Bug

Advanced indexing is slower than numpy.

## To Reproduce

Steps to reproduce the behavior:



## Expected behavior

PyTorch should be as fast as numpy when doing advanced indexing of tensors that do not require grad.

## Environment

 - PyTorch Version (e.g., 1.0): 1.0.0.dev20181202
 - OS (e.g., Linux): Ubuntu 18.04.1 LTS
 - How you installed PyTorch (, , source): conda install pytorch-nightly -c pytorch
 - Python version: 3.7.0


cc @mruberry @rgommers @heitorschueroff @VitalyFedyunin @ngimel"
739,422,0,"get rid of requiring to do super.__init__ in nn.Container subclasses. This seems like its something that users'll forget to do, and we might get a lot of subtle bug reports because of this. Seems like a good usability fix early on."
434,2620,0,"nn.Embedding error when using max_norm. 
"
242,5790,0,"Add hookable weights. It would be nice to have hookable weights, where per-layer parameters can be operated upon and determined dynamically during each forward pass. The following is a short list of techniques that could benefit from this functionality:

- Quantization[[1]](https://arxiv.org/abs/1609.07061)
- Pruning[[2]](https://arxiv.org/abs/1510.00149)[[3]](https://arxiv.org/abs/1707.06168)[[4]](https://arxiv.org/abs/1802.00124)
- Stochastic gradient estimation[[5]](https://arxiv.org/abs/1711.00123)[[6]](https://arxiv.org/abs/1703.07370)
- DropConnect[[7]](https://cs.nyu.edu/~wanli/dropc/)
- Regularization[[8]](https://openreview.net/forum?id=H1Y8hhg0b)
- Manifold Tangent Classifier[[9]](https://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf)

I've begun work on [such a framework](https://github.com/castorini/candle), but the code is research-oriented and not production ready. I think the community would benefit from having official support for hookable weights.

I think it would also be interesting to have a more extensible backprop framework, in the sense of allowing for computation of different user-defined quantities across the computation graph. For example, the Hessian diagonals can be computed efficiently[[10]](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf) using a backprop-like approach.

Thanks for reading.


cc @albanD @mruberry @jbschlosser @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @vkuzo"
529,19104,0,"Is there 0.5.0version of pytorch?. when I tried to use Dataparallel, I came across the problem"" Arguments are located on different GPUs"",
I found that I should use the 0.5.0 version to solve the problem on the forum.
However ,I have been trying to find the correct version for a long time with no result.Can anyone help me ?
(ps, I tried to run the code on pytorch1.0, but error came in the dataloader part “RandomSampler’ object has no attribute 'replacement”.and I found no solution.
Thank you !!"
563,6140,0,"[jit][script] Support `dim` wrapping. The following does not compile:

The expected behavior should be that the dim wraps around (to match torch semantics).

cc @zdevito "
476,19305,0,"libtorch gpu efficiency. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce
here is my code

auto out = faceDetect->forward({ tensor_image }).toTuple();
c10::cuda::CUDACachingAllocator::emptyCache();
...some tensor ops on gpu
boundingBoxesOfOne = torch::masked_select(boundingBoxesOfOne, mask);

faceDetect is a sfd net,after forward(),if i call emptyCache(),it will cost 170ms time to emptyCache;if i don't call emptyCache(),masked_select will cost 170ms;so i don't know how to optimize this situation，thks！


## Environment

libtorch Version 1.0 :
OS windows10:
CUDA/cuDNN version: 9.0
GPU models and configuration: GTX 1060

## Additional context

<!-- Add any other context about the problem here. -->
"
76,13716,1,"depthwise convolution are slow on cpu . I try to use depthwise convolution to reduce parameters of my model. However I found depthwise convolutions are slow on cpu, just 4x~5x than normal 3x3 convolution, while input_channel and output channel are 256. Is there any way to speed up the process? My version is pytorch 0.4.1.



cc @VitalyFedyunin @ngimel"
157,2126,0,when i changed thetorchvision.utils source code long to int. how do i do this
53,5426,1,"torch.from_numpy is slow for mmap'ped numpy arrays. I'm looking at feasibility of using Ray as parameter server combined with single-machine NN implementation. 

The bottleneck for using it with PyTorch is that result of Ray calls come as numpy array created on top of [mmaped memory](https://groups.google.com/forum/#!msg/ray-dev/pNsxWI-iSyI/vhWnjgAfAwAJ), and PyTorch  on these arrays is slow, working at about 2.5 GB/sec, which is the speed of single-threaded memcpy.

Regular numpy array can be turned into PyTorch GPU tensors at 8-11 GB/sec, depending on whether memory has been page locked.

Is this memcpy necessary? Can it be done in multi-threaded way?
Related issue on Ray side: https://github.com/ray-project/ray/issues/1614

https://github.com/diux-dev/cluster/blob/master/yuxin_numpy/tf_numpy_benchmark.py

Pytorch 0.3.0.post4
Repro instructions like in https://github.com/pytorch/pytorch/issues/5412#issuecomment-368658892
"
703,4281,0,"Implementation of Bipolar Activation Functions. We're looking to make a pull request for [bipolar activation functions](https://arxiv.org/abs/1709.04054). Hope to get some feedback.

This is a trick to make activation functions self centering. In short: Because the ReLU keeps only positive numbers, it shifts the post-activation mean in a positive direction. However, if we for every other neuron we instead keep the negative numbers, we cancel this effect. 

So for half the neurons we do , and other half we do . For an i.i.d. input vector, the effect of this is to halve the mean of the vector, post-activation, .

In our paper we find empirically that this can help learning in RNNs and ConvNets. Our empirical results are with ReLUs and ELUs, but I would expect it to hold for anything ReLU like.

One way to implement it would be to put something like the following in nn/functional.py:



This also includes bipolar max-pooling functions (i.e. min-pooling for half the inputs).


1) Would there be interest in a pull request for this?
2) Any comments on the implementation? Of course, we'll have to add doc strings, and also put something similar into nn/modules/activation.py"
462,19024,0,"Loss explosion with DataParallel on WGAN models . See https://discuss.pytorch.org/t/huge-loss-with-dataparallel/40749 for the original report.

This seems to have regressed between 0.4.1 and 1.0 and needs to be debugged.

cc @mrshenli "
559,16428,0,"'swap' type in torch.nn.TripletMarginLoss. ## 📚 Documentation

is it  instead of ?

https://pytorch.org/docs/stable/nn.html?highlight=tripletmarginloss#torch.nn.TripletMarginLoss


<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->
"
285,14839,0,"Test OpenCV4 in CI. It's not tested, evidence https://github.com/pytorch/pytorch/pull/14356

cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra"
289,21135,0,"[cmake build] can't build pytorch with install mkl library . ### pytorch can't be built with  installed mkl library 
 - PyTorch version : master branch 
 - OS : CentOS-7.5

I had installed intel . So when I built  from the scratch, the build system couldn't find the installed  libraries.

I found  the mkl-related module, which was  . -related enviroments  were set to default path as the following:

In this way, I couldn't build  through  with the installed  libraries. Is there any effective method to build  with installed . 
### What I did to build with installed 
In order to build  with installed  and accept environments from the command line, I did modify the  as the following: 

And to accept the environments from the command line, the file  was modified as the following: 
`
--- a/tools/build_pytorch_libs.py
+++ b/tools/build_pytorch_libs.py
@@ -228,6 +228,18 @@ def run_cmake(version,
     if os.getenv('MKL_TBB'):
         cmake_defines(cmake_args, INTEL_MKL_TBB=check_env_flag('MKL_TBB'))

+    if os.getenv('INTEL_COMPILER_DIR'):
+        cmake_defines(cmake_args, INTEL_COMPILER_DIR=os.getenv('INTEL_COMPILER_DIR'))
+
+    if os.getenv('INTEL_MKL_DIR'):
+        cmake_defines(cmake_args, INTEL_MKL_DIR=os.getenv('INTEL_MKL_DIR'))
+
+    if os.getenv('INTEL_MKL_SEQUENTIAL'):
+        cmake_defines(cmake_args, INTEL_MKL_SEQUENTIAL=check_env_flag('INTEL_MKL_SEQUENTIAL'))
+
+    if os.getenv('INTEL_MKL_TBB'):
+        cmake_defines(cmake_args, INTEL_MKL_TBB=check_env_flag('INTEL_MKL_TBB'))
+
     mkldnn_threading = os.getenv('MKLDNN_THREADING')
     if mkldnn_threading:
         cmake_defines(cmake_args, MKLDNN_THREADING=mkldnn_threading)"
748,2527,0,"Pytorch giving a cuda runtime error (30). I have made small changes in the  example code. Changed [these lines](https://github.com/pytorch/examples/blob/86bc3e516dde92fee0ce00668a3714c03155dd44/imagenet/main.py#L80-L88) to:


And, when I run the script, it throws a cuda runtime error which is as follows:


My cuda version is as follows:


A much more detailed post is also available here: https://stackoverflow.com/q/45861767/4993513"
92,6126,1,"Inf and nan loss . Here is my network:

Then, I created the network and the training loops as :


The initial output was very large, then it became Inf and finally Nan. Why is this happening?"
493,12981,0,"torchvision install error. Sorry to trouble you, but I get some error when I start pytorch
I set 
- PyTorch Build —— Stable
- Your OS——Windows
- Package——Conda
- Language——Python 3.6
- CUDA——8.0

It shows 
Run this Command:
pytorchpip3

The first command is alright, shows  # All requested packages already installed. And I can import torch.
But the second command shows:

I have tried use , and shows the same message. 
I find 
 
 in the error message,But




"
523,10807,0,"[caffe2] libcaffe2.so：onnx_c2::GetEmptyStringAlreadyInited[abi:cxx11]  undefined reference. I build caffe2 with origin caffe2 + ubuntu16.04 + anaconda3, the cmake summary is : 

-- ******** Summary ********
-- General:
--   CMake version         : 3.6.3
--   CMake command         : /home/hengshan/.conda/envs/caffe2_py2/bin/cmake
--   Git version           : v0.8.1-1502-g3c9081d-dirty
--   System                : Linux
--   C++ compiler          : /usr/bin/c++
--   C++ compiler version  : 5.4.0
--   BLAS                  : Eigen
--   CXX flags             :  -fvisibility-inlines-hidden -DONNX_NAMESPACE=onnx_c2 -O2 -fPIC -Wno-narrowing -Wno-invalid-partial-specialization
--   Build type            : Release
--   Compile definitions   : 
--   BUILD_BINARY          : ON
--   BUILD_CUSTOM_PROTOBUF : OFF
--     Protobuf compiler   : 
--     Protobuf includes   : 
--     Protobuf libraries  : 
--   BUILD_DOCS            : OFF
--   BUILD_PYTHON          : ON
--     Python version      : 2.7.13
--     Python includes     : /home/hengshan/.conda/envs/caffe2_py2/include/python2.7
--   BUILD_SHARED_LIBS     : ON
--   BUILD_TEST            : ON
--   USE_ATEN              : OFF
--   USE_ASAN              : OFF
--   USE_CUDA              : ON
--     CUDA version        : 8.0
--     CuDNN version       : 6.0.21
--     CUDA root directory : /usr/local/cuda-8.0
--     CUDA library        : /usr/lib/x86_64-linux-gnu/libcuda.so
--     CUDA NVRTC library  : /usr/local/lib/libnvrtc.so
--     CUDA runtime library: /usr/local/cuda-8.0/lib64/libcudart.so
--     CUDA include path   : /usr/local/cuda-8.0/include
--     NVCC executable     : /usr/local/cuda-8.0/bin/nvcc
--     CUDA host compiler  : /usr/bin/cc
--   USE_EIGEN_FOR_BLAS    : 1
--   USE_FFMPEG            : ON
--   USE_GFLAGS            : ON
--   USE_GLOG              : ON
--   USE_GLOO              : ON
--   USE_LEVELDB           : ON
--     LevelDB version     : 1.18
--     Snappy version      : 1.1.3
--   USE_LITE_PROTO        : OFF
--   USE_LMDB              : ON
--     LMDB version        : 0.9.17
--   USE_METAL             : OFF
--   USE_MKL               : 
--   USE_MOBILE_OPENGL     : OFF
--   USE_MPI               : ON
--   USE_NCCL              : ON
--   USE_NERVANA_GPU       : OFF
--   USE_NNPACK            : ON
--   USE_OBSERVERS         : ON
--   USE_OPENCV            : ON
--     OpenCV version      : 3.3.0
--   USE_OPENMP            : OFF
--   USE_PROF              : OFF
--   USE_REDIS             : OFF
--   USE_ROCKSDB           : OFF
--   USE_ZMQ               : OFF
-- Configuring done
-- Generating done

the error is : 
../lib/libcaffe2.so：对‘onnx_c2::GetEmptyStringAlreadyInited[abi:cxx11]()’未定义的引用
collect2: error: ld returned 1 exit status
caffe2/CMakeFiles/logging_test.dir/build.make:117: recipe for target 'bin/logging_test' failed
make[2]: *** [bin/logging_test] Error 1
CMakeFiles/Makefile2:1346: recipe for target 'caffe2/CMakeFiles/logging_test.dir/all' failed
make[1]: *** [caffe2/CMakeFiles/logging_test.dir/all] Error 2
../lib/libcaffe2.so：对‘onnx_c2::GetEmptyStringAlreadyInited[abi:cxx11]()’未定义的引用
collect2: error: ld returned 1 exit status

"
58,21828,1,"torch::zeros is slow for small tensors (C++). ## 🐛 Bug

For small 1d tensors (smaller than about 4000), setting each element to zero with an accessor is faster than using make_zeros.

Benchmarking code (uses https://github.com/google/benchmark):



And here are the results:


As you can see, for small 1d tensors make_zero is sometime almost 3x slower than using an accessor. For 2d tensors, make_zeros is still slower for very small tensors.

## Environment
PyTorch version: 1.1.0a0+de582e2
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.2 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce GTX TITAN X
Nvidia driver version: 418.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.1

Versions of relevant libraries:
[pip3] numpy==1.13.3
[conda] _tflow_select             2.3.0                       mkl  
[conda] blas                      1.0                         mkl  
[conda] magma-cuda10              2.4.0                         1    cpbotha
[conda] mkl                       2019.3                      199  
[conda] mkl-include               2019.3                      199  
[conda] mkl_fft                   1.0.12           py37ha843d7b_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] mkldnn                    0.16.1                        0    mingfeima
[conda] tensorflow                1.13.1          mkl_py37h54b294f_0  
[conda] tensorflow-base           1.13.1          mkl_py37h7ce6ba3_0  
[conda] torch                     1.1.0a0+de582e2          pypi_0    pypi
"
333,10714,0,"[feature request][caffe2] extend FC/FCTranspose op to handle 2d bias.. Currently, FC/FCTranspose only accepts 1d bias. The FC's implementation will do the broadcast to create the 2d bias from 1d bias, but we could just provide the whole 2d bias. Then, we can do the better optimization for onnx Gemm op with caffe2 backend.

I will try to implement this idea and create a pull request later.

@houseroad @bddppq 
"
549,6194,0,"Import order matters: segmentation fault if torch is imported first. I have a couple of packages that I import in my project. Obviously, one of them is . I noticed that if I import  after only one of my other packages things are fine. However, if I import  first I get a segmentation fault. I'm not picky about importing the other module first and then do  but for some reason, I need to import torch first in my work and this has made things a bit complicated for me. I wonder, is this a Python issue or could it be  issue or maybe the other package (bpy) issue? Does anyone know how I can figure out what is causing the segmentation fault?"
585,17465,0,"The source code for conv2d. Hi, I wanna to do some work about the conv2d, but the release did not make the source code of con2d available.  What should I do to approach the conv2d?

## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)
"
126,4115,0,"Feature Request : Alias Multinomial. Would you be interested in extending the alias multinomial sampling to pytorch. The C and the CUDA version are already present in [](https://github.com/torch/torch7/blob/aed31711c6b8846b8337a263a7f9f998697994e7/doc/maths.md#res-torchmultinomialaliasoutput-state) (linked)
I believe it could be very useful for people who want to use multinomial sampling from a static distribution."
511,27690,0,"Multi-GPU Gather is much slower than Scatter. ## 🐛 Bug

Multi-GPU gathering is much slower than scattering

## To Reproduce
Can run the following script on a Multi-GPU machine which should replicate the issue. It creates a large tensor on the CPU and scatters it to multiple GPUs, then also creates large I have tested with both pytorch 1.1.0 and pytorch 1.2.0.


Output: 
Note that when batch size (args.sz) = 128, the tensor to be scattered/gathered is 1GiB.

| NGpus        | batch size           | Scatter Time (s)  |  Gather Time (s) |
| ------------- |:-------------:| -----:|---:|
| 2 | 64 |  0.07 | 0.35 |
| 2 | 128 | 0.14 | 0.7 |
| 2 | 256 | 0.27 | 1.4 |
| 4     | 64   | 0.12 | 0.35 |
| 4 | 128   |  0.16 | 0.7 |
| 4 | 256 | 0.29 | 1.6 |

## Expected behavior

I would expect the scatter and gather timings to be a lot closer than this, not a factor of 5 out.

## Environment

"
479,24093,0,"pytorch forced to use cuda 10 after new update. ## 🐛 Bug
When installing pytorch with conda in a new environment, it always installs compiled with the cuda version 10 and cudatoolkit=10.0. Whether I conda install pytorch 1.2, 1.1, 1.0.1, or 1.0 with cuda90 it always tries to install the cuda 10 version of the package as well as cudatoolkit=10.0. This is via the pytorch conda channel. My machine has an older driver version that can't easily be upgraded. I also tried pip installing from https://download.pytorch.org/whl/cu90/stable and it still insists on using cuda 10.

## To Reproduce
conda create --name test_env python=3.6
conda activate test_env
conda install pytorch=1.1 cuda90 -c pytorch
conda list

In python when you run torch.version.cuda it also lists cuda 10.

The following NEW packages will be INSTALLED:

    blas:           1.0-mkl
    cffi:           1.12.3-py36h2e261b9_0
    cuda90:         1.0-h6433d27_0                        pytorch
    cudatoolkit:    10.0.130-0
    intel-openmp:   2019.4-243
    libgfortran-ng: 7.3.0-hdf63c60_0
    mkl:            2019.4-243
    mkl_fft:        1.0.12-py36ha843d7b_0
    mkl_random:     1.0.2-py36hd81dba3_0
    ninja:          1.9.0-py36hfd86e86_0
    numpy:          1.16.4-py36h7e9f1db_0
    numpy-base:     1.16.4-py36hde5b4d6_0
    pycparser:      2.19-py36_0
    pytorch:        1.1.0-py3.6_cuda10.0.130_cudnn7.5.1_0 pytorch

Proceed ([y]/n)? n

## Expected behavior

Should be able to select the cuda version.

## Environment

Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: TITAN Xp
GPU 1: TITAN Xp
GPU 2: TITAN Xp
GPU 3: TITAN Xp

Nvidia driver version: 390.67
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect

"
664,6131,0,"Build picks up wrong copy of mkl when multiple available. @aosokin reports from https://github.com/pytorch/pytorch/issues/6068#issuecomment-377226963

There might be another related issue with dependencies.
When doing import torch from an anaconda env pytorch uses mkl (libmkl_gf_lp64.so) installed in anaconda root.
Consequently, if the root and the env have different versions (in my case the root had 2018.0.1 and the env had 2018.0.2-1) I was getting this error:



Updating mkl in anaconda root to 2018.0.2-1 solved the problem."
422,22298,0,"[c++] Can't use Sequential inside Sequential . Hi. I was implementing MNASNet for C++ side in torchvision and I ran into this problem. apparently using  inside  is not possible. Here is the minimal code:



@yf225 Can you have a look?"
209,15676,0,"Tests for Vec256 classes. ## 🚀 Feature
We should have tests in C++ for the Vec256 classes.

## Motivation
Currently, the Vec256 code is only tested indirectly through operators that use it. This has led to insufficient test coverage and a number of recent bugs in Vec256 and the operators that use it. We should add sufficient test infrastructure so that we can easily add test cases for new functions on Vec256. It's important that the tests cover the different data types (float, double, int32, int64, etc.) and instruction sets (generic, avx, avx2).

This will require changes to CMake since the current test cases don't enable the AVX/AVX2 instruction sets.

cc @VitalyFedyunin"
166,4673,0,"GPU memory consumption with cudnn.enabled = False. 
According to this code, difference in used GPU memory  before and after convolution is  4809 MB - 1345 MB = 3.5 GB . This is too much since *inp* is just 536MB and *embed0* is 134 MB. From my experiments this issue is fixed if i delete nn.BatchNorm2d block.

What can be the reason for this strange behavior? 

PS: I turned off CUDNN to avoid another bug with ConvTranspose3d  [#4344]

Pyton2.7, PyTorch 0.3 (pip),  CUDA 8, CUDNN 7
"
5,28198,1,"DLRM performance regression on #26963. ## 🐛 DLRM performance regression

<!-- A clear and concise description of what the bug is. -->
This check-in cause a little regression on the [DLRM](https://github.com/facebookresearch/dlrm) benchmark.

Without this check-in the DLRM benchmark result is like:

with this check-in , the result is like:


The profiling data show that there are some time increased in the 
 ,  and  operations

|                 | This      | Before    |
|-----------------|----------:|----------:|
|   | 1559.31ms | 1503.48ms |
|             | 27.14ms   |   14.58ms |
|          | 22.38ms   | 9.79966ms |


## To Reproduce

Steps to reproduce the behavior:

1. Download the DLRM from https://github.com/facebookresearch/dlrm
1. Modify the bench/dlrm_s_benchmark.sh to just run pytorch on cpu version, as
     build=0
     cpu=1
     gpu=0
     pt=1
     c2=0
   And export two KMP variables as
    export KMP_BLOCKTIME=1
    export KMP_AFFINITY=""granularity=fine,compact,1,0""
1. Run bench/dlrm_s_benchmark.sh on SKX8180 machine. performance profiling data is stored at file model1_CPU_PT_28.prof
     'This' is got from commit-id: d0a4b2f586e0901c3c65f1f0e0bae15364e28821
     'Before' is got from commit-id: 42e7eb0426190e07339f03d4e6afb61b7ff5ae9c
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
The DLRM performance has no impacted, Thanks 

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): commit-id: d0a4b2f586e0901c3c65f1f0e0bae15364e28821
 - OS (e.g., Linux): Ubuntu 16.04.5 LTS
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source): python setup.py install
 - Python version: 3.7
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information:
   GCC version: (Ubuntu 8.3.0-16ubuntu3~16.04) 8.3.0
   CMake version: version 3.14.4

    [pip3] numpy==1.16.2
    [pip3] numpydoc==0.8.0
    [conda] blas                      1.0                         mkl
    [conda] mkl                       2019.0                   pypi_0    pypi
    [conda] mkl-devel                 2019.3                      200
    [conda] mkl-include               2019.0                   pypi_0    pypi
    [conda] mkl-service               1.1.2            py37he904b0f_5
    [conda] mkl_fft                   1.0.10           py37ha843d7b_0
    [conda] mkl_random                1.0.2            py37hd81dba3_0

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @jerryzh168"
506,5066,0,"Import Error after updating to 0.3. I tried to update pytorch from 0.2 to 0.3.

The update itself is fine, but when I run the codes, it raises this exception:

CXXABI_1.3.8' not found (required by /home/sliu426/anaconda2/lib/python2.7/site-packages/rdkit/Chem/../../../../././libicui18n.so.58)

import torch
from rdkit import Chem
pip install pytorch`, I guess the installation changes some paths. So I simply switch these two lines of codes and it works well for now:


The solution is a little tricky, and I'm wondering if there's a better one?"
51,27946,1,"torch.mean() is imprecise for very large Tensors. ## 🐛 Bug

"
208,2470,0,"var(dim) and std(dim) are broken for FloatTensor. It seems like  and  are broken on version 0.2.0.1. Please note that you need to pass in the  argument to reproduce this bug. The parameterless  and  are working correctly.

For example:
"
740,6655,0,"cannot load forum. cannot load forum,  the error code is ERR_TIMED_OUT
I get 300ms when ping discuss.pytorch.org
I use Chrome on Win10-64
"
405,11989,0,"RuntimeError: CUDA error: unknown error. ## Issue description
I upgrade CUDA from 8.0 to 9.0, and reinstall cudnn and pytorch.
Then following error happen:
RuntimeError: CUDA error: unknown error

## Code example


Traceback (most recent call last):
  File ""example.py"", line 3, in <module>
    x = torch.tensor([1., 2.], device=device)
RuntimeError: CUDA error: unknown error

## System Info
Collecting environment information...
PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: GPU 0: TITAN Xp
Nvidia driver version: 384.130
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.0
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect
"
268,7214,0,"Do not put system paths in RPATH. This is what I'm seeing today:


Paths  and  were taken from the build environment, but they do not belong in RPATH since those paths are not shipped with the application. Those paths may contain versions of DT_NEEDED libraries that you do not want to use with pytorch, however RPATH has precedence over everything else, so those libraries will get picked, and there is no way around that.

For example, let's say libcublas provided in   is not the version I want since it's too old, I have a newest version in , but I have no way to point to it since  will be ignored.

Caused by: https://github.com/pytorch/pytorch/pull/3255 (for )



cc @malfet @seemethere @walterddr"
152,6111,0,"Report bug script. In our issue submission checklist, we collect a lot of information that can be gotten mechanically. We should have a simple way of collecting this info as a script and uploading it to GitHub."
218,20029,0,"RuntimeError:CUDA Error:out of memory. ## 🐛 Bug
 With a CUDA 10.1 and RTX GPU. 
When I try even the most simpliest code like
nums = torch.randn(2,2).cuda(), it show the above error
And torch.cuda.is_available() is True.
Any idea?"
553,15066,0,[docs] Delete ffi documentation. there's still a page on it: https://pytorch.org/docs/master/ffi.html?highlight=ffi
364,26646,0,"[RFC] Drop Windows CUDA 9.2 support. @peterjc123 and I propose that we drop CUDA 9.2 binary build support for Windows, meaning that we only provide CUDA 10.1 binaries for Windows. Here is our reasoning:

* We strongly prefer to have a single CI provider on which we build all binaries. At the moment, CircleCI is that provider; however, it only supports a single Visual Studio version, that is too new for CUDA 9.2. To run CUDA 9.2 builds on CircleCI, we would have to install both CUDA 9.2 (they provide CUDA 10.1) but also an older version of MSVC which is compatible with CUDA 9.2.
* To workaround this for CUDA 9.2 builds today, we currently do these builds on Azure. However, the agents provided by Microsoft to build nightlies are facing problems, and the release of GitHub Actions has tied up most of the resources available to resolve these problems.

We will not drop from-source support for CUDA 9.2; users are always welcome to build from source if they need this configuration. However, building from source on Windows is quite painful, and @peterjc123 is concerned for LibTorch users, where we rely on the user's environment (as opposed to packaging CUDA for them).

If you are a Windows CUDA 9.2 user that would be unduly affected by this change, please comment on this issue. Thank you!

cc @ezyang @peterjc123"
718,3064,0,"double backward for Conv2d failing with CUDNN_STATUS_NOT_SUPPORTED. Reported at https://discuss.pytorch.org/t/cudnn-status-not-supported-error-occurs-when-apply-autograd-grad-to-compute-high-order-differentiation/8256

"
684,5285,0,"[Bug] Memory leak on Convnet on CPU. - OS: Ubuntu 16.04
- PyTorch version: 0.3.1b0+2b47480
- How you installed PyTorch (conda, pip, source): source
- Python version: 3.6.4
- CUDA/cuDNN version: CUDA 9.0/CuDnn 7
- GPU models and configuration: GTX 1080Ti (as well as GTX 1070)
- GCC version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609

Hello,
While implementing the [SRGan paper](https://arxiv.org/abs/1609.04802) I ran into a memory leak issue while doing the inference on my CPU.
This issue should be easily reproductible from your side as my implementation is [here](https://github.com/EKami/Torchlight/tree/showcase/memory-leak). All you have to do is to clone the repository with , cd into the  folder then run the script with  to run the inference on the cpu. Then you should get a memory leak with the following message:

If you run the same script but with  (defaults on cuda) then the memory leak vanishes. The exact line which cause the memory leak is [this one](https://github.com/EKami/Torchlight/blob/showcase/memory-leak/torchlight/nn/models/srgan.py#L42). Remove that line to get:

and execute the script on the cpu again with  and poof the memory leak vanishes.
I tried on 2 different computer each with the same software installed but for the hardware one has:
 - AMD FX 8350
 - GTX 1070
The second:
 - Intel i7 7700k
 - GTX 1080Ti

And I get the same memory leak on both machines."
61,17745,1,"distributed data parallel, gloo backend works, but nccl deadlock. I have run into the same problem as #14870 .  Because I cannot reopen the issue, I opened a new issue. But the GPUs are all empty, except that 11MB memory is used (no process running). 

The code is easy to reproduce:
Nmax_utilizationmax_utilizationmax_memory_usageN_per_processmax_utilizationmax_memory_usage
it hangs.

But it works fine with ."
137,14954,0,"undefined reference to `vmdLog2'  . 

## Issue description

/data1/bryanleoliu/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp: In member function ‘c10d::AlgorithmEntry* c10d::ProcessGroupGloo::checkout(const c10d::AlgorithmKey&)’:
/data1/bryanleoliu/pytorch/torch/lib/c10d/ProcessGroupGloo.cpp:445:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (vec.size() != cacheNumAlgorithmEntries_) {
                     ^
[ 42%] Linking CXX static library libc10d.a
[ 42%] Built target c10d
[ 47%] Building NVCC (Device) object test/CMakeFiles/c10d_cuda_test.dir/c10d_cuda_test_generated_CUDATest.cu.o
Scanning dependencies of target ProcessGroupMPITest
Scanning dependencies of target TCPStoreTest
Scanning dependencies of target FileStoreTest
[ 52%] Building CXX object test/CMakeFiles/FileStoreTest.dir/FileStoreTest.cpp.o
[ 57%] Building CXX object test/CMakeFiles/TCPStoreTest.dir/TCPStoreTest.cpp.o
[ 61%] Building CXX object test/CMakeFiles/ProcessGroupMPITest.dir/ProcessGroupMPITest.cpp.o
[ 66%] Linking CXX executable FileStoreTest
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp: In function ‘void testAllreduce(int)’:
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp:19:57: warning: ‘at::Tensor at::ones(const at::Type&, at::IntList)’ is deprecated (declared at /data1/bryanleoliu/pytorch/torch/lib/tmp_install/include/ATen/Functions.h:3936) [-Wdeprecated-declarations]
     auto tensor = at::ones(at::CPU(at::kFloat), {16, 16}) * i;
                                                         ^
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp: In function ‘void testBroadcast(int)’:
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp:61:59: warning: ‘at::Tensor at::ones(const at::Type&, at::IntList)’ is deprecated (declared at /data1/bryanleoliu/pytorch/torch/lib/tmp_install/include/ATen/Functions.h:3936) [-Wdeprecated-declarations]
       auto tensor = at::ones(at::CPU(at::kFloat), {16, 16}) * i;
                                                           ^
/data1/bryanleoliu/pytorch/torch/lib/c10d/test/ProcessGroupMPITest.cpp:64:60: warning: ‘at::Tensor at::zeros(const at::Type&, at::IntList)’ is deprecated (declared at /data1/bryanleoliu/pytorch/torch/lib/tmp_install/include/ATen/Functions.h:4374) [-Wdeprecated-declarations]
       auto tensor = at::zeros(at::CPU(at::kFloat), {16, 16});
                                                            ^
/data1/bryanleoliu/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to vmsLog2'
collect2: error: ld returned 1 exit status
make[2]: *** [test/FileStoreTest] Error 1
make[1]: *** [test/CMakeFiles/FileStoreTest.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
[ 71%] Linking CXX executable TCPStoreTest
[ 76%] Linking CXX executable ProcessGroupMPITest
/data1/bryanleoliu/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to vmsLog2'
collect2: error: ld returned 1 exit status
make[2]: *** [test/TCPStoreTest] Error 1
make[1]: *** [test/CMakeFiles/TCPStoreTest.dir/all] Error 2
/data1/bryanleoliu/pytorch/torch/lib/tmp_install/lib/libcaffe2.so: undefined reference to vmsLog2'
collect2: error: ld returned 1 exit status
make[2]: *** [test/ProcessGroupMPITest] Error 1
make[1]: *** [test/CMakeFiles/ProcessGroupMPITest.dir/all] Error 2
Scanning dependencies of target c10d_cuda_test
[ 80%] Linking CXX static library libc10d_cuda_test.a
[ 80%] Built target c10d_cuda_test
make: *** [all] Error 2
Failed to run 'bash tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn nccl caffe2 nanopb libshm gloo THD c10d'




## System Info

`

- PyTorch or Caffe2:Pytorch
- How you installed PyTorch (conda, pip, source): source 
- Build command you used (if compiling from source):python setup.py install
- OS: centos 7
- PyTorch version:0.4.1
- Python version:3.6
- CUDA/cuDNN version: 9.0/7.3
- GPU models and configuration: 8 nvidia p40
- GCC version (if compiling from source):4.8.5
- CMake version:3.12.0
- Versions of any other relevant libraries: 


cc @malfet"
345,20165,0,"torch.nn.threshold cannot accept tensor as a threshold. ## 🚀 Feature
I think it will be useful if we can pass threshold as a tensor to the threshold function. in this way we can compute the backprop of the threshold tensor and use it in training process.

right now the threshold function just takes non-tensor threshold"
579,17048,0,"Can not subclass `subclass torch.multiprocessing.Pool()`. ## 🐛 Bug

Trying to subclass:

Ends up with:


This does makes sense, as it is not possible to subclass python's  as well - but only .  
 is **not** a type, but rather a context method:


The main problem is that  doesn't expose the  module - so it impossible to subclass  


This might be related to #16954 - but I don't think it's a dupe, as the main problem here is exposing the  module.

## To Reproduce

Steps to reproduce the behavior:

1. 

## Expected behavior
It should be possible to subclass torch's multiprocessing pool.

## Environment

PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04 LTS,
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: Tesla P100-PCIE-12GB
Nvidia driver version: 390.30
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.14.3
[pip3] torch==1.0.0
[pip3] torchvision==0.2.1
[conda] Could not collect"
526,28639,0,"Set CUDA device is expensive operation. ## 🐛 Bug

Calling cudart::cudaApiSetDevice appears to be a very expensive operation. However, as my system has only a single GPU, it is unnecessary to call it at all.

Tested on Windows.

![image](https://user-images.githubusercontent.com/61218/67546817-99188780-f740-11e9-9566-792c5eab40e4.png)


## To Reproduce

Steps to reproduce the behavior:

1. Run models with benchmarking turned on
2. Profile the application

## Expected behavior

Set device should not even be called on a single GPU device because device 0 will always be used anyway.

## Environment

 - PyTorch Version (e.g., 1.0): 1.3.0
 - OS (e.g., Linux): Windows x64 MSVC 2019
 - How you installed PyTorch (, , source): prebuilt off website
 - Build command you used (if compiling from source): N/A
 - Python version: N/A
 - CUDA/cuDNN version: 10.1/7.6
 - GPU models and configuration: GTX 2060
 - Any other relevant information:


cc @VitalyFedyunin @ngimel @mruberry"
613,1528,0,"LongTensor indexing with duplication not propagating gradients. It seems like if we use LongTensor indexing to duplicate entries of a Variable, this breaks autograd. Specifically, if any entry in a vector is duplicated, backward() only maintains gradients for the last copy of the element. Simple example:



However:


This is version under version 0.1.11+fc48d2c, tested on 0.1.11+9f3119a as well.  This happens for both cuda and cpu tensors.
"
537,675,0,"[build/nccl] failed to build libnccl on Debian unstable. Failed to build CUDA version of pytorch (without CUDNN) with the latest source.

OS: debian unstable/experimental
Compiler: gcc-5, g++-5
CUDA: 8.0.44 (package provided by Debian)

buildlog: http://debomatic-amd64.debian.net/distribution#experimental/pytorch-contrib/0.1.7~1/buildlog



I have no idea about this ..."
359,14366,0,"How to use model.net.Clip?. I want to clip loss using Clip op like this model.net.Clip([input,output],  0,10)  but it dosen't work!!
the Operators Catalog of caffe2 is too simple. Does anyone can help me? Thank you very much!!!"
136,26142,0,"TorchScript fails to compile methods with misindented comments. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Reproducer:




cc @suo"
690,4651,0,"distributed pytorch in cluster. How can I launch a  distributed pytorch code in the cluster which has two nodes?

Suppose each node has only one gpu.

def init_processes(rank, size, fn, backend='gloo'):
    dist.init_process_group(backend,init_method='tcp://172.16.1.186:2222', rank=rank,world_size=size)
    fn(rank, size)

if __name__ == ""__main__"":
    size = 2
    processes = []
    for rank in range(size):
        p = Process(target=init_processes, args=(rank, size, run))
        p.start()
        processes.append(p)

   for p in processes:
        p.join()

This is part of my code.I run it.However,the result is only run in the one node when I qsub the run command in the two nodes.

The part of pbs command is nodes=gpu06:ppn=24+gpu07:ppn=24.

The run command is python train_dist.py.

Is there something wrong?Thanks for your help."
484,20048,0,"Forking is not possible anymore when using any PyTorch version from about 2 months ago. I have been building PyTorch from source in the past couple of months. Since around 1 (or maybe 2) month(s) ago I am unable to do forward passing to my models when using the  package of PyTorch. If I'm not mistaken, the last version of PyTorch that I built and still allows me to use ing is [this](https://github.com/pytorch/pytorch/tree/bad4442) one; or more precisely, the version of PyTorch that I am using now and allows me to do ing is .

Not being able to do ing is pretty annoying cause I have written all of my work flow in a way that it depends on using  with ing. In one part of my work flow I start tens of processes and do forward passing on a model that is on the CPU memory. This is pretty useful and super efficient since the weights of the model are not being copied every time I start a new process. However, if I  processes the weights of the model needs to get copied and it's not gonna be efficient anymore. 

In another part of my framework, I start a process while the model is still on CPU's memory and then do  within the process and the model is then copied to the GPU memory. This is a bit inefficient but still allows me to start a 4-5 processes (depending on the GPU memory) and do forward-pass my inputs. 

Here's a pseudocode of what I do:



With the current versions of PyTorch (since 1-2 months ago) I cannot do this anymore. I'm afraid this is happening due to some indirect affects of some commits which did not intend to disable ing. So I wonder if you guys can either revert the changes that have caused this or enable this feature again.

Also, when using some of the PyTorch versions from the master branch (since 1-2 months ago) I might get the following if I do  within an started process:



This might be somewhat relevant to [lazy initialization of CUDA](https://github.com/pytorch/pytorch/pull/2811)."
120,6894,0,"Maybe a bug in torch.nn.Upsample. ## Issue description

An unfriendly exception output when the input data size (the last two dims) and Upsample param 'size' have the same value

## Code example


and the output is:
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python': free(): invalid pointer: 0x00007fbea6fdfd40 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7fbed7b017e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7fbed7b0a37a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7fbed7b0e53c]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/lib/python3.6/site-packages/torch/_thnn/_THNN.cpython-36m-x86_64-linux-gnu.so(+0x2f367)[0x7fbe764f6367]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python(_PyCFunction_FastCallKeywords+0x279)[0x4aefe9]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python[0x54060e]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python(_PyEval_EvalFrameDefault+0x102d)[0x54268d]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python[0x540275]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python(PyEval_EvalCodeEx+0x3e)[0x54118e]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python[0x485828]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/bin/python(PyObject_Call+0x5c)[0x45322c]
/home/USER/.pyenv/versions/3.6.4/envs/shinerio-python3.6.4-env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_Z17THPFunction_applyP7_objectS0_+0x271)[0x7fbea5ae7ac1]

... ...

7fbed899f000-7fbed89a6000 r--s 00000000 08:02 17172406                   /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache
7fbed89a6000-7fbed89a7000 r--p 00025000 08:02 8912913                    /lib/x86_64-linux-gnu/ld-2.23.so
7fbed89a7000-7fbed89a8000 rw-p 00026000 08:02 8912913                    /lib/x86_64-linux-gnu/ld-2.23.so
7fbed89a8000-7fbed89a9000 rw-p 00000000 00:00 0 
7ffc0354d000-7ffc0356e000 rw-p 00000000 00:00 0                          [stack]
7ffc03575000-7ffc03578000 r--p 00000000 00:00 0                          [vvar]
7ffc03578000-7ffc0357a000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

Process finished with exit code -1

`


## System Info

-   PyTorch version: 0.3.1
    Is debug build: No
    CUDA used to build PyTorch: 8.0.61
-   OS: Ubuntu 16.04.2 LTS
    GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
    CMake version: version 3.5.1

-   Python version: 3.6
    Is CUDA available: Yes
    CUDA runtime version: Could not collect
    GPU models and configuration: GPU 0: Tesla M40 24GB
    Nvidia driver version: 384.111
    cuDNN version: Probably one of the following:
    /usr/local/cuda-9.0/lib64/libcudnn.so
    /usr/local/cuda-9.0/lib64/libcudnn.so.7
    /usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
    /usr/local/cuda-9.0/lib64/libcudnn_static.a

-   Versions of relevant libraries:
    [pip3] numpy (1.14.2)
    [conda] Could not collect
"
98,18053,0,"cuDNN error when using 3d convolutions. ## 🐛 Bug

The following simple script: https://gist.github.com/vlasenkov/b3aa7c12570fe0056fca3421453470ca crashes with the following traceback:




## To Reproduce

Run the code above on single GPU.

## Expected behavior

The script just successfully finishes.

## Environment

"
205,24826,0,"Transformer Lack of Embedding Layer and Positional Encodings. The Transformer implementation docs (https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer) state that they implement the original paper but fail to acknowledge that they don’t implement the following: 
* Layer Norm as the default normalization option. 
* Positional Encodings
* Embeddings before the encoding and decoding step 

It’s fine that these are all not implemented directly in the module but making it more clear that they aren’t and were in the original paper would be helpful. 

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @brianjo @mruberry @albanD @zhangguanheng66 @jlin27"
69,9592,1,"nn.Conv* incostintent error depending on stride. ## Issue description

Convolutions do not raise an exception for too small input shape if .

## Code example

Suppose we have a following convolution:

This code raises a . Which is quite expected.

Now change the  to 2:

Which gives . Is this expected behavior? 

I guess this is because in the formula for the output shape there is a division by the stride and the check is performed before taking the  operation.

## System Info
- How you installed PyTorch: conda
- OS: Linux
- PyTorch version: 0.4
- Python version: 3.6

The code was run on CPU."
699,23616,0,"[jit] string.split only splits on newlines. It should split on any whitespace, right now it just splits on 


outputs



cc @suo"
716,6486,0,Where is the Caffe2 website?. The gh-pages branch doesn't exist.
101,16527,0,"Delete _cudart from torch/cuda/__init__.py. _cudart is the Python ctypes binding to the libcudart.so library. It was previously used for Streams and Events but is now unused. The initialization (in _load_cudart) can sometimes fail. For example, @zou3519 ran into:

"
89,2520,1,"DistributedDataParallel doesn't exit properly; Leaves reduction threads running. 
When using the DDP module the process always hangs. I believe because the reduction threads are in a  loop.

https://github.com/pytorch/pytorch/blob/4c69697d2acbbe8e418a0921464c09aaf09da82a/torch/nn/parallel/distributed.py#L313-L315

Repro Code:

"
432,2032,0,"Possible bug in RNN module. 
...................................................................................................

Sorry, I have to reopen this because I think this is a bug within the RNN module and I could't get help from discuss.pytorch

I run a similar experiment with one linear layer neural network and the output is correct. So a reasonable explanation would be the RNN module keep different set of parameters (for optimization reason?) and didn't mix up at the end of the batch training. The following code will duplicate this problem.

........................................................................................................

I encounter this problem when I am trying to implement seq2seq to familiarize with this new framework. This issue seems related to parameters sharing in mini-batch. I setup a dummy training set with only one mini-batch. This mini-batch has 3 data entries in it. All with the same input, and different outputs:

training_data = [
[[[4,5,1,7,8],[4,5,1,7,8],[4,5,1,7,8]], (input 1)
[[0],[0],[0]], (input 2)
[[1],[3],[5]]] (target)
]

In theory, the model will never learn this data because the contradiction. However, the loss reach near 0 after only a few hundred epochs. But if I split 3 data entries into 3 mini-batch, the model will not learn the data set which should be the correct result.

So the model must be keeping different set of parameter for each position in mini-batch? And the parameters are not updated to be the same after each mini-batch forward-backward? Can someone tell me if I misunderstood something?

"
336,20997,0,"ReduceLROnPlateau will fail when add new parameter group to the optimizer. ## 🐛 Bug

ReduceLROnPlateau will fail when add new parameter group to the optimizer.
The  function will raise list index out of range error

## To Reproduce

Steps to reproduce the behavior:

1. initialize optimizer: 
1. initialize scheduler: 
1. add parameter group to the optimizer: 
1. raise error when the learning rate should be changed

## Expected behavior

when add a parameter group to the optimizer, the  attribute of the scheduler should be updated to avoid this error.

## Environment

 - PyTorch Version (1.1.0):"
344,9945,0,WERROR=1 doesn't work with FULL_CAFFE2. It seems to toggle too many warnings for Caffe2 and then  fails.
339,15163,0,"doc error:torch.nn.NLLLoss. if size_average=True and reduce=True,
loss(x,class)=−weights[class]∗x[class]/N"
66,18277,1,"Precision loss of large tensor averaging on CPU. ## 🐛 Bug

Taking  of a large CPU tensor results in severe precision loss. 
I would like to know if the following behaviour is known and normal. 

## To Reproduce


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
produces 




## Expected behavior

Close match between CPU and GPU versions of mean operation. 

## Environment

PyTorch version: 1.0.1.post2
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 390.116
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.16.2
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.1                      144  
[conda] mkl-service               1.1.2            py37he904b0f_5  
[conda] mkl_fft                   1.0.6            py37hd81dba3_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] pytorch                   1.0.1           py3.7_cuda9.0.176_cudnn7.4.2_2    pytorch
[conda] torchvision               0.2.1                      py_2    pytorch

"
2,13843,1,"Possible CPU-side memory leak even when fitting on GPU. ## 🐛 Bug

A possible CPU-side memory leak even when fitting on the GPU using PyTorch 0.4.1.

## To Reproduce

I am quite new to PyTorch, having used TF/Keras extensively in the past, but am now trying to use PyTorch as a replacement. I decided to start small with a seq2seq Skip-Thought model, cobbled together using the PyTorch NLP tutorials. Everything seems to work fine when I run small scale tests, however, when I use the code to run a large scale fit on 3000 separate paragraphs (each paragraph having a variable number of sentences) I notice that my system RAM usage slowly goes up as the script runs, until eventually it hits 100% and the box becomes unresponsive and has to be force rebooted. The Linux box has 64 GB of RAM, and when the script starts, usage is 4.7% but this climbs steadily over time to 100%, at which point the box becomes unresponsive.

Since I'm new to PyTorch, I'm not sure if perhaps I'm doing something blatantly wrong in my code which would account for this behaviour?

This is my core fitting logic, which runs as a script on linux:



Here are the helper functions which create the tensors passed to the model by converting all text words into indices from a predefined gensim dictionary:



And, finally, here is the SkipThought model itself, with all its helper classes (apologies for the wall of code):



## Expected behavior

Memory usage in Linux should not linearly increase as the fitting script runs, especially not to the point at which the box dies.

## Environment

 - PyTorch Version (e.g., 1.0): 0.4.1
 - OS (e.g., Linux): Linux (Ubuntu 16.04)
 - How you installed PyTorch (, , source): pip install torch
 - Build command you used (if compiling from source): N/A
 - Python version: 3.6.6
 - CUDA/cuDNN version: CUDA 9.0.176 / CuDNN 7.4.1.5
 - GPU models and configuration: Tesla V100
 - Any other relevant information:

"
625,987,0,"Implement nn.Module.__dir__. If I create a module class and add a layer to it, I can't see this new layer when I try to use IPython's autocomplete. For example, with the the following class:

    class Example(nn.Module):
        def __init__(self):
            super().__init__()
            self.fully_connected = nn.Linear(20, 1)

    ex = Example()

When I type , the only options are 'forward' and 'float'. And when I type , I don't get the completion option for the  attribute.

From my limited understanding, I think that this has to do with the strings that are returned with the  method:

    [attr for attr in ex.__dir__() if attr.startswith('f')]
    => ['forward', 'float']
    
So for the first issue, if I modify the class to have a method

    def __dir__(self):
        return super().__dir__() + list(self._modules)
        
then I get the  as a possible completion.
    
I'm not sure if this is because of an intentional design decision, but it would be nice if all of the relevant possible attributes were available with the auto complete, as I'm still becoming familiar with this library.

"
335,14112,0,"[sparse] add descriptions and examples for methods at torch.sparse doc page. - There is no descriptions / examples for the methods listed at at doc page
- It will be much easier for new comers to learn how to use sparse if we improve doc a little bit

cc @aocsa @nikitaved @pearu @mruberry @IvanYashchuk"
395,1981,0,"[Controversial] `pyyaml` is 6 years old, might be best to switch to `ruamel.yaml`?. Commit history from pyyaml, as far as I can tell, http://pyyaml.org/log/ 

<img width=""733"" alt=""screen shot 2017-07-05 at 10 34 54 am"" src=""https://user-images.githubusercontent.com/123560/27858562-9e33dc44-616d-11e7-9525-0cf771d24b31.png"">

Commit history from , https://bitbucket.org/ruamel/yaml/commits/all

<img width=""1355"" alt=""screen shot 2017-07-05 at 10 35 36 am"" src=""https://user-images.githubusercontent.com/123560/27858592-bd36de8e-616d-11e7-8213-548f81909bd2.png"">

ruamel.yaml supports yaml 1.2, and handles round-trip, including comments."
744,6996,0,"[pytorch][0.4][bug] ""torch.min"" and ""torch.max"" ignores ""nan"" in cuda case and crash with pure ""nan"" tensor. ## Issue description

 and  ignores  in cuda case and crash with pure  tensor.

## Code example

## System Info

- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): conda
- Build command you used (if compiling from source):
- OS: Ubuntu 16.04
- PyTorch version: 0.4.0
- Python version: 3.6.3
- CUDA/cuDNN version: 8 / 5
- GPU models and configuration: GTX 1050 Ti
- GCC version (if compiling from source):
- CMake version:
- Versions of any other relevant libraries:
"
385,12555,0,"add nvidia driver version to environment collection script. https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py currently misses collecting nvidia driver version.
Would be useful to have that."
388,12608,0,"ONNX model export issue. Hi,

I would like to get some general ideas on exporting ONNX model when model accepts both input sequence and output labels as arguments. How would you set up Variables to traverse through the model to export ONNX?

Thanks.
"
12,20053,1,"RFC: accscalar_t for float on CPU. ## Issue
Currently  for  is  on the CPU:

https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/AccumulateType.h#L34

I suggest to have a small discussion whether it'd be better to switch to float.

## Motivation

This is prompted by @wanchaol asking about a [](https://gist.github.com/wanchaol/aceb0a1e8d3a93c8853689730b0f709f) error, but I think there are three possible reasons to consider changing this:
- Consistency with GPU,
- support platforms on which float is faster (e.g. arm32)
- get rid of UB.

I seem to recall @apaszke preferring the current behaviour a year ago or so back (in the context of #6855, which always dispatched a double auxilliary function on CPU or so).

## Pitch

Switch to  generally.

## Alternatives

Switch to  only for specific platforms (e.g. arm32),  somehow get rid of the UB warning.

## Additional context

We (e.g. @ljk53 and me) might be interested in changing this for Android specifically if we don't generally.
"
419,7645,0,"Max pooling behavior for nan values. ## Issue description

max pooling functions are not consistent with max functions.
Below an example, every max pooling (be it 1d, 2d or 3d, adaptive or not) acts the same, on cpu or on cuda.

Essentially, there are two fondamental differences :
 - max pooling of all  values is  while for  it's 
 - max pooling of nan and valid values is valid values, which means s get ignored, while for , as soon as there is a  value, the result is .

More generally, choosing explicetely how to deal with  as in numpy ([e.g.](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.nanmax.html)) could be a solution, but maybe this is related to CuDNN's max pooling ?

## Code example



## System Info

Built from latest sources (as of 05/17)
PyTorch version: 0.5.0a0+331a04d
Is debug build: No
CUDA used to build PyTorch: 9.1.85

OS: Ubuntu 16.04.4 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
CMake version: version 3.9.4

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
GPU models and configuration: GPU 0: Quadro M1000M
Nvidia driver version: 390.30
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3
/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a

Versions of relevant libraries:
[conda] magma-cuda91              2.3.0                         1    pytorch
[conda] torch                     0.5.0a0+331a04d           <pip>
[conda] torch                     0.3.1b0+2b47480           <pip>
[conda] torch                     0.5.0a0+4251e38           <pip>

"
74,27016,1,"inconsistent of einsum and torch.mm. 
## Issue description
When comparing the outcomes of torch.mm and torch.einsum for matrix multiplication, the results is not consistent. The same result is also produced by numpy. 

## Code example
A minimal example is down here. 


[[ True  True  True  True]
 [ True  True  True  True]
 [ True  True  True  True]
 [ True  True  True  True]]
[[ True  True  True  True]
 [ True  True  True  True]
 [ True  True  True  True]
 [ True  True  True  True]]
tensor([[ True,  True,  True,  True],
        [False,  True, False,  True],
        [ True, False,  True,  True],
        [False,  True,  True,  True]])
[[ True  True  True  True]
 [False  True False  True]
 [ True False  True  True]
 [False  True  True  True]]

While of course we should expect all trues.

## System Info
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Red Hat Enterprise Linux release 8.0 (Ootpa)
GCC version: (GCC) 8.2.1 20180905 (Red Hat 8.2.1-3)
CMake version: version 3.11.4

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
Nvidia driver version: 430.14
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.2
[pip] numpydoc==0.8.0
[pip] torch==1.2.0
[conda] _pytorch_select           0.2                       gpu_0  
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      243  
[conda] mkl-service               2.3.0            py37he904b0f_0  
[conda] mkl_fft                   1.0.10           py37ha843d7b_0  
[conda] mkl_random                1.0.2            py37hd81dba3_0  
[conda] pytorch                   1.2.0           cuda100py37h938c94c_0

"
291,31596,0,"pytorch forward hangs in multiprocess environment. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
This a very peculiar bug I am encountering. I am using MTCNN face detector from https://github.com/TreB1eN/InsightFace_Pytorch/  on pytorch 1.3.1+cpu

The face detection works fine when API is called from __main__ thread. But if I forked a new process and initialized MTCNN as well as face detector model (MobileFaceNet), it hangs in first net forward call (in MTCNN). If load_state_dict for MobileFaceNet is commented out, again it works fine.

## To Reproduce

Steps to reproduce the behavior:

1. Clone https://github.com/TreB1eN/InsightFace_Pytorch/
2. Try following code

3. If test function is called in __main__ thread, this code works fine.
4. If  line is removed, again code works fine

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Environment
pkg | version
--- | ---
Python | 3.7.4
torch |                            1.3.1+cpu   (pip)
Pillow |                           5.4.1      
opencv-python |                    4.1.1.26   
OS | ubuntu 18.04, linux 4.15.0-72-generic
"
610,7660,0,"Canonicalize aten operators in IR. Currently many operators in the aten IR have multiple forms.

Any operator that takes a non-tensor argument (e.g. IntList or int64_t) will have two forms: one with the non-tensor arguments encoded as tensor inputs and one with the arguments as attributes on the graph.

This makes every pass that we write in the IR harder because it needs to handle both cases.

Proposed solution:

1. Canonicalize on the tensor input format. Graphs are optimized in this format. This format is more general because inputs are not required to be constants.

2. expose the  function in compiler for general usage. Optimizations then check for const-ness of these inputs to work.

3. Before interpretation, lower back to the attribute-only format when thing are constants.  already includes this function, it just has to be written into a pass."
250,9172,0,"Kernel breaks  when doing backward pass with .permute with the -1 option. ## Issue description

Permuting the axis of a tensor using the  option breaks the kernel (tested in Jupyter) when doing the backward pass. The kernel just restart, there is no error provided.

## Code example

This code:

automatically breaks the Kernel of a Jupyter Notebook (run in JupyterLab).

Note that the following code:


worked well.

## System Info
PyTorch version: 0.3.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 16.04.3 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
CMake version: version 3.5.1

Python version: 3.5
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: GPU 0: Tesla K80
Nvidia driver version: 384.111
cuDNN version: Probably one of the following:
/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21
/usr/local/cuda-8.0/lib64/libcudnn_static.a
/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5
/usr/local/cuda-9.0/lib64/libcudnn_static.a
/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-7b07b0f1.so.7.0.5

Versions of relevant libraries:
[pip3] msgpack-numpy (0.4.1)
[pip3] numpy (1.14.0)
[pip3] torch (0.3.1)
[pip3] torchvision (0.2.0)
[conda] Could not collect"
186,27793,0,"[quantization] QNNPACK Engine for Dynamic Quantization. We should support QNNPACK for dynamic quantization.

- [ ] RNN classes
- [ ] RNNCell classes
- [ ] Linear

cc @jerryzh168 @jianyuh @dzhulgakov"
316,1051,0,"DataParallel does not correctly handle running variables in batch norm. Running variables accumulated on non-master GPUs are lost, and in the next step running variables accumulated on the master GPU are broadcast to non-master GPUs. Convergence-wise it does not make much difference;-), but it is still a bug. 
Also, device_ids in DataParallel should be checked to not contain the same values, otherwise stuff like this will break: 
https://github.com/pytorch/pytorch/blob/master/torch/backends/cudnn/rnn.py#L38-L43
If creating replicas on the same GPU is actually intended to be supported, then the above referenced lines that create a dictionary indexed by current_device constitute a bug. 
Both this issues ultimately are caused but not well defined behaviour of DataParallel with respect to stateful modules, of which batchnorm and recurrent nets with cudnn backend are examples. 

cc @ngimel @albanD @mruberry"
651,28201,0,"Optimize GroupNorm in PyTorch. ## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
Improve the performance of GroupNorm operator.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

Similar as https://github.com/pytorch/pytorch/issues/27633, the current GroupNorm implementation is reshaping the input and doing BatchNorm to get the moments of input, then using addcmul for affine. This implementation is inefficient for both CPU and CUDA. 

The performance benchmark for input shape = [128, 256, 28, 28], num_groups = 32 is shown below.



And the performance benchmark for input shape = [256, 512, 56, 56], num_groups = 32 is shown below.



We can see that for both CPU and GPU version of GroupNorm, using BatchNorm with addcmul make things slow especially for backward pass. Actually on CPU side, since BatchNorm for inference is a affine function and can be fused with Conv, it makes the GroupNorm very slow when using BatchNorm on CPU for inference.

## Pitch

<!-- A clear and concise description of what you want to happen. -->

To implement a optimized version of GroupNorm which fused everything together.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->
cc @dzhulgakov @ngimel @ppwwyyxx 
"
487,2011,0,"Build passing, but runtime failing (at commit ebdec9a837074a303fd5ffb6f319cd593955becc). Running import torch gives the following error on the latest master:



# git log
commit ebdec9a837074a303fd5ffb6f319cd593955becc
Author: lynic <xuanlangjian@gmail.com>
Date:   Fri Jul 7 23:06:56 2017 +0800

    Skip distributed tests if not supported (#2004)
"
514,140,0,"Index / SetValue don't work for 1D Variables. Snippet to reproduce.



The problem is in [here](https://github.com/pytorch/pytorch/blob/master/torch/autograd/functions/tensor.py#L27), [here](https://github.com/pytorch/pytorch/blob/master/torch/autograd/functions/tensor.py#L40) and [here](https://github.com/pytorch/pytorch/blob/master/torch/autograd/functions/tensor.py#L45), and is due to the fact that  return numbers when indexing a 1D tensor (which I think is the desired behaviour as we don't have broadcasting yet?).

Maybe we should check if the input is 1D (and that the index is of range 1) and to use  instead of  in this case?
Autograd  return a 1D tensor anyway if we index it with a 1D tensor in forward, but this solution could avoid the sync point with cuda tensors in  (as we don't return a number).

Or, another solution would be to have something like 0D tensors which behave almost like numbers? This seems to be what numpy does.
"
255,22277,0,Handle all IntArrayRef expansions in ATen. See #22032 and #20866 for the rationale and #22073 for an example.
112,24608,0,"Migrate `nll_loss2d_forward` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
678,30989,0,"Does Distributed Data Parallel Support model parallel where output of the model are in different devices?. By following the tutorial, I combine the model parallel and DDP together but the last layer(the parameter is really large) is doing some model parallel work like this code [source](https://github.com/bindog/pytorch-model-parallel/blob/8d1c0b91d4eb75c861059f4940159b06574189df/model.py#L39). However, it stuck at here



the gpu list for first DDP is [0, 1, 2, 3]
the gpu list for second DDP is [4, 5, 6, 7]
I modify the source code to make sure the chunk follow the parameter of gpu list



If it does not support output on multiple device, how can I reorganize the last layer?

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528"
71,23425,1,"Hanging on when one gpu node return zero as loss in the context of distributed data parallel training. ## 🐛 Bug

Hello.
I am training a multi task network on a cluster using 8 gpu with .
On every iteration, I process one image per gpu.
Suppose I have  and  with  and  respectively.
In some case, the  could be zero on one gpu, which means that there is no loss computed.
Then I just return the zero tensor.
When this situation happens, the code just hang on.

 - PyTorch Version: 1.1.0 
 - OS (e.g., Linux): Ubuntu 
 - How you installed PyTorch (, , source): docker conda
 - Python version: 3.6
 - CUDA/cuDNN version:  CUDA 9.0
 - GPU models and configuration: 1080ti
 - Any other relevant information:

"
150,12210,0,"PackagesNotFoundError: unable to install torchvision in anaconda prompt on windows 10. 

returns the following:

"
143,26726,0,"[jit] Recursive script doesn't pick up TorchScript class attributes. 

cc @suo"
571,2369,0,"Calculate the derivative of a node with respect to an input image. 
I would like to calculate the derivative of any node in respect of image input to implement visualization methods using PyTorch.

Any suggestions ? "
588,27507,0,"[ONNX] Export torch.meshgrid. ## 🚀 Feature
Export meshgrid to ONNX

## Motivation

Instead of an error:



Can use  and 

## Pitch

Use  and  to export meshgrid

## Alternatives

Use  and  in the code

"
334,14672,0,"Inconsistent behavior for log_prob when values are outside of support. ## 🐛 Bug

In Scipy, if you try to calculate the log probability of a value outside of the given distribution's support, scipy will return .

The Uniform Distribution for torch follows this behavior, as shown below:

However, this behavior does not extend to other distributions that have a constrained support, such as the exponential, beta, and gamma, or discrete distributions such as the geometric. 









These four are the distributions that I've tested; I'm unsure if there are more.  Is this discrepancy in behavior expected?  Please let me know if I've missed something.

### System Info
Collecting environment information...
PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.0.176

OS: Ubuntu 18.10
GCC version: (Ubuntu 8.2.0-7ubuntu1) 8.2.0
CMake version: version 3.12.1

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 960M
Nvidia driver version: 415.13
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy (1.15.4)
[pip] numpydoc (0.8.0)
[pip] torch (0.4.1)
[conda] pytorch                   0.4.1            py37ha74772b_0


cc @fritzo @neerajprad @alicanb @nikitaved"
294,3990,0,"Raise an error when using magma built against wrong version of cuda. Previous instances of this: https://github.com/pytorch/pytorch/issues/3018

I myself was affected by this bug when a server I was working on upgraded from cuda 8 to cuda 9. I dutifully rebuilt PyTorch but forgot to uninstall magma-cuda80: instant hang on CUDA initialization."
182,4655,0,"Error with cuda9 docker build. When building pytorch with docker with 


I get the following error and the build stops:


I cloned the latest master with 
Ubuntu 16.04
Docker version 17.09.1-ce, build 19e2cf6"
564,956,0,"load_lua yields ""object has no attribute 'running_var'"" for BatchNormalization. When I try to load OpenFace model (https://storage.cmusatyalab.org/openface-models/nn4.small2.v1.t7) as:

    model = load_lua('nn4.small2.v1.t7',unknown_classes=True),

then, to convert unknown classes. 
But, I got the errors at the line below:

torch/utils/serialization/read_lua_file.py"", line 269, in BatchNorm_reader
    obj.running_var = obj.running_var.pow(-2).add(-obj.eps)

Thank you.
"
331,29692,0,[FR] add generator= kwarg support for torch.randn and torch.rand. All stochastic functions should accept . These should as well.
670,12241,0,"torch.sum() for sparse tensor. - Add 
- Add 
- Add autograd support"
380,20056,0,"Creation of too big multidimensional array returns empty tensor.. ## 🐛 Bug

When trying to create a tensor of too many dimensions it simply returns an empty tensor with its shape is the dimensions I passed even though it should have contained the value .

## To Reproduce

Steps to reproduce the behavior:

Number of 2's in the following example is .

Output: 



Another related issue



These are  2's. 

Output: 



When they're  2's. The output changes to:



<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

I expected to see the aforementioned dimensions filled with  if it's possible, or an exception that says a value error.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


## Additional context

I suspect that the desired behavior is not happening because tensors seem to be basing on NumPy  which accepts only  dims.

Also, I have just found this [line](https://github.com/pytorch/pytorch/blob/7ddd5d06ed07a50b94aa6b2fdffa2f667d677c4b/aten/src/TH/THGeneral.cpp#L184).



I think the integer division of those two elements results in the error. "
551,17971,0,"CuDNN error: CUDNN_STATUS_EXECUTION_FAILED. ## 🐛 Bug
![2019-03-13 20-11-29](https://user-images.githubusercontent.com/12708080/54278734-2d791400-45ce-11e9-8926-64ffb9c5c2ed.png)
![2019-03-13 20-12-06](https://user-images.githubusercontent.com/12708080/54278743-34078b80-45ce-11e9-90ef-f2291ef9b95e.png)


## To Reproduce

Steps to reproduce the behavior:
Implementing such a network like this (or change from https://github.com/DrSleep/light-weight-refinenet  +  https://github.com/DrSleep/multi-task-refinenet)

![2019-03-13 20-39-56](https://user-images.githubusercontent.com/12708080/54279505-310d9a80-45d0-11e9-803a-0c7b5eeffb01.png)
![2019-03-13 20-39-35](https://user-images.githubusercontent.com/12708080/54279506-310d9a80-45d0-11e9-8429-bb7dfc595c70.png)
![2019-03-13 20-38-45](https://user-images.githubusercontent.com/12708080/54279508-31a63100-45d0-11e9-9463-71bd9e3ea21b.png)
![2019-03-13 20-38-34](https://user-images.githubusercontent.com/12708080/54279509-31a63100-45d0-11e9-87ee-bdd67041d22d.png)


and replace datasets.py to:
class Pad(object):
    """"""Pad image and mask to the desired size

    Args:
      size (int) : minimum length/width
      img_val (array) : image padding value
      msk_val (int) : mask padding value

    """"""
    def __init__(self, size, img_val, msk_val, dpt_val=None):
        self.size = size
        self.img_val = img_val
        self.msk_val = msk_val
        if dpt_val is None :
            self.dpt_val = msk_val
        else:
            self.dpt_val = dpt_val

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        h, w = image.shape[:2]
        h_pad = int(np.clip(((self.size - h) + 1)// 2, 0, 1e6))
        w_pad = int(np.clip(((self.size - w) + 1)// 2, 0, 1e6))
        pad = ((h_pad, h_pad), (w_pad, w_pad))
        image = np.stack([np.pad(image[:,:,c], pad,
                         mode='constant',
                         constant_values=self.img_val[c]) for c in range(3)], axis=2)
        mask = np.pad(mask, pad, mode='constant', constant_values=self.msk_val)
        depth = np.pad(depth, pad, mode='constant', constant_values=self.dpt_val)
        return {'image': image, 'mask': mask, 'depth': depth}

class RandomCrop(object):
    """"""Crop randomly the image in a sample.

    Args:
        output_size (tuple or int): Desired output size. If int, square crop
            is made.
    """"""

    def __init__(self, crop_size):
        assert isinstance(crop_size, int)
        self.crop_size = crop_size
        if self.crop_size % 2 != 0:
            self.crop_size -= 1

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        h, w = image.shape[:2]
        new_h = min(h, self.crop_size)
        new_w = min(w, self.crop_size)
        top = np.random.randint(0, h - new_h + 1)
        left = np.random.randint(0, w - new_w + 1)
        image = image[top: top + new_h,
                        left: left + new_w]
        mask = mask[top: top + new_h,
                    left: left + new_w]
        depth = depth[top: top + new_h,
                    left: left + new_w]
        return {'image': image, 'mask': mask, 'depth': depth}

class ResizeShorterScale(object):
    """"""Resize shorter side to a given value and randomly scale.""""""

    def __init__(self, shorter_side, low_scale, high_scale):
        assert isinstance(shorter_side, int)
        self.shorter_side = shorter_side
        self.low_scale = low_scale
        self.high_scale = high_scale

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        min_side = min(image.shape[:2])
        scale = np.random.uniform(self.low_scale, self.high_scale)
        if min_side * scale < self.shorter_side:
            scale = (self.shorter_side * 1. / min_side)
        image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)
        mask = cv2.resize(mask, None, fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST)
        depth = cv2.resize(depth, None, fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST)
        return {'image': image, 'mask': mask, 'depth': depth}

class RandomMirror(object):
    """"""Randomly flip the image and the mask""""""

    def __init__(self):
        pass

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        do_mirror = np.random.randint(2)
        if do_mirror:
            image = cv2.flip(image, 1)
            mask = cv2.flip(mask, 1)
            depth = cv2.flip(depth, 1)
        return {'image': image, 'mask' : mask, 'depth': depth}

class Normalise(object):
    """"""Normalise a tensor image with mean and standard deviation.
    Given mean: (R, G, B) and std: (R, G, B),
    will normalise each channel of the torch.*Tensor, i.e.
    channel = (channel - mean) / std

    Args:
        mean (sequence): Sequence of means for R, G, B channels respecitvely.
        std (sequence): Sequence of standard deviations for R, G, B channels
            respecitvely.
    """"""

    def __init__(self, scale, mean, std):
        self.scale = scale
        self.mean = mean
        self.std = std

    def __call__(self, sample):
        image = sample['image']
        return {'image': (self.scale * image - self.mean) / self.std, 'mask' : sample['mask'], 'depth': sample['depth']}

class ToTensor(object):
    """"""Convert ndarrays in sample to Tensors.""""""

    def __call__(self, sample):
        image, mask, depth = sample['image'], sample['mask'], sample['depth']
        # swap color axis because
        # numpy image: H x W x C
        # torch image: C X H X W
        image = image.transpose((2, 0, 1))
        return {'image': torch.from_numpy(image),
                'mask': torch.from_numpy(mask),
                'depth': torch.from_numpy(depth)}

class NYUDataset(Dataset):
    """"""NYUv2-40""""""

    def __init__(
        self, data_file, data_dir, transform_trn=None, transform_val=None
        ):
        """"""
        Args:
            data_file (string): Path to the data file with annotations.
            data_dir (string): Directory with all the images.
            transform_{trn, val} (callable, optional): Optional transform to be applied
                on a sample.
        """"""
        with open(data_file, 'rb') as f:
            datalist = f.readlines()
        self.datalist = [(i,l,m) for i,l,m in map(lambda x: x.decode('utf-8').strip('\n').split('\t'), datalist)]
        self.root_dir = data_dir
        self.transform_trn = transform_trn
        self.transform_val = transform_val
        self.stage = 'train'

    def set_stage(self, stage):
        self.stage = stage

    def __len__(self):
        return len(self.datalist)

    def __getitem__(self, idx):
        #img_name = os.path.join(self.root_dir, self.datalist[idx][0])
        #msk_name = os.path.join(self.root_dir, self.datalist[idx][1])
        img_name = self.datalist[idx][0]
        msk_name = self.datalist[idx][1]
        dpt_name = self.datalist[idx][2]
        def read_image(x):
            img_arr = np.array(Image.open(x))
            if len(img_arr.shape) == 2: # grayscale
                img_arr = np.tile(img_arr, [3, 1, 1]).transpose(1, 2, 0)
            return img_arr
        image = read_image(img_name)
        mask = np.array(Image.open(msk_name))
        depth = np.array(Image.open(dpt_name))
        if img_name != msk_name:
            assert len(mask.shape) == 2, 'Masks must be encoded without colourmap'
        sample = {'image': image, 'mask': mask, 'depth': depth}
        if self.stage == 'train':
            if self.transform_trn:
                sample = self.transform_trn(sample)
        elif self.stage == 'val':
            if self.transform_val:
                sample = self.transform_val(sample)
        return sample



## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

conda install --yes --file requirements.txt

# This file may be used to create an environment using:
# $ conda create --name <env> --file <this file>
# platform: linux-64
atomicwrites=1.2.1=py36_0
attrs=18.2.0=py36h28b3542_0
backcall=0.1.0=py36_0
blas=1.0=mkl
ca-certificates=2018.03.07=0
certifi=2018.10.15=py36_0
cffi=1.11.5=py36he75722e_1
cloudpickle=0.6.1=py36_0
cudatoolkit=9.0=h13b8566_0
cudnn=7.1.2=cuda9.0_0
cycler=0.10.0=py36h93f1223_0
cython=0.29=py36he6710b0_0
cytoolz=0.9.0.1=py36h14c3975_1
dask-core=0.20.0=py36_0
dbus=1.13.2=h714fa37_1
decorator=4.3.0=py36_0
expat=2.2.6=he6710b0_0
fontconfig=2.13.0=h9420a91_0
freetype=2.9.1=h8a8886c_1
glib=2.56.2=hd408876_0
gst-plugins-base=1.14.0=hbbd80ab_1
gstreamer=1.14.0=hb453b48_1
icu=58.2=h9c2bf20_1
imageio=2.4.1=py36_0
intel-openmp=2019.0=118
ipython=7.1.1=py36h39e3cac_0
ipython_genutils=0.2.0=py36hb52b0d5_0
jedi=0.13.1=py36_0
jpeg=9b=h024ee3a_2
kiwisolver=1.0.1=py36hf484d3e_0
libedit=3.1.20170329=h6b74fdf_2
libffi=3.2.1=hd88cf55_4
libgcc-ng=8.2.0=hdf63c60_1
libgfortran-ng=7.3.0=hdf63c60_0
libpng=1.6.35=hbc83047_0
libstdcxx-ng=8.2.0=hdf63c60_1
libtiff=4.0.9=he85c1e1_2
libuuid=1.0.3=h1bed415_2
libxcb=1.13=h1bed415_1
libxml2=2.9.8=h26e45fe_1
markdown=2.6.8=py36_0
matplotlib=3.0.1=py36h5429711_0
mkl=2018.0.3=1
mkl_fft=1.0.6=py36h7dd41cf_0
mkl_random=1.0.1=py36h4414c95_1
more-itertools=4.3.0=py36_0
nccl=1.3.5=cuda9.0_0
ncurses=6.1=hf484d3e_0
networkx=2.2=py36_1
ninja=1.8.2=py36h6bb024c_1
numpy=1.15.3=py36h1d66e8a_0
numpy-base=1.15.3=py36h81de0dd_0
olefile=0.46=py36_0
openssl=1.0.2p=h14c3975_0
parso=0.3.1=py36_0
pcre=8.42=h439df22_0
pexpect=4.6.0=py36_0
pickleshare=0.7.5=py36_0
pillow=5.3.0=py36h34e0f95_0
pip=10.0.1=py36_0
pluggy=0.8.0=py36_0
prompt_toolkit=2.0.7=py36_0
ptyprocess=0.6.0=py36_0
py=1.7.0=py36_0
pycparser=2.19=py36_0
pydot=1.2.4=py36_0
pygments=2.2.0=py36h0d3125c_0
pyparsing=2.2.2=py36_0
pyqt=5.9.2=py36h05f1152_2
pytest=3.9.3=py36_0
pytest-runner=4.2=py36_0
python=3.6.6=h6e4f718_2
python-dateutil=2.7.5=py36_0
pytorch=0.4.1=py36_py35_py27__9.0.176_7.1.2_2
pytz=2018.7=py36_0
pywavelets=1.0.1=py36hdd07704_0
qt=5.9.6=h8703b6f_2
readline=7.0=h7b6447c_5
scikit-image=0.14.1=py36he6710b0_0
scipy=1.1.0=py36hfa4b5c9_1
setuptools=40.4.3=py36_0
sip=4.19.8=py36hf484d3e_0
six=1.11.0=py36_1
sqlite=3.25.2=h7b6447c_0
tk=8.6.8=hbc83047_0
toolz=0.9.0=py36_0
torchvision=0.2.1=py_2
tornado=5.1.1=py36h7b6447c_0
traitlets=4.3.2=py36_0
wcwidth=0.1.7=py36_0
wheel=0.32.2=py36_0
xz=5.2.4=h14c3975_4
zlib=1.2.11=ha838bed_2

condapip`, source): conda
 - Build command you used (if compiling from source): conda install --yes --file requirements.txt 
 - Python version: 3.6
 - CUDA/cuDNN version: 9.0/7.1.2
 - GPU models and configuration: GTX1070Ti
 - Any other relevant information:
"
700,21680,0,"Disable nondeterministic CTCLoss from cuDNN. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.I i updated pytorch version and ctc，use pytorch_nightly, but in my train ,nn.CTCloss() is still zero,so,i would like to ask if the version pytorch(nightly) has been solved this problem
1.
1.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0):
 - OS (e.g., Linux):
 - How you installed PyTorch (, , source):
 - Build command you used (if compiling from source):
 - Python version:
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
372,25034,0,"make add_module accept tuples as well or change containers(ModuleList, Sequential, etc) to allow this. ## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->

## Motivation
The API is not consistent with the way architectures are made. That is you can do some actions using a set of methods, but cant do the same actions using a slightly different version of the same methods. To be more clear for example, currently if you want to alter/use a part of an existing architecture you may use   like this : 
1. method 1 :    

2. or a slightly different version 

However, the very time you  try to use the named version of same method (i.e ), you no longer can do the same thing and will face the error : 

in the process of using only  method in such situations, you lose the information originally placed in the architecture, meaning you no longer have access to the modules using their names. 
The source of the issue lies in  the  method which interestingly accepts a name and a module by default. 

  
## Pitch
allow for ,  and alikes that accept modules, also accept named modules. meaning these two snippets be interchangeable : 

using modules only ()

using tuple ()




cc @SsnL"
22,15482,1,"GPU allocated memory is not released. I create CNN to work with images. This network is of the U-Net type for segmentation, which by its architecture saves intermediate calculations. This leads to the conclusion that with a small amount of memory you need to either reduce the size of the image, the number of layers, the size of the batch.
![default](https://user-images.githubusercontent.com/37583380/50356634-ccf4b080-0563-11e9-824a-44f7ef669087.png)
If you give the network a single image of a small size, the memory becomes slightly larger, which is logical.
![default](https://user-images.githubusercontent.com/37583380/50356532-5d7ec100-0563-11e9-95d2-9c7e27bd1558.png)
When you restart with the same image, the amount of allocated memory does not change, only the occupied part of the memory cache increases.
![default](https://user-images.githubusercontent.com/37583380/50356677-ed246f80-0563-11e9-8595-8eb10b651868.png)
Error text:
RuntimeError                              Traceback (most recent call last)
<ipython-input-25-8ba8cee8f3f0> in <module>()
      1 test = np.random.sample([1, 3, 1280, 1280]) * 0.5
----> 2 out = net(torch.FloatTensor(test).to(device))
      3 out.shape

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--> 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

<ipython-input-12-552150536d36> in forward(self, x)
     50         x = self.maxpool(d1)
     51 
---> 52         d2 = self.down2(x)
     53         x = self.maxpool(d2)
     54 

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--> 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\container.py in forward(self, input)
     90     def forward(self, input):
     91         for module in self._modules.values():
---> 92             input = module(input)
     93         return input
     94 

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--> 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

E:\Python\Anaconda\lib\site-packages\torch\nn\modules\activation.py in forward(self, input)
     48     @weak_script_method
     49     def forward(self, input):
---> 50         return F.threshold(input, self.threshold, self.value, self.inplace)
     51 
     52     def extra_repr(self):

E:\Python\Anaconda\lib\site-packages\torch\nn\functional.py in threshold(input, threshold, value, inplace)
    838         result = _VF.threshold_(input, threshold, value)
    839     else:
--> 840         result = _VF.threshold(input, threshold, value)
    841     return result
    842 

RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 2.00 GiB total capacity; 1.34 GiB already allocated; 5.20 MiB free; 12.75 MiB cached)

After a large image again give the network image smaller and again get the same error:
![default](https://user-images.githubusercontent.com/37583380/50356722-2230c200-0564-11e9-8fba-54855c20de9f.png)
Things with memory become like this:
![default](https://user-images.githubusercontent.com/37583380/50356785-67ed8a80-0564-11e9-8a9c-895b453410ba.png)

Without restarting jupyter notebook in which I work, the memory does not clear out.

"
667,685,0,"detach not working properly for stochastic variables. I'm not able to reproduce a simple example for now, will try to make one later. But basically I'm facing this issue in my code:



I have a variable , and doing  does not solve the issue. However,  solves it, which seems weird to me.."
119,13491,0,"Numerical ODE solvers. ## 🚀 Feature
It would be useful to have a library of numerical ODE solvers that are compatible with PyTorch.

## Motivation
Neural networks are increasingly being combined with ODEs. For example, when modelling ODEs that form part of a probabilistic model, or when defining a derivative with a neural network (neural ODEs).

You may want to get a speedup by using the GPU with PyTorch for this, and it's useful to be able to differentiate through these algorithms too.

## Pitch
I'm proposing to create a  module that contains half a dozen common ODE solvers. For instance, you could call  to calculate  points from  to  for the equation 
"
235,5331,0,"[ppc64le] FAIL: test_set_flush_denormal (__main__.TestTorch). hi,  I just saw this error in the last few days.  both for GPU and CPU only environment.
Is the test precision assertion correct ?


"
281,23512,0,"Build reconfiguration should consistently honor env variables. Per discussion here: https://github.com/pytorch/pytorch/pull/23323#issuecomment-515168182

We currently have

(plainly == no additional build options passed in to setup.py)

for clean tree:

- configuration only: 
- configuration + build: invoke setup.py plainly

For rebuild:

- reconfiguration without rebuild:  (unreliable), or edit CMakeCache.txt and run cmake directly
- rebuild without reconfiguration: invoke setup.py plainly, all build options persist
- reconfiguration + rebuild:  (unreliable), or edit CMakeCache.txt and invoke setup.py plainly

We need to make the two ""unreliable"" spots ""reliable""."
715,22017,0,"mkldnn_version.h: No such file or directory. I compiled pytorch under ubuntu18.04, gcc 7.4.0, and I git checkout v1.0.0 before compiling.
After the compiling process begins, I terminated at about 34% with the error:


So what's wrong with it and how can I solve this problem? Can anyone help me with this issue?
"
599,12946,0,"[jit][trace] non-argument tensors with requires_grad=True are traced as constants. ## 🐛 Bug

non-argument tensors with requires_grad=True are traced as constants

## To Reproduce

## Expected behavior

The output should have requires_grad=True so that one can backprop through it

## Environment
PT master

Reported by @SsnL "
262,21683,0,"Import ONNX model to Pytorch. ## 🚀 Feature

Importing ONNX models into Pytorch.
## Motivation

Almost all other frameworks already support this. Importing ONNX models into Pytorch makes Pytorch much more flexible.

## Pitch

In , a function should be created to take the ONNX model and outputs a Pytorch model. "
48,5406,1,"BatchNorm behaves different in train() and eval(). 

Sample output:

Also in master:
"
492,20465,0,"Second order gradient cuda error. ## 🐛 Bug
Convnet training on GPU: when penalizing gradient growth (backpropagating a gradient of a gradient) the following error happens:

indexValue >= 0 && indexValue < src.sizes[dim]

## Environment
PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: version 3.6.2

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.1.85
GPU models and configuration:
GPU 0: TITAN X (Pascal)
GPU 1: TITAN X (Pascal)
GPU 2: TITAN X (Pascal)
GPU 3: TITAN X (Pascal)

Nvidia driver version: 418.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3
/usr/local/cuda-9.0/lib64/libcudnn.so.7.2.1

Versions of relevant libraries:
[pip3] numpy==1.15.0
[pip3] torch==1.1.0
[pip3] torchfile==0.1.0
[pip3] torchvision==0.2.2
[conda] blas                      1.0                         mkl
[conda] cuda92                    1.0                           0    pytorch
[conda] mkl                       2018.0.3                      1
[conda] mkl_fft                   1.0.4            py36h4414c95_1
[conda] mkl_random                1.0.1            py36h4414c95_1
[conda] pytorch                   1.1.0           py3.6_cuda10.0.130_cudnn7.5.1_0    pytorch
[conda] torchfile                 0.1.0                      py_0    conda-forge
[conda] torchvision               0.2.2                      py_3    pytorch
## Additional context

<!-- Add any other context about the problem here. -->
If I change the coefficient in  from 0.1 to 10 the error does NOT happen (at least not before ~40 epochs), same if I reduce the learning rate from 1 to 0.001. 
"
14,23642,1,"Performance Regression of Dataloader. ## 🐛 Bug

Latest change to Dataloader (#19228) leads to severe performance regression for large scale training up to 30%. We finally root the cause to theses change: https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L889-L891. It causes the exit of each epoch has additional 5 seconds.

## To Reproduce

Steps to reproduce the behavior:

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->



## Expected behavior

The exit is basically free in pytorch 1.1, but it takes 5s in pytorch 1.2.


<!-- A clear and concise description of what you expected to happen. -->

## Environment



## Additional context

<!-- Add any other context about the problem here. -->

The suggest fix is to recover previous lines around https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L889-L891. For example, following code will fix the problem:


"
52,25904,1,"Python hang after using torch.exp(). ## 🐛 Bug

Using function torch.exp() makes python hang. I test some other functions like torch.sum(), torch.abs(), these functions can return right results.
<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:


Python just hang, python cannot be closed by using CTRL+C under Ubuntu 16.04.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

Out[2]: Tensor([1, 2])
<!-- A clear and concise description of what you expected to happen. -->

## Environment

Collecting environment information...
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.5 LTS
GCC version: (Ubuntu 5.3.1-14ubuntu2) 5.3.1 20160413
CMake version: version 3.5.1

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.0.176
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 410.93
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.16.2
[pip] numpydoc==0.9.1
[pip] torch==1.2.0
[pip] torch-cluster==1.4.2
[pip] torch-geometric==1.2.1
[pip] torch-scatter==1.2.0
[pip] torch-sparse==0.4.0
[pip] torch-spline-conv==1.1.0
[pip] torchsummaryX==1.3.0
[pip] torchvision==0.4.0a0+6b959ee
[conda] blas                      1.0                         mkl  
[conda] cuda90                    1.0                  h6433d27_0    pytorch
[conda] mkl                       2018.0.3                      1  
[conda] mkl-service               1.1.2            py36h90e4bf4_5  
[conda] torch                     1.2.0                    pypi_0    pypi
[conda] torch-cluster             1.4.2                    pypi_0    pypi
[conda] torch-geometric           1.2.1                    pypi_0    pypi
[conda] torch-scatter             1.2.0                    pypi_0    pypi
[conda] torch-sparse              0.4.0                    pypi_0    pypi
[conda] torch-spline-conv         1.1.0                    pypi_0    pypi
[conda] torchsummaryx             1.3.0                    pypi_0    pypi
[conda] torchvision               0.2.2.post2              pypi_0    pypi
"
302,12181,0,"Network surgery for transfer fails. ## Per the pytorch/caffe2 Readme I am asking here.

I would like to use an existing network definition and weights from the model zoo as the backbone for a new network. In this specific example the architecture will be squeezenet, and the new network simply has a different shape for the top parameterized layers ['conv10_w', 'conv10_b'], to accommodate a different set of classes from Imagenet. 

Unfortunately, it is not clear from the documentation, tutorials, or examples how to achieve this (to me). Some OS notes: I have built caffe2+OpenCV from source with the current master, into a python2.7.12 virtualenv, cuda 9.0, cuDNN 7.0. 

I wrote a script ( based on https://nbviewer.jupyter.org/gist/kyamagu/6cff70840c10ca374e069a3a7eb00cb4/dogs-vs-cats.ipynb )
that I think should do this: https://gist.github.com/johncorring/d735675e75add96fbdfbcc40fa00f3ba

I get the following error message:
Traceback (most recent call last):
  File ""dogsvscats.py"", line 184, in <module>
    shtyp = workspace.InferShapesAndTypes([train_model.net])
  File ""/home/john/Code/pytorch/build/caffe2/python/workspace.py"", line 258, in InferShapesAndTypes
    blobdesc_prototxt = C.infer_shapes_and_types_from_workspace(net_protos)
MemoryError: std::bad_alloc

which isn't very helpful (especially since cross referencing against caffe2 docs doesn't yield anything).

When I comment out the offending line and try to continue to training I recieve a seg fault that I have narrowed down to coming from line 204, workspace.RunNet(train_model.net). lldb returns the following stack trace:

 thread #1: tid = 9130, 0x00007fffaa112240 libcaffe2.sovoid caffe2::math::CopyMatrix<float, caffe2::CPUContext>(int, int, float const*, int, int, float*, int, int, caffe2::CPUContext*) + 208
    frame #1: 0x00007fffaa11392f libcaffe2.socaffe2::ConvOp<float, caffe2::CPUContext>::RunOnDeviceWithOrderNCHW()::{lambda(caffe2::Tensor*)#1}::operator()(caffe2::Tensor*) const + 1169
    frame #3: 0x00007fffaa3f77f8 libcaffe2.socaffe2::ConvPoolOpBase<caffe2::CPUContext>::RunOnDevice() + 301
    frame #5: 0x00007fffa9fb52e5 libcaffe2.socaffe2::SimpleNet::Run() + 460
    frame #7: 0x00007fffaa0aeb8a libcaffe2.sovoid pybind11::cpp_function::initialize<caffe2::python::addGlobalMethods(pybind11::module&)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool)#21}, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, pybind11::name, pybind11::scope, pybind11::sibling>(caffe2::python::addGlobalMethods(pybind11::module&)::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool)#21}&&, bool (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) + 311
    frame #9: 0x00007fffab160220 caffe2_pybind11_state_gpu.soPyEval_EvalFrameEx + 29342
    frame #11: 0x00000000004b9ab6 pythonPyEval_EvalFrameEx + 24639
    frame #13: 0x00000000004b9ab6 pythonPyEval_EvalFrameEx + 22711
    frame #15: 0x00000000004b9ab6 python??? + 63
    frame #17: 0x00000000004e5422 pythonPyRun_SimpleFileExFlags + 390
    frame #19: 0x0000000000493ae2 python__libc_start_main(main=(python_start + 41


"
543,18682,0,"Time-consuming gradient computation and backward for realizing meta network. ## ❓ Questions and Help
I am trying to implement the [paper](https://arxiv.org/pdf/1803.09050.pdf) for noisy label task. We found the training time is extremely time consuming, which troubled us for a long time.

The algorithm which i am tryinng to implement is the following:
![image](https://user-images.githubusercontent.com/30048368/55319105-c94bc080-54a6-11e9-923f-81b819c96154.png)

And the code is the following:
 

We find time_backward in meta-val stage takes up 99% of the meta-val training time. Moreover, the meta-val training time takes up 93% of the all training time. 

Could anyone give some suggestions on how to resolve this issue? Thank you very much!"
639,17161,0,"index operation is not supported in torch.HalfTensor . ## 🚀 Feature
Currently, an indexing of torch.HalfTensor like half_tensor[indices] gives this runtime error.
RuntimeError: ""index"" not implemented for 'torch.HalfTensor'. So, requesting for an indexing method. 

## Motivation
It would be great to be able to index half tensors. My current use case for this is while doing large scale similarity metric computations on embeddings on the GPU where I figured I could use float16 to reduce memory usage.

## Pitch
Indexing method added to HalfTensor objects.

## Alternatives
For my use-case the alternative is to chunk up tensors appropriately to do similarity computations. While its works just as well, I can imagine other use cases where indexing on HalfTensor objects are needed.
"
447,24088,0,"RuntimeError: ""eye"" not implemented for 'Bool' when on CPU. ## 🐛 Bug
torch.eye doesn't work for dtype torch.bool when the device is CPU, but it does work on GPU. 

## To Reproduce
>>> torch.eye(5, dtype=torch.bool, device=torch.device(""cpu""))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: ""eye"" not implemented for 'Bool'

>>> torch.eye(5, dtype=torch.bool, device=torch.device(""cuda""))
tensor([[ True, False, False, False, False],
        [False,  True, False, False, False],
        [False, False,  True, False, False],
        [False, False, False,  True, False],
        [False, False, False, False,  True]], device='cuda:0')

## Expected behavior
I was expecting torch.eye for bool to be implemented for both CPU and CUDA. Maybe there are other torch functions with this issue as well?

## Environment
Collecting environment information...
PyTorch version: 1.2.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 1060
Nvidia driver version: 430.40
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.17.0
[pip3] torch==1.2.0
[pip3] torchvision==0.4.0
[conda] Could not collect

 - PyTorch Version (e.g., 1.0): 1.2
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (, , source): pip
 - Python version: 3.6.8
 - CUDA/cuDNN version: 10.1
 - GPU models and configuration: GTX 1060 (6GB)

## Additional context
I can workaround on CPU for now by casting to torch.bool after calling torch.eye.
"
16,10851,1,"`profiler.table()` too slow. ## Issue description

When processing large amount of output lines(about 100000 lines), profiler.table() can spend several minutes. I think it's because in , new lines are added to the end of a string, and this can result in a copy operation of the whole string. The best practise here may be using ."
78,13722,1,"Extremely high GPU memory usage for a simple architecture. ## 🐛 Bug

When using a particular architecture, pytorch is throwing CUDA OOME much faster (with ""batch_size"" of 10k) than tensorflow (which runs smoothly with ""batch_size"" of 200k-500k).

## To Reproduce

Steps to reproduce the behavior:

I wrote two almost identical implementations of a problematic NN architecture in both pytorch and tensorflow.

Here is PyTorch version:


And tensorflow:

If you run both scripts on 11GB GPU you should see CUDA OOME in pytorch (on loss.backward() line), but not in tensorflow script.

## Expected behavior

I would expect adequate GPU memory usage for such a simple architecture and small input size (10k numbers), which is the case in tensorflow, but not in pytorch.

## Environment

(This is the environment of Google Colaboratory, but the same issue on two other machines)

PyTorch version: 0.4.1
Is debug build: No
CUDA used to build PyTorch: 9.2.148

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 9.2.148
GPU models and configuration: GPU 0: Tesla K80
Nvidia driver version: 396.44
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect"
363,15608,0,"[Caffe2] How to switch to test phase?. Hi, I just spent 1 hour looking at the documentation and some tutorials on the website, and googling ; but I cannot figure out how I can put a net in testing phase. I would like an equivalent in Caffe2 of ."
521,28653,0,"QAT with CUDA. Try post train with my own network and dataset but accuracy is not good enough. 
Try QAT with too slow. 
Any way to quickly run QAT on NVIDIA GPU?

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a"
371,15298,0,"Momentum problem (1-momentum is correct?) in BatchNorm2d. ## 🐛 Bug

In BatchNorm2d' doc, the explanation of momentum is:


where  is the newly computed batch statistics,  is the estimated statistics in history.
While in my experiments, when momentum set to 0, the batchnorm operates just use the newly computed statistics.

So I guess maybe the following explanation is correct:



## To Reproduce

 is a torch.Tensor with shape (8, 3, 299, 299), which read from an image
 is the model load from 
I have set the momentum as 0 in [Inception v3 model struct](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py):



Then I calculate the mean and var:


The result is that  while 

## Expected behavior

Expected  while 

## Environment

 - PyTorch Version 0.4.1:
 - Windows:
 - pip:
 - Python version: 3.6.5


cc @brianjo @mruberry @albanD @jbschlosser"
454,15262,0,"doc err:torch.nn.BCELoss. pytorch 1.0
-------------
DOC:This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets y should be numbers between 0 and 1.
--------------
import torch 
import torch.nn.functional as F
b=[[0.2,0.1]]
c=[[1.,3.]]
aa = torch.tensor(b,requires_grad=True)
target = torch.tensor(c)
output = F.binary_cross_entropy(aa,target,reduce=True,size_average=True)
z=output.sum()
z.backward()
print(output)
-----------------------
tensor(4.1532, grad_fn=<BinaryCrossEntropyBackward>)
---------------
target value is 1 and 3.,not between 0 and 1.
"
206,26218,0,"Default adam epsilon to 1e-7 when on fp16. ## 🚀 Feature
If you use 1e-8 as default and you use 16 bit, it will round to zero, and thus you get no stability gains.


cc @vincentqb"
234,28404,0,"[Quantization] (Error): No function is registered for schema aten. ## ❓ Questions and Help
    I tried to quantizate my own shuffle model through 'STATIC QUANTIZATION WITH EAGER MODE IN PYTORCH'.
    But when it comes to forward propagation, the time to assess model losses, I failed:

    

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100"
257,22013,0,"Mysterious Tensor Indexing Problem. ## 🐛 Bug

Indexing into a tensor with a 2d list of indices seems to fail sometimes, with a critical point when the number of indices is less than 32.

## To Reproduce



 fails with:


## Expected behavior

I expected this indexing to work the same for any number of indices.  This is problematic in my actual code as the size of the data varies.

## Environment

PyTorch version: 1.1.0
Is debug build: No
CUDA used to build PyTorch: 10.0.130

OS: Ubuntu 16.04.6 LTS
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.16.4
[pip3] pytorch-pretrained-bert==0.6.2
[pip3] torch==1.1.0
[conda] Could not collect

## Additional context

<!-- Add any other context about the problem here. -->


cc @mruberry @rgommers @heitorschueroff"
43,26386,1,pytorch inference fp16. The model is trained with fp32. I try to use .half() to change  layers and inputs to fp16. Actually，it indeed accelerate the inference. But its acceleration effect is far away from twice  of fp32. My platform is nvidia tx2. Its compute capability is 6.2. It supports fp16 very well. So what I want to ask is whether fp16 cannot be twice as fast as fp32 in pytorch. Looking for your reply.Thank you.
689,23925,0,"'border' and 'reflection’ modes of grid_sample have incorrect gradients at border. ## 🐛 Bug

When  in , and a grid point falls exactly on the high boundary of the image (), the gradient should be based on the border padding scheme, which should give either the gradient from just inside the boundary, or zero from just outside the boundary (either could be valid, since it’s a non differentiable point). Instead, the gradient is currently based on zero padding the image, which gives wacky results.

Same problem occurs with  for 2D  on CPU.
Reflection modes of both the cuda version and the 3D CPU version also have this problem, but it’s arguably worse, since the incorrect gradient is also negated. Furthermore, this is an inconsistency between the behavior of CPU and CUDA kernels.

## Example:



Notice the wacky last row and last column. This is because the gradient there is currently calculated as if the image was zero-padded.

The result should ideally look like



which finds the gradient using the in-bounds neighbor.

A less ideal, but still palatable result would be



which finds the gradient using the out-of-bounds, border-padded neighbor.

Reflection mode on cpu (for instance, try using these same commands, but with ) gives the exact same problematic result.
When using reflection mode on cuda, however, (as well as for 3D grid_sample on cpu) the problematic gradients are negated!



This is also problematic, of course, but even more so because of the mismatch between the cpu and cuda behaviors.

For  mode, I think it makes sense to set the gradient in such cases to zero, since it’s sort of at the apex of a symmetric hill. But setting it to take the gradient of one side or the other might also be acceptable for most practical purposes.

For  mode, by contrast, I think it makes more sense to always take the non-zero gradient from the inner side, since the outer side gradient will be zero and so effectively stop training (see the related discussion for  at #7002 and #7049).




PyTorch Version: tested on commit https://github.com/pytorch/pytorch/commit/0539462ca2966aa29657b58aeb17a85c21524d31"
313,11850,0,"[Caffe2/Bug] Cannot enable MKL-DNN. ## Solution
A solution is found for the problem, but maybe a bug in Caffe2

MKL-DNN can be enabled by first compiling the PyTorch. Then turn on MKL by

and recompile PyTorch. Then MKL-DNN is enabled.

## Issue description

Trying to accelerate caffe2 inference with MKL-DNN.

The MKL-DNN lib can be detected:

But mkl operators are not compiled

And MKL-DNN cannot be found after installation since

returns False.

Also, I try to enable MKL by changing the CMakeLists.txt:

It acutally cause an error:


## System Info
PyTorch version: 1.0.0a0+98aebed
Is debug build: No
CUDA used to build PyTorch: None

OS: Ubuntu 18.04.1 LTS
GCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0
CMake version: version 3.10.2

Python version: 2.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy (1.15.1)
[pip] torch (1.0.0a0+98aebed)
[conda] Could not collect

"
216,4540,0,"Cuda out of memory but gpu memory is utilized about a half. I'm running slightly modified code of EDSR baseline network from here: https://github.com/thstkdgus35/EDSR-PyTorch
The training on first epoch goes well, but on the test stage when loading a model it raises an error:

But what is strange is that gpu memory at that moment is utilised about a half. Here's nvidia-smi output (at the moment of peak memory usage):

In test function I also use volatile=True option. What could be the problem?
OS: Win10
GPU: 1060 6G


  
  "
720,21701,0,Add comments on classes to `bailout_graph.cpp`
645,11482,0,"Error -3 while decompressing data: invalid block type. ## Issue description
I got the problem when I tried to download MNIST dataset. Tested twice: with pytorch installed using pip and pytorch compiled from source.
## Code example




## System Info

PyTorch version: 0.3.1
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.13.6
GCC version: Could not collect
CMake version: version 3.12.1

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] Could not collect
[conda] Could not collect
"
719,16326,0,"[JIT] C++ frontend is unable to support std::vector<std::vector<Tensor>>. ## 🐛 Bug
C++ frontend is unable to support std::vector<std::vector<Tensor>>

## To Reproduce
Python Code



CPP code

Error Message




## Some preliminary debugging
In particular, this is the offending function




this is the call site of the offending function. It does not seem to check generic types. Like . Instead, it goes straight to 



## Environment
Pytorch version: 4edc8273eb223785929c0017caf15c101964d480

"
436,2735,0,nk
23,2180,1,"Discrepancy in BCEWithLogitsLoss and ClassNLLLoss. In BCEWithLogitsLoss, weights are unnormalized and used, while in ClassNLLLoss weights are normalized and used. 

Some discussion around this can be found [here](https://github.com/pytorch/pytorch/commit/67968cb60b1d3021834594967d4140a36a8213e3#commitcomment-23155134). Is this discrepancy expected? Else if this is an issue I can send a PR to resolve this."
283,17850,0,"Build fails for caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o; possibly MKL-DNN issue. ## 🐛 Bug

PyTorch fails to build with an issue related to MKL-DNN. I previously built MKL-DNN by ing from their GitHub repo.

Error trace is below:



## Build command


## Environment

 - PyTorch Version (e.g., 1.0): Master branch
 - OS (e.g., Linux): Debian:Stretch
 - How you installed PyTorch (, , source): source
 - Build command you used (if compiling from source): see above
 - Python version: 3.6.5
 - CUDA/cuDNN version: NA
 - GPU models and configuration: NA
 - Any other relevant information: NA
"
72,16258,1,"Inconsistent Behavior of permute() in different calling order. ## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
I am trying to convert the format between cv::Mat and torch::Tensor using c++(libtorch), one crucial step of which is to permute the shape of the Tensor. However I found that in some cases the permute() function failed to manage the memory of the image.

## To Reproduce

I provide some code to reproduce the bug:

1. Image Version


1. A Simpler Matrix Version
this code:

would output (wrong):


but change the permute line from 

to 


will output (correct):


Note that when the permute() failed to managed the memory, it always returned the ""correct"" shape of the permuted tensor.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
The two different usage of permute() should behave consistently( both should  permute the memory not only the shape)

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 - PyTorch Version (e.g., 1.0): v1.0.0
 - OS (e.g., Linux): Linux Arm
 - How you installed PyTorch (, , source): From source, and following the step of [issue 15138](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
 - Build command you used (if compiling from source): python setup.py build
 - Python version: 3.5.2
 - CUDA/cuDNN version: libcudnn.so.7.0.5
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
84,3018,1,"CUDA initialization hangs on CUDA 9. I just installed pytorch HEAD from source with cuda 9 and cudnn 7 using gcc 5.4 on ubuntu 16.04. It seemed to build and install without issue, however the python process hangs if I try to move a tensor to gpu using the cuda method. The following snippet also hangs.



There are 2 Maxwell Titan X and 2 Pascal 1080 Ti cards attached to the machine."
299,24045,0,"torch.{save,load} data corruption when serializing a Module with __{get,set}state__. Repro:



Output



cc @ezyang @gchanan @zou3519"
525,15557,0,"main.exe ,which was made  from main.py by pyinstaller , faced on error  . ## ❓ Questions and Help
I made main.exe ,which is made from main.py by the pyinstaller.
But I did main.exe,then it faced  ""main.exe: error: unrecognized arguments: --multiprocessing-fork
parent_pid=7056 pipe_handle=1188"" erroe and stopped.

How do I change main.py code to operate main.exe without this error?

I'd really appreciate it if anyone will give me the advice as soon as possile

・CUDA9, Python3.6 ,Pytorch Windows for CUDA9,
・OS　Windows10 Pro 64bit
・Nvidia GTX980
        
"
644,10857,0,"got error ""ninja: error: loading 'build/build.global.ninja': No such file or directory"". If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

## Code example

## System Info
Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


- PyTorch or Caffe2: PyTorch
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): python setup.py build
- OS: CentOS7
- PyTorch version: master branch (ca567862)
- Python version: 3.7.0
- CUDA/cuDNN version: 9.2/7.2.1
- GPU models and configuration: NVIDIA GTX1070
- GCC version (if compiling from source): 4.8.5
- CMake version: 3.12.0
- Versions of any other relevant libraries:
"
617,125,0,"make tests deterministic. by default the tests should be deterministic, individually.
randomized tests can be run nightly or something, but not by default.
"
544,3999,0,"Save big file is slow. It spent a lot of time for I to save file by torch.save().

The file size is about 3.5G, and it costs 2hr 49m totally. Is it make sense?
"
555,828,0,"backward() in Autograd Index Function is broken when indexing with LongTensor. In :


I think the index-copy statement doesn't work as intended.


I guess the statement should be something like
"
358,8386,0,"Properly release NCCL resources. Right now we're simply leaking them, because NCCL segfaults if you attempt to destroy them after the driver is unloaded.

One strategy to fix it would be to use an  handler.

See #8352 for more context."
115,24625,0,"Migrate `sigmoid_backward` from the TH to Aten (CUDA). Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review."
485,8652,0,"Batch Sampler and Dataset indexing too restricted. I was trying to use a Dataloader with a sampler of strings to index my dataset. The default [BatchSampler](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py#L104) threw me an error that essentially str cannot be casted to int. The cast in question happens on line 139 (currently):

I copied the source for the Batch Loader from master, removed the cast, and everything worked as expected.

[torch.utils.data](https://pytorch.org/docs/stable/data.html) DataLoader doesn't suggest that samplers should return integers, and the various Samplers just say list of indices. An index set doesn't have to be integers. Now Dataset *does* say __getitem__ should support **integer** indexing, but why?

Really, the choice of sampler (including choosing by default the default sampler) drives whether a Dataset must be indexable by integers. On the other hand, sometimes it's more sensible to ""key"" the dataset. For example, the sampler could ""request"" the a filename be loaded. Or, the actual data could be organized in a dict (like I have in my examples below).

Along the way, I noticed that BatchLoader is now asserting that sampler is a Sampler. (This isn't the case in 0.4.0, just master). I don't really understand why. Why not duck type that sampler is iterable? Just a try-except around the for-loop that catches TypeError and returns whatever type of error. It breaks the example if I'm not mistaken, and excludes a very useful way to unittest.

## MWE
"
702,6285,0,"AutoGPU header relies on WITH_CUDA. AutoGPU is compiled with WITH_CUDA. Instead, it should be in an auto_gpu.cpp file so we do not have to set this macro while compiling it...

If a third party lib includes auto_gpu.h without doing a , then they try to use it, the things in auto_gpu.h wrapped with  will not compile and it won't work. The solution I can see is to split it out into a .cpp so CUDA is baked into the pytorch .so."
327,25417,0,"Add a mode to check input tensor sizes in allreduce_coalesced. @jfc4050 added CPU  in #24949. The current version flattens all input tensors into one, and then allreduce that tensor. It works, even if, say, process 0 provides a tensor of size 4 and process 1 provides two tensors of size 2 each. This is a reasonable shortcut to avoid using additional communications to check tensor sizes, but it will be good to add size checking in the default mode, and asking users to explicitly set  to they indeed want to skip that.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera"
318,25381,0,"regarding builtin_function_or_method. ## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
some way to see the code for builtin_function_or_method
## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
consider this scenario
1) I am using google colab, I want to see the implementation of nn.Dropout
2) I ctrl+click on nn.Dropout, it takes me to dropout.py, where I find Dropout class, which returns

3) so, I ctrl+click on F.dropout, it takes me to functional.py, where I see this

4) now, I am stuck, because this is a builtin, and only way for me to look at its code, is to search for this in github.
5) same thing happens when I want to see the implementation of nn.CrossMapLRN2d, it redirects to torch.group_norm, which is builtin.
6) similarly for view, permute, this way to understand what these builtins are doing becomes difficult

because of this I raised an issue in google colab, to provide a way to look at builtin_function_or_method, they told 

""
No, there's no way to get directly to the source in this case.

In particular, for a .so, it could be a cython file, or it could be another kind of C extension (built via vanilla C/C++ code, or SWIG'd in, or something else completely). All we have is the installed package -- there's no way for us to trace that back to source, and in cases like this (torch is distributed as a .whl file) the source isn't included.
""


## Pitch

<!-- A clear and concise description of what you want to happen. -->

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->
"
306,12322,0,"[Caffe2] Segmentation fault (core dumped) while import caffe2.python.core. ## 🐛 Bug

When I want to import caffe2.pytho.core, I get a Segmentation fault.

I followed the install guide for ubuntu 16 with prebuilt binaries and also installed nccl, I've been browsing past issues but can't find a solution.

please help me

## To Reproduce

Steps to reproduce the behavior:

1. Install NCCL 2.3.5
2. conda install -c caffe2 caffe2-cuda9.0-cudnn7
3. import caffe2.python.core

ERROR : 



## Environment


Followed : [](https://caffe2.ai/docs/getting-started.html?platform=ubuntu&configuration=prebuilt)

Cmake : I installed with anaconda I can't find the cmake output




"
486,9383,0,"[docs] Make clear the format of torch.eig eigenvalues. The second dimension must be for the complex imaginary part, but it is not clear in the docs.


Currently in https://pytorch.org/docs/master/torch.html?highlight=eig#torch.eig:
 without specified shapes"
228,12887,0,"Segmentation fault when summing uint8 tensor. Hi. I'm using PyTorch 0.5.0a0+09896d1

This code:


is returning me a ""Segmentation fault"". If instead I do , it works. Also, the problem does not occur with .

EDIT: a more efficient way to reproduce the bug:
"
149,22780,0,"Build Pytorch/Libtorch with TBB support is failing. ## 🐛 Bug

Build Pytorch/Libtorch with TBB support is failing

## To Reproduce

Steps to reproduce the behavior:

1. Set environment variables  to use TBB:

2. Build: 


## Expected behavior

It builds successfully. Building with OpenMP is working in the same environment.

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:


 Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: Ubuntu 14.04.6 LTS
GCC version: (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4
CMake version: version 3.11.1

Python version: 2.7
Is CUDA available: N/A
CUDA runtime version: 10.0.130
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21

Versions of relevant libraries:
[pip] numpy==1.8.2
[conda] Could not collect
## Additional context

Attached CMakeOutput and CMakeError files.
[CMakeError.log](https://github.com/pytorch/pytorch/files/3384330/CMakeError.log)
[CMakeOutput.log](https://github.com/pytorch/pytorch/files/3384331/CMakeOutput.log)


"
737,9527,0,"AttributeError: Method AffineChannelNd is not a registered operator. Did you mean: [AffineChannel]. Traceback (most recent call last):
File ""tools/test_net.py"", line 151, in
main(ind_range=args.range, multi_gpu_testing=False)
File ""tools/test_net.py"", line 113, in main
engine.test_net_on_dataset(multi_gpu=multi_gpu_testing)
File ""/home/chandu/workspace/tracking/DaT/lib/core/test_engine.py"", line 321, in test_net_on_dataset
all_boxes, all_segms, all_keyps = test_net()
File ""/home/chandu/workspace/tracking/DaT/lib/core/test_engine.py"", line 135, in test_net
model = initialize_model_from_cfg()
File ""/home/chandu/workspace/tracking/DaT/lib/core/test_engine.py"", line 59, in initialize_model_from_cfg
init_params=cfg.TEST.INIT_RANDOM_VARS_BEFORE_LOADING)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 63, in create
return get_func(model_name)(init_model(model_name, train, init_params))
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 156, in keypoint_rcnn
add_roi_keypoint_head_func=get_func(cfg.KRCNN.ROI_KEYPOINTS_HEAD))
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 307, in build_generic_fast_rcnn_model
build_data_parallel_model(model, _single_gpu_build_func)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 953, in build_data_parallel_model
single_gpu_build_func(model)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/model_builder.py"", line 199, in _single_gpu_build_func
blob_conv, dim_conv, spatial_scale_conv = add_conv_body_func(model)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/FPN3D.py"", line 51, in add_fpn_ResNet101_conv5_body
ResNet.stage_info_ResNet101_conv5)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/FPN3D.py"", line 98, in add_fpn_generic_onto_body
conv_body_func(model)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/ResNet3D.py"", line 389, in add_ResNet101_conv5_body
return add_ResNet_convX_body(model, (3, 4, 23, 3), freeze_at=2)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/ResNet3D.py"", line 262, in add_ResNet_convX_body
p = model.AffineChannelNd(p, 'res_conv1_bn', dim_out=feat_dims[0], inplace=True)
File ""/home/chandu/workspace/tracking/DaT/lib/modeling/detector.py"", line 94, in AffineChannelNd
if cfg.MODEL.USE_BN:
File ""/home/chandu/workspace/tracking/anaconda2/envs/gtracking/lib/python2.7/site-packages/caffe2/python/core.py"", line 2082, in getattr
"","".join(workspace.C.nearby_opnames(op_type)) + ']'
AttributeError: Method AffineChannelNd is not a registered operator. Did you mean: [AffineChannel]

**Why am I getting this error?**"
11,3665,1,"Slight memory leak for LSTM . Hi everyone, 
I find there is a slight memory leak during training my lstm networks. 
Here is the running environment:

> torch: 0.2.0.3
> cuda: 7.5
> OS:ubuntu 16.04LTS

This issue is similar to the discuss [here](https://discuss.pytorch.org/t/tracking-down-a-suspected-memory-leak/1130/9). 
Following with the discussion, I found to disable the cuDNN (adding ) can solve this problem but the speed is affected either. 

I am quite confused about the memory leak when using cuDNN. 



Results when cuDNN is enabled, the memory leak occurs in every epoch. Screenshot:
![cudnn_enable](https://user-images.githubusercontent.com/9111828/32723137-558d6d6a-c8a7-11e7-8934-644aeb1aa762.png)

Results when cuDNN is disabled, the memory leak is almost eleminated (although happens in first several epochs). Screenshot:
![cudnn_disable](https://user-images.githubusercontent.com/9111828/32723151-638edee4-c8a7-11e7-825b-f6f604b7aaf2.png)
"
578,1623,0,"incompatibility between Pytorch 0.1.12 and cuda 8.0.61/cudnn 5.1.10 ?. Hi,

I have two linux servers. My code works in one machine but not the other. In the one that it doesn't work, the error message is:  occured during batch normalization( convolution is fine). Also, the same code works in CPU but not in GPU. 

I guess this problem is mainly related to GPU libraries like cuda or cudnn. *The one that doesn't work has the following configurations*:
- Nvidia driver 375.26
- cuda 8.0.61
- cudnn 5.1.10

*While in the one it does work*, I have the following configurations:
- Nvidia driver 367.48
- cuda 8.0.44
- cudnn 5.1.5

TensorFlow can run in the above environment flawless though. So I am wondering if this is caused by the incompatibility between PyTorch 0.1.12 and cuda 8.0.61/cudnn 5.1.10 ?
"
581,9226,0,warn if batchsize < #gpu in data parallel
176,9054,0,"UBSAN error: call to function THDRequest_free through pointer to incorrect function type 'void (*)(void *)'. Both  and  in  throws error with UBSAN (error line: https://github.com/pytorch/pytorch/blob/master/torch/csrc/PtrWrapper.cpp#L49):

"
648,10435,0,"[Caffe2 Bug] Error ISO C++ forbids declaration of DISABLE_COPY_AND_ASSIGN with no type. ## Issue description

Compile error when using 

https://github.com/pytorch/pytorch/blob/ab6afc2b238deb9e3b731399a367385518b788e5/modules/rocksdb/rocksdb.cc#L70

## System Info

- PyTorch or Caffe2: Caffe2
- How you installed PyTorch (conda, pip, source): source
- Build command you used (if compiling from source): cmake+ninja
- OS: CentOS
- PyTorch version: master
- GCC version (if compiling from source): 6.3.1
- CMake version: 3.12
"
629,14563,0,"Address multiple process groups from torch.distributed. In  you can initialize a global process group using the  function. This function takes a backend argument and will initialize **either** the Gloo, NCCL, or MPI backend. You're at a loss if you want to use both Gloo and NCCL through the  frontend. The only way to do this today is by manually creating ProcessGroup instances and using them directly.

For the case where you're dealing with both CPU and CUDA tensors and want to use Gloo for the CPU ones and NCCL2 for the CUDA ones, we should consider supporting a mode where we don't initialize a single backend, but multiple at the same time. We then use a process group dispatcher class that forwards the calls to the appropriate process group depending on the device type of the tensor arguments. This class should be a subclass of the ProcessGroup base class in C++ such that we can use it both from the Python side as well as from the C++ side.

Have to figure out what is the right way to expose this to the  function.

cc @teng-li "
528,31419,0,"[android] initHybrid missing/broken in pytorch 1.4.0 nightly. ## 🐛 Bug

Trying to load a tensor from a blob leads to an error:
    java.lang.UnsatisfiedLinkError: No implementation found for com.facebook.jni.HybridData org.pytorch.Tensor.initHybrid() (tried Java_org_pytorch_Tensor_initHybrid and Java_org_pytorch_Tensor_initHybrid__)
        at org.pytorch.Tensor.initHybrid(Native Method)
        at org.pytorch.Tensor.initHybrid(Tensor.java:337)
        at org.pytorch.Tensor.fromBlob(Tensor.java:289)

This is a very recently introduced bug, as the nightly snapshot last week worked fine.

## To Reproduce

Steps to reproduce the behavior:
1. Create Android Project and add pytorch 1.4.0 nighly snapshot support (e.g. as shown here https://github.com/pytorch/pytorch/issues/29806) 
2. add following lines
        val inputHeight = 299
        val inputWidth = 299
        val inputTensorBuffer = Tensor.allocateFloatBuffer(3 * inputWidth * inputHeight)
        val inputTensor = Tensor.fromBlob( inputTensorBuffer, longArrayOf( 1, 3, inputHeight.toLong(), inputWidth.toLong()))
3. build & run app
4. App crashes with error above

## Expected behavior

Loading a tensor into memory without error (worked with nightly build last week)

## Environment

 - PyTorch Version (e.g., 1.0): 1.4.0 nightly snapshot (18 Dec 2019)
 - OS (e.g., Linux): Windows 10
 - Android studio 3.5.3, OpenJDK JRE 1.8.0_202
 - CUDA/cuDNN version: None (CPU-only)
 - GPU models and configuration: None 

## Additional context

Using a stable version is not an option, as 1.3.0 has a breaking bug in loading the inception v3 model (https://github.com/pytorch/pytorch/issues/29806) "
217,15436,0,"Type Conversions. ## 🐛 Bug

I'm quite new to pytorch so I'm not exactly sure if this is a bug or just an error on my end.

I'm attempting to build a simple LogisticRegression Module on the  dataset in sklearn but for some reason, I keep getting this.

> RuntimeError: Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'

I've tried to convert the both my X and y variables to  but it still seems to think it's float for some reason.

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:








<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

The error generated is:

> ---------------------------------------------------------------------------
> RuntimeError                              Traceback (most recent call last)
> <ipython-input-122-c1edd1a76c76> in <module>
>      10         optimizer.zero_grad()
>      11 
> ---> 12         outputs = model(train)
>      13         loss = criterion(outputs, test)
>      14 
> 
> ~/miniconda3/envs/mlbook/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
>     487             result = self._slow_forward(*input, **kwargs)
>     488         else:
> --> 489             result = self.forward(*input, **kwargs)
>     490         for hook in self._forward_hooks.values():
>     491             hook_result = hook(self, input, result)
> 
> <ipython-input-117-fca446c51ad3> in forward(self, x)
>       5 
>       6     def forward(self, x):
> ----> 7         out = self.layer(x)
>       8         return out
> 
> ~/miniconda3/envs/mlbook/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
>     487             result = self._slow_forward(*input, **kwargs)
>     488         else:
> --> 489             result = self.forward(*input, **kwargs)
>     490         for hook in self._forward_hooks.values():
>     491             hook_result = hook(self, input, result)
> 
> ~/miniconda3/envs/mlbook/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)
>      65     @weak_script_method
>      66     def forward(self, input):
> ---> 67         return F.linear(input, self.weight, self.bias)
>      68 
>      69     def extra_repr(self):
> 
> ~/miniconda3/envs/mlbook/lib/python3.6/site-packages/torch/nn/functional.py in linear(input, weight, bias)
>    1352         ret = torch.addmm(torch.jit._unwrap_optional(bias), input, weight.t())
>    1353     else:
> -> 1354         output = input.matmul(weight.t())
>    1355         if bias is not None:
>    1356             output += torch.jit._unwrap_optional(bias)
> 
> RuntimeError: Expected object of scalar type Double but got scalar type Float for argument #2 'mat2'

## Expected behavior
Train and test the data on the network
<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:

PyTorch version: 1.0.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 10.14.3
GCC version: Could not collect
CMake version: version 3.13.2

Python version: 3.6
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy (1.14.0)
[pip] numpydoc (0.7.0)
[pip] torch (1.0.0)
[pip] torchvision (0.2.1)
[conda] blas                      1.0                         mkl
[conda] mkl                       2018.0.1             hfbd8650_4
[conda] mkl-service               1.1.2            py36h7ea6df4_4
[conda] pytorch                   1.0.0                   py3.6_1    pytorch
[conda] torchvision               0.2.1                      py_2    pytorch

## Additional context

<!-- Add any other context about the problem here. -->
When I attempt to check the datatype at each point, I get  and 
 respectively."
415,23232,0,"Add collective communication APIs for Python objects. . ## 🚀 Feature

Support , , , and  general python objects. Instead of overloading existing API, it might be clearer to make them explicit, e.g.: , , , and .


## Motivation

Sharing objects across processes is a very common use case. People actually started building their own solutions by serializing meta and object into tensors and then using tensor collective comm APIs to share them (see,  in #22490,  in #23228). 

## Pitch

Python object on different processes might not be of the same size. So this needs to be done in multiple steps:

1. Pickle local python object into byte tensors. 
2. Communicate the tensor sizes across processes. Then, each process allocates local buffer tensors using the max size.
3. Copy local tensor to the buffer tensor and communicate. 
4. Extract results from the buffer tensors and unpickle them back into Python objects.

cc @pietern @zhaojuanmao @pritamdamania87 
"
475,23002,0,"Wrong input to Categorical makes Python quit unexpectedly. I pass a zero tensor to Categorical, which I expect torch should throw me some exception. Instead, Python quits unexpectedly



"
225,14864,0,"[discussion] Recommend a different file extension for models (.PTH is a special extension for Python). *.pth files are used by Python to list additional package search paths: https://docs.python.org/3/library/site.html

The pth files will be loaded as text files by Python interpreter. At some point when I had some PyTorch model pth file placed along with the sources, it caused a hang of Python at startup (it was trying to parse the big binary file as a list of paths).

Maybe just *.pt?"
712,24991,0,"[Feature request] Add support for selu activation to calculate_gain function.. ## 🚀 Feature

Add support for selu activation to calculate_gain function.

https://pytorch.org/docs/stable/_modules/torch/nn/init.html#calculate_gain

cc @albanD @mruberry @SsnL"
481,14653,0,"[JIT] Error reporting on imported modules highlights serialization code. Repro:



Output:



This has the potential to be super confusing to end users. We should probably preserve the original source for highlighting"
348,19969,0,"libtorch Segmentation Fault: RHEL 7 - easy to reproduce. ## 🐛 Bug

tl; dr; I traced a (non-pretrained) resnet18 model in python and saved this to a .pt. I successfully loaded this .pt in a C++ program. Model inference seems to work. When the C++ program exits, I encounter a segmentation fault.

## To Reproduce

Here are code snippets to minimally reproduce the issue:

(1) First, generate the model by running the following python script. If successful, you should see a 0 print out. I'm able to run this code with no problems.



(2) Here is the C++ program I run that generates a segmentation fault. To run, do 



When I run the C++ program, here's the output I receive:



## Things I've tried
- I've tried running the above C++ code on the CPU (so, replacing  with ). This doesn't result in a segmentation fault.

- I've tried running the above Python/C++ code on both CPU, GPU for a simple feedforward net (30 inputs -> 20 hiddens -> ReLU -> 2 outputs -> Sigmoid); this doesn't result in a segmentation fault. Running with  on CPU, I don't see any issues. On GPU, however here's what I get:



## Expected behavior

In running with hard.pt, I expect the code to run without any segfaults.

## Environment

 - PyTorch Version (e.g., 1.0): 1.1.0.dev20190425
 - OS (e.g., Linux): Red Hat Enterprise Linux Server 7.4 (Maipo)
 - How you installed PyTorch (, , source): 
 - Build command you used (if compiling from source): 
 - Python version: 
 - CUDA/cuDNN version: 8.0.44/7
 - GPU models and configuration: 

Versions of relevant libraries:
[pip] numpy==1.15.4
[pip] numpydoc==0.8.0
[pip] torch==1.0.1.post2
[pip] torchvision==0.2.2
[conda] blas                      1.0                         mkl
[conda] mkl                       2019.1                      144
[conda] mkl-service               1.1.2            py37he904b0f_5
[conda] mkl_fft                   1.0.6            py37hd81dba3_0
[conda] mkl_random                1.0.2            py37hd81dba3_0
[conda] pytorch                   1.0.1           py3.7_cuda8.0.61_cudnn7.1.2_2    pytorch
[conda] torchvision               0.2.2                      py_3    pytorch

## Additional context

I've tried following the instructions to resolve this issue here: https://github.com/pytorch/pytorch/issues/12705 but haven't seen success; I've tried building pytorch from source + linking my C++ code to the generated libs, but this doesn't work either.

cc @ezyang @gchanan @zou3519 @ngimel"
448,29117,0,"[tracking issue] RPC tests are flaky. 

cc @ezyang @gchanan @zou3519 @jerryzh168 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528"
